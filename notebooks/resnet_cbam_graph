digraph {
	graph [size="324.45,324.45"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	137615231157328 [label="
 (1, 10)" fillcolor=darkolivegreen1]
	137614695639904 [label=AddmmBackward0]
	137614695639088 -> 137614695639904
	137614719687936 [label="fc.bias
 (10)" fillcolor=lightblue]
	137614719687936 -> 137614695639088
	137614695639088 [label=AccumulateGrad]
	137614695639040 -> 137614695639904
	137614695639040 [label=ViewBackward0]
	137614695639136 -> 137614695639040
	137614695639136 [label=MeanBackward1]
	137614695639856 -> 137614695639136
	137614695639856 [label=ReluBackward0]
	137614695638944 -> 137614695639856
	137614695638944 [label=MulBackward0]
	137614695638848 -> 137614695638944
	137614695638848 [label=MulBackward0]
	137614695638368 -> 137614695638848
	137614695638368 [label=AddBackward0]
	137614695638464 -> 137614695638368
	137614695638464 [label=NativeBatchNormBackward0]
	137614695638272 -> 137614695638464
	137614695638272 [label=ConvolutionBackward0]
	137614695627792 -> 137614695638272
	137614695627792 [label=ReluBackward0]
	137614695628224 -> 137614695627792
	137614695628224 [label=NativeBatchNormBackward0]
	137614695627936 -> 137614695628224
	137614695627936 [label=ConvolutionBackward0]
	137614695627168 -> 137614695627936
	137614695627168 [label=ReluBackward0]
	137615234727264 -> 137614695627168
	137615234727264 [label=NativeBatchNormBackward0]
	137615234727744 -> 137615234727264
	137615234727744 [label=ConvolutionBackward0]
	137614695640000 -> 137615234727744
	137614695640000 [label=ReluBackward0]
	137615234726496 -> 137614695640000
	137615234726496 [label=MulBackward0]
	137615234719008 -> 137615234726496
	137615234719008 [label=MulBackward0]
	137615234726928 -> 137615234719008
	137615234726928 [label=AddBackward0]
	137615234719920 -> 137615234726928
	137615234719920 [label=NativeBatchNormBackward0]
	137615234727840 -> 137615234719920
	137615234727840 [label=ConvolutionBackward0]
	137615234727888 -> 137615234727840
	137615234727888 [label=ReluBackward0]
	137615234726880 -> 137615234727888
	137615234726880 [label=NativeBatchNormBackward0]
	137615234727216 -> 137615234726880
	137615234727216 [label=ConvolutionBackward0]
	137615234727168 -> 137615234727216
	137615234727168 [label=ReluBackward0]
	137615234717664 -> 137615234727168
	137615234717664 [label=NativeBatchNormBackward0]
	137615234727552 -> 137615234717664
	137615234727552 [label=ConvolutionBackward0]
	137615234727648 -> 137615234727552
	137615234727648 [label=ReluBackward0]
	137615234720160 -> 137615234727648
	137615234720160 [label=MulBackward0]
	137615234718912 -> 137615234720160
	137615234718912 [label=MulBackward0]
	137615234727072 -> 137615234718912
	137615234727072 [label=AddBackward0]
	137615234719104 -> 137615234727072
	137615234719104 [label=NativeBatchNormBackward0]
	137615234717712 -> 137615234719104
	137615234717712 [label=ConvolutionBackward0]
	137615234718864 -> 137615234717712
	137615234718864 [label=ReluBackward0]
	137615234718672 -> 137615234718864
	137615234718672 [label=NativeBatchNormBackward0]
	137615234718480 -> 137615234718672
	137615234718480 [label=ConvolutionBackward0]
	137615234718288 -> 137615234718480
	137615234718288 [label=ReluBackward0]
	137615234719680 -> 137615234718288
	137615234719680 [label=NativeBatchNormBackward0]
	137615234719296 -> 137615234719680
	137615234719296 [label=ConvolutionBackward0]
	137615234718768 -> 137615234719296
	137615234718768 [label=ReluBackward0]
	137615234719152 -> 137615234718768
	137615234719152 [label=MulBackward0]
	137615234719632 -> 137615234719152
	137615234719632 [label=MulBackward0]
	137615234719584 -> 137615234719632
	137615234719584 [label=AddBackward0]
	137615234720352 -> 137615234719584
	137615234720352 [label=NativeBatchNormBackward0]
	137615234719824 -> 137615234720352
	137615234719824 [label=ConvolutionBackward0]
	137615235056160 -> 137615234719824
	137615235056160 [label=ReluBackward0]
	137615235055680 -> 137615235056160
	137615235055680 [label=NativeBatchNormBackward0]
	137615235055776 -> 137615235055680
	137615235055776 [label=ConvolutionBackward0]
	137615235058992 -> 137615235055776
	137615235058992 [label=ReluBackward0]
	137615235056592 -> 137615235058992
	137615235056592 [label=NativeBatchNormBackward0]
	137615235056736 -> 137615235056592
	137615235056736 [label=ConvolutionBackward0]
	137615234719344 -> 137615235056736
	137615234719344 [label=ReluBackward0]
	137615235056352 -> 137615234719344
	137615235056352 [label=MulBackward0]
	137615235057072 -> 137615235056352
	137615235057072 [label=MulBackward0]
	137615235056928 -> 137615235057072
	137615235056928 [label=AddBackward0]
	137615235057600 -> 137615235056928
	137615235057600 [label=NativeBatchNormBackward0]
	137615235057168 -> 137615235057600
	137615235057168 [label=ConvolutionBackward0]
	137615235057312 -> 137615235057168
	137615235057312 [label=ReluBackward0]
	137615235057888 -> 137615235057312
	137615235057888 [label=NativeBatchNormBackward0]
	137615235057792 -> 137615235057888
	137615235057792 [label=ConvolutionBackward0]
	137615235058080 -> 137615235057792
	137615235058080 [label=ReluBackward0]
	137615235057984 -> 137615235058080
	137615235057984 [label=NativeBatchNormBackward0]
	137615235058224 -> 137615235057984
	137615235058224 [label=ConvolutionBackward0]
	137615235057120 -> 137615235058224
	137615235057120 [label=ReluBackward0]
	137615235058560 -> 137615235057120
	137615235058560 [label=MulBackward0]
	137615235058608 -> 137615235058560
	137615235058608 [label=MulBackward0]
	137615235058752 -> 137615235058608
	137615235058752 [label=AddBackward0]
	137615235058848 -> 137615235058752
	137615235058848 [label=NativeBatchNormBackward0]
	137615235059040 -> 137615235058848
	137615235059040 [label=ConvolutionBackward0]
	137615235059328 -> 137615235059040
	137615235059328 [label=ReluBackward0]
	137615235059424 -> 137615235059328
	137615235059424 [label=NativeBatchNormBackward0]
	137615235059520 -> 137615235059424
	137615235059520 [label=ConvolutionBackward0]
	137615235059712 -> 137615235059520
	137615235059712 [label=ReluBackward0]
	137615235059856 -> 137615235059712
	137615235059856 [label=NativeBatchNormBackward0]
	137615235059952 -> 137615235059856
	137615235059952 [label=ConvolutionBackward0]
	137615235058944 -> 137615235059952
	137615235058944 [label=ReluBackward0]
	137615235060240 -> 137615235058944
	137615235060240 [label=MulBackward0]
	137615235060336 -> 137615235060240
	137615235060336 [label=MulBackward0]
	137615235060480 -> 137615235060336
	137615235060480 [label=AddBackward0]
	137615235060624 -> 137615235060480
	137615235060624 [label=NativeBatchNormBackward0]
	137615235060768 -> 137615235060624
	137615235060768 [label=ConvolutionBackward0]
	137615235060960 -> 137615235060768
	137615235060960 [label=ReluBackward0]
	137615235061104 -> 137615235060960
	137615235061104 [label=NativeBatchNormBackward0]
	137615235061200 -> 137615235061104
	137615235061200 [label=ConvolutionBackward0]
	137615235061392 -> 137615235061200
	137615235061392 [label=ReluBackward0]
	137615235061536 -> 137615235061392
	137615235061536 [label=NativeBatchNormBackward0]
	137615235061632 -> 137615235061536
	137615235061632 [label=ConvolutionBackward0]
	137615235060576 -> 137615235061632
	137615235060576 [label=ReluBackward0]
	137615235061920 -> 137615235060576
	137615235061920 [label=MulBackward0]
	137615235062016 -> 137615235061920
	137615235062016 [label=MulBackward0]
	137615235062160 -> 137615235062016
	137615235062160 [label=AddBackward0]
	137615235062304 -> 137615235062160
	137615235062304 [label=NativeBatchNormBackward0]
	137615235062448 -> 137615235062304
	137615235062448 [label=ConvolutionBackward0]
	137615235062640 -> 137615235062448
	137615235062640 [label=ReluBackward0]
	137615235062784 -> 137615235062640
	137615235062784 [label=NativeBatchNormBackward0]
	137615235062880 -> 137615235062784
	137615235062880 [label=ConvolutionBackward0]
	137615235063072 -> 137615235062880
	137615235063072 [label=ReluBackward0]
	137615235063216 -> 137615235063072
	137615235063216 [label=NativeBatchNormBackward0]
	137615235063312 -> 137615235063216
	137615235063312 [label=ConvolutionBackward0]
	137615235062256 -> 137615235063312
	137615235062256 [label=ReluBackward0]
	137615235063600 -> 137615235062256
	137615235063600 [label=MulBackward0]
	137615235063696 -> 137615235063600
	137615235063696 [label=MulBackward0]
	137615235063840 -> 137615235063696
	137615235063840 [label=AddBackward0]
	137615235063984 -> 137615235063840
	137615235063984 [label=NativeBatchNormBackward0]
	137615235064128 -> 137615235063984
	137615235064128 [label=ConvolutionBackward0]
	137615235064320 -> 137615235064128
	137615235064320 [label=ReluBackward0]
	137615235064464 -> 137615235064320
	137615235064464 [label=NativeBatchNormBackward0]
	137615235064560 -> 137615235064464
	137615235064560 [label=ConvolutionBackward0]
	137615235064752 -> 137615235064560
	137615235064752 [label=ReluBackward0]
	137615235064896 -> 137615235064752
	137615235064896 [label=NativeBatchNormBackward0]
	137615235064992 -> 137615235064896
	137615235064992 [label=ConvolutionBackward0]
	137615235065184 -> 137615235064992
	137615235065184 [label=ReluBackward0]
	137615235065328 -> 137615235065184
	137615235065328 [label=MulBackward0]
	137615235065424 -> 137615235065328
	137615235065424 [label=MulBackward0]
	137615235065568 -> 137615235065424
	137615235065568 [label=AddBackward0]
	137615235065712 -> 137615235065568
	137615235065712 [label=NativeBatchNormBackward0]
	137615235065856 -> 137615235065712
	137615235065856 [label=ConvolutionBackward0]
	137615235066048 -> 137615235065856
	137615235066048 [label=ReluBackward0]
	137615235066192 -> 137615235066048
	137615235066192 [label=NativeBatchNormBackward0]
	137615235066288 -> 137615235066192
	137615235066288 [label=ConvolutionBackward0]
	137615235066480 -> 137615235066288
	137615235066480 [label=ReluBackward0]
	137615235066624 -> 137615235066480
	137615235066624 [label=NativeBatchNormBackward0]
	137615235066720 -> 137615235066624
	137615235066720 [label=ConvolutionBackward0]
	137615235065664 -> 137615235066720
	137615235065664 [label=ReluBackward0]
	137615235067008 -> 137615235065664
	137615235067008 [label=MulBackward0]
	137615235067104 -> 137615235067008
	137615235067104 [label=MulBackward0]
	137615235067248 -> 137615235067104
	137615235067248 [label=AddBackward0]
	137615235067392 -> 137615235067248
	137615235067392 [label=NativeBatchNormBackward0]
	137615235067488 -> 137615235067392
	137615235067488 [label=ConvolutionBackward0]
	137615233381712 -> 137615235067488
	137615233381712 [label=ReluBackward0]
	137615233381856 -> 137615233381712
	137615233381856 [label=NativeBatchNormBackward0]
	137615233381952 -> 137615233381856
	137615233381952 [label=ConvolutionBackward0]
	137615233382144 -> 137615233381952
	137615233382144 [label=ReluBackward0]
	137615233382288 -> 137615233382144
	137615233382288 [label=NativeBatchNormBackward0]
	137615233382384 -> 137615233382288
	137615233382384 [label=ConvolutionBackward0]
	137615235067344 -> 137615233382384
	137615235067344 [label=ReluBackward0]
	137615233382672 -> 137615235067344
	137615233382672 [label=MulBackward0]
	137615233382768 -> 137615233382672
	137615233382768 [label=MulBackward0]
	137615233382912 -> 137615233382768
	137615233382912 [label=AddBackward0]
	137615233383056 -> 137615233382912
	137615233383056 [label=NativeBatchNormBackward0]
	137615233383200 -> 137615233383056
	137615233383200 [label=ConvolutionBackward0]
	137615233383392 -> 137615233383200
	137615233383392 [label=ReluBackward0]
	137615233383536 -> 137615233383392
	137615233383536 [label=NativeBatchNormBackward0]
	137615233383632 -> 137615233383536
	137615233383632 [label=ConvolutionBackward0]
	137615233383824 -> 137615233383632
	137615233383824 [label=ReluBackward0]
	137615233383968 -> 137615233383824
	137615233383968 [label=NativeBatchNormBackward0]
	137615233384064 -> 137615233383968
	137615233384064 [label=ConvolutionBackward0]
	137615233383008 -> 137615233384064
	137615233383008 [label=ReluBackward0]
	137615233384352 -> 137615233383008
	137615233384352 [label=MulBackward0]
	137615233384400 -> 137615233384352
	137615233384400 [label=MulBackward0]
	137615233384656 -> 137615233384400
	137615233384656 [label=AddBackward0]
	137615233384800 -> 137615233384656
	137615233384800 [label=NativeBatchNormBackward0]
	137615233384944 -> 137615233384800
	137615233384944 [label=ConvolutionBackward0]
	137615233385136 -> 137615233384944
	137615233385136 [label=ReluBackward0]
	137615233385280 -> 137615233385136
	137615233385280 [label=NativeBatchNormBackward0]
	137615233385376 -> 137615233385280
	137615233385376 [label=ConvolutionBackward0]
	137615233385568 -> 137615233385376
	137615233385568 [label=ReluBackward0]
	137615233385712 -> 137615233385568
	137615233385712 [label=NativeBatchNormBackward0]
	137615233385808 -> 137615233385712
	137615233385808 [label=ConvolutionBackward0]
	137615233386000 -> 137615233385808
	137615233386000 [label=ReluBackward0]
	137615233386144 -> 137615233386000
	137615233386144 [label=MulBackward0]
	137615233386240 -> 137615233386144
	137615233386240 [label=MulBackward0]
	137615233386384 -> 137615233386240
	137615233386384 [label=AddBackward0]
	137615233386528 -> 137615233386384
	137615233386528 [label=NativeBatchNormBackward0]
	137615233386672 -> 137615233386528
	137615233386672 [label=ConvolutionBackward0]
	137615233386864 -> 137615233386672
	137615233386864 [label=ReluBackward0]
	137615233387008 -> 137615233386864
	137615233387008 [label=NativeBatchNormBackward0]
	137615233387104 -> 137615233387008
	137615233387104 [label=ConvolutionBackward0]
	137615233387296 -> 137615233387104
	137615233387296 [label=ReluBackward0]
	137615233387440 -> 137615233387296
	137615233387440 [label=NativeBatchNormBackward0]
	137615233387536 -> 137615233387440
	137615233387536 [label=ConvolutionBackward0]
	137615233386480 -> 137615233387536
	137615233386480 [label=ReluBackward0]
	137615233387824 -> 137615233386480
	137615233387824 [label=MulBackward0]
	137615233387920 -> 137615233387824
	137615233387920 [label=MulBackward0]
	137615233388064 -> 137615233387920
	137615233388064 [label=AddBackward0]
	137615233388208 -> 137615233388064
	137615233388208 [label=NativeBatchNormBackward0]
	137615233388352 -> 137615233388208
	137615233388352 [label=ConvolutionBackward0]
	137615233388544 -> 137615233388352
	137615233388544 [label=ReluBackward0]
	137615233388688 -> 137615233388544
	137615233388688 [label=NativeBatchNormBackward0]
	137615233388784 -> 137615233388688
	137615233388784 [label=ConvolutionBackward0]
	137615233388976 -> 137615233388784
	137615233388976 [label=ReluBackward0]
	137615233389120 -> 137615233388976
	137615233389120 [label=NativeBatchNormBackward0]
	137615233389216 -> 137615233389120
	137615233389216 [label=ConvolutionBackward0]
	137615233388160 -> 137615233389216
	137615233388160 [label=ReluBackward0]
	137615233389504 -> 137615233388160
	137615233389504 [label=MulBackward0]
	137615233389600 -> 137615233389504
	137615233389600 [label=MulBackward0]
	137615233389744 -> 137615233389600
	137615233389744 [label=AddBackward0]
	137615233389888 -> 137615233389744
	137615233389888 [label=NativeBatchNormBackward0]
	137615233390032 -> 137615233389888
	137615233390032 [label=ConvolutionBackward0]
	137615233390224 -> 137615233390032
	137615233390224 [label=ReluBackward0]
	137615233390368 -> 137615233390224
	137615233390368 [label=NativeBatchNormBackward0]
	137615233390464 -> 137615233390368
	137615233390464 [label=ConvolutionBackward0]
	137615234760816 -> 137615233390464
	137615234760816 [label=ReluBackward0]
	137615234726256 -> 137615234760816
	137615234726256 [label=NativeBatchNormBackward0]
	137615234725728 -> 137615234726256
	137615234725728 [label=ConvolutionBackward0]
	137615234726784 -> 137615234725728
	137615234726784 [label=ReluBackward0]
	137615234970864 -> 137615234726784
	137615234970864 [label=NativeBatchNormBackward0]
	137615234970816 -> 137615234970864
	137615234970816 [label=ConvolutionBackward0]
	137615233390704 -> 137615234970816
	137614705282560 [label="conv1.weight
 (64, 3, 3, 3)" fillcolor=lightblue]
	137614705282560 -> 137615233390704
	137615233390704 [label=AccumulateGrad]
	137615234970720 -> 137615234970864
	137614719688016 [label="bn1.weight
 (64)" fillcolor=lightblue]
	137614719688016 -> 137615234970720
	137615234970720 [label=AccumulateGrad]
	137615234971296 -> 137615234970864
	137614708741264 [label="bn1.bias
 (64)" fillcolor=lightblue]
	137614708741264 -> 137615234971296
	137615234971296 [label=AccumulateGrad]
	137615234726544 -> 137615234725728
	137614541086592 [label="layer1.0.conv1.weight
 (64, 64, 1, 1)" fillcolor=lightblue]
	137614541086592 -> 137615234726544
	137615234726544 [label=AccumulateGrad]
	137615234726640 -> 137615234726256
	137614541086752 [label="layer1.0.bn1.weight
 (64)" fillcolor=lightblue]
	137614541086752 -> 137615234726640
	137615234726640 [label=AccumulateGrad]
	137615234724624 -> 137615234726256
	137614541086672 [label="layer1.0.bn1.bias
 (64)" fillcolor=lightblue]
	137614541086672 -> 137615234724624
	137615234724624 [label=AccumulateGrad]
	137615234725248 -> 137615233390464
	137614541088432 [label="layer1.0.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	137614541088432 -> 137615234725248
	137615234725248 [label=AccumulateGrad]
	137615233390416 -> 137615233390368
	137614541089232 [label="layer1.0.bn2.weight
 (64)" fillcolor=lightblue]
	137614541089232 -> 137615233390416
	137615233390416 [label=AccumulateGrad]
	137615233390272 -> 137615233390368
	137614541088672 [label="layer1.0.bn2.bias
 (64)" fillcolor=lightblue]
	137614541088672 -> 137615233390272
	137615233390272 [label=AccumulateGrad]
	137615233390176 -> 137615233390032
	137614541089152 [label="layer1.0.conv3.weight
 (256, 64, 1, 1)" fillcolor=lightblue]
	137614541089152 -> 137615233390176
	137615233390176 [label=AccumulateGrad]
	137615233389984 -> 137615233389888
	137614541089312 [label="layer1.0.bn3.weight
 (256)" fillcolor=lightblue]
	137614541089312 -> 137615233389984
	137615233389984 [label=AccumulateGrad]
	137615233389936 -> 137615233389888
	137614541089872 [label="layer1.0.bn3.bias
 (256)" fillcolor=lightblue]
	137614541089872 -> 137615233389936
	137615233389936 [label=AccumulateGrad]
	137615233389840 -> 137615233389744
	137615233389840 [label=NativeBatchNormBackward0]
	137615234726400 -> 137615233389840
	137615234726400 [label=ConvolutionBackward0]
	137615234726784 -> 137615234726400
	137615234970912 -> 137615234726400
	137614707714672 [label="layer1.0.downsample.0.weight
 (256, 64, 1, 1)" fillcolor=lightblue]
	137614707714672 -> 137615234970912
	137615234970912 [label=AccumulateGrad]
	137615234726352 -> 137615233389840
	137614713176848 [label="layer1.0.downsample.1.weight
 (256)" fillcolor=lightblue]
	137614713176848 -> 137615234726352
	137615234726352 [label=AccumulateGrad]
	137615234724528 -> 137615233389840
	137614541081632 [label="layer1.0.downsample.1.bias
 (256)" fillcolor=lightblue]
	137614541081632 -> 137615234724528
	137615234724528 [label=AccumulateGrad]
	137615233389696 -> 137615233389600
	137615233389696 [label=SigmoidBackward0]
	137615234971392 -> 137615233389696
	137615234971392 [label=AddBackward0]
	137615233390320 -> 137615234971392
	137615233390320 [label=ConvolutionBackward0]
	137615233390560 -> 137615233390320
	137615233390560 [label=ReluBackward0]
	137615233390848 -> 137615233390560
	137615233390848 [label=ConvolutionBackward0]
	137615233390944 -> 137615233390848
	137615233390944 [label=MeanBackward1]
	137615233389744 -> 137615233390944
	137615233390896 -> 137615233390848
	137614704477264 [label="layer1.0.cbam.ca.mlp.0.weight
 (16, 256, 1, 1)" fillcolor=lightblue]
	137614704477264 -> 137615233390896
	137615233390896 [label=AccumulateGrad]
	137615233390752 -> 137615233390848
	137614704479664 [label="layer1.0.cbam.ca.mlp.0.bias
 (16)" fillcolor=lightblue]
	137614704479664 -> 137615233390752
	137615233390752 [label=AccumulateGrad]
	137615233390656 -> 137615233390320
	137614710675696 [label="layer1.0.cbam.ca.mlp.2.weight
 (256, 16, 1, 1)" fillcolor=lightblue]
	137614710675696 -> 137615233390656
	137615233390656 [label=AccumulateGrad]
	137615233390080 -> 137615233390320
	137614705282640 [label="layer1.0.cbam.ca.mlp.2.bias
 (256)" fillcolor=lightblue]
	137614705282640 -> 137615233390080
	137615233390080 [label=AccumulateGrad]
	137615233390512 -> 137615234971392
	137615233390512 [label=ConvolutionBackward0]
	137615233390992 -> 137615233390512
	137615233390992 [label=ReluBackward0]
	137615233391088 -> 137615233390992
	137615233391088 [label=ConvolutionBackward0]
	137615233391184 -> 137615233391088
	137615233391184 [label=AdaptiveMaxPool2DBackward0]
	137615233389744 -> 137615233391184
	137615233390896 -> 137615233391088
	137615233390752 -> 137615233391088
	137615233390656 -> 137615233390512
	137615233390080 -> 137615233390512
	137615233389552 -> 137615233389504
	137615233389552 [label=SigmoidBackward0]
	137615233389792 -> 137615233389552
	137615233389792 [label=NativeBatchNormBackward0]
	137615233391136 -> 137615233389792
	137615233391136 [label=ConvolutionBackward0]
	137615233390800 -> 137615233391136
	137615233390800 [label=CatBackward0]
	137615233391472 -> 137615233390800
	137615233391472 [label=MaxBackward0]
	137615233389600 -> 137615233391472
	137615233391424 -> 137615233390800
	137615233391424 [label=MeanBackward1]
	137615233389600 -> 137615233391424
	137615233391232 -> 137615233391136
	137614705282720 [label="layer1.0.cbam.sa.conv.weight
 (1, 2, 7, 7)" fillcolor=lightblue]
	137614705282720 -> 137615233391232
	137615233391232 [label=AccumulateGrad]
	137615233390608 -> 137615233389792
	137614705282800 [label="layer1.0.cbam.sa.bn.weight
 (1)" fillcolor=lightblue]
	137614705282800 -> 137615233390608
	137615233390608 [label=AccumulateGrad]
	137615233389648 -> 137615233389792
	137614705282880 [label="layer1.0.cbam.sa.bn.bias
 (1)" fillcolor=lightblue]
	137614705282880 -> 137615233389648
	137615233389648 [label=AccumulateGrad]
	137615233389408 -> 137615233389216
	137614541088912 [label="layer1.1.conv1.weight
 (64, 256, 1, 1)" fillcolor=lightblue]
	137614541088912 -> 137615233389408
	137615233389408 [label=AccumulateGrad]
	137615233389168 -> 137615233389120
	137614541089072 [label="layer1.1.bn1.weight
 (64)" fillcolor=lightblue]
	137614541089072 -> 137615233389168
	137615233389168 [label=AccumulateGrad]
	137615233389024 -> 137615233389120
	137614541089712 [label="layer1.1.bn1.bias
 (64)" fillcolor=lightblue]
	137614541089712 -> 137615233389024
	137615233389024 [label=AccumulateGrad]
	137615233388928 -> 137615233388784
	137614723017088 [label="layer1.1.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	137614723017088 -> 137615233388928
	137615233388928 [label=AccumulateGrad]
	137615233388736 -> 137615233388688
	137614541087312 [label="layer1.1.bn2.weight
 (64)" fillcolor=lightblue]
	137614541087312 -> 137615233388736
	137615233388736 [label=AccumulateGrad]
	137615233388592 -> 137615233388688
	137614723018848 [label="layer1.1.bn2.bias
 (64)" fillcolor=lightblue]
	137614723018848 -> 137615233388592
	137615233388592 [label=AccumulateGrad]
	137615233388496 -> 137615233388352
	137614723022288 [label="layer1.1.conv3.weight
 (256, 64, 1, 1)" fillcolor=lightblue]
	137614723022288 -> 137615233388496
	137615233388496 [label=AccumulateGrad]
	137615233388304 -> 137615233388208
	137614723022528 [label="layer1.1.bn3.weight
 (256)" fillcolor=lightblue]
	137614723022528 -> 137615233388304
	137615233388304 [label=AccumulateGrad]
	137615233388256 -> 137615233388208
	137614723021888 [label="layer1.1.bn3.bias
 (256)" fillcolor=lightblue]
	137614723021888 -> 137615233388256
	137615233388256 [label=AccumulateGrad]
	137615233388160 -> 137615233388064
	137615233388016 -> 137615233387920
	137615233388016 [label=SigmoidBackward0]
	137615233388448 -> 137615233388016
	137615233388448 [label=AddBackward0]
	137615233388832 -> 137615233388448
	137615233388832 [label=ConvolutionBackward0]
	137615233389264 -> 137615233388832
	137615233389264 [label=ReluBackward0]
	137615233389456 -> 137615233389264
	137615233389456 [label=ConvolutionBackward0]
	137615233391280 -> 137615233389456
	137615233391280 [label=MeanBackward1]
	137615233388064 -> 137615233391280
	137615233391040 -> 137615233389456
	137614705283200 [label="layer1.1.cbam.ca.mlp.0.weight
 (16, 256, 1, 1)" fillcolor=lightblue]
	137614705283200 -> 137615233391040
	137615233391040 [label=AccumulateGrad]
	137615233390128 -> 137615233389456
	137614705283520 [label="layer1.1.cbam.ca.mlp.0.bias
 (16)" fillcolor=lightblue]
	137614705283520 -> 137615233390128
	137615233390128 [label=AccumulateGrad]
	137615233389360 -> 137615233388832
	137614705283280 [label="layer1.1.cbam.ca.mlp.2.weight
 (256, 16, 1, 1)" fillcolor=lightblue]
	137614705283280 -> 137615233389360
	137615233389360 [label=AccumulateGrad]
	137615233388640 -> 137615233388832
	137614705283440 [label="layer1.1.cbam.ca.mlp.2.bias
 (256)" fillcolor=lightblue]
	137614705283440 -> 137615233388640
	137615233388640 [label=AccumulateGrad]
	137615233388880 -> 137615233388448
	137615233388880 [label=ConvolutionBackward0]
	137615233391568 -> 137615233388880
	137615233391568 [label=ReluBackward0]
	137615233391328 -> 137615233391568
	137615233391328 [label=ConvolutionBackward0]
	137615233391664 -> 137615233391328
	137615233391664 [label=AdaptiveMaxPool2DBackward0]
	137615233388064 -> 137615233391664
	137615233391040 -> 137615233391328
	137615233390128 -> 137615233391328
	137615233389360 -> 137615233388880
	137615233388640 -> 137615233388880
	137615233387872 -> 137615233387824
	137615233387872 [label=SigmoidBackward0]
	137615233388112 -> 137615233387872
	137615233388112 [label=NativeBatchNormBackward0]
	137615233391616 -> 137615233388112
	137615233391616 [label=ConvolutionBackward0]
	137615233389072 -> 137615233391616
	137615233389072 [label=CatBackward0]
	137615233391904 -> 137615233389072
	137615233391904 [label=MaxBackward0]
	137615233387920 -> 137615233391904
	137615233391856 -> 137615233389072
	137615233391856 [label=MeanBackward1]
	137615233387920 -> 137615233391856
	137615233391712 -> 137615233391616
	137614705283760 [label="layer1.1.cbam.sa.conv.weight
 (1, 2, 7, 7)" fillcolor=lightblue]
	137614705283760 -> 137615233391712
	137615233391712 [label=AccumulateGrad]
	137615233389312 -> 137615233388112
	137614705283840 [label="layer1.1.cbam.sa.bn.weight
 (1)" fillcolor=lightblue]
	137614705283840 -> 137615233389312
	137615233389312 [label=AccumulateGrad]
	137615233387968 -> 137615233388112
	137614705283920 [label="layer1.1.cbam.sa.bn.bias
 (1)" fillcolor=lightblue]
	137614705283920 -> 137615233387968
	137615233387968 [label=AccumulateGrad]
	137615233387728 -> 137615233387536
	137614723021568 [label="layer1.2.conv1.weight
 (64, 256, 1, 1)" fillcolor=lightblue]
	137614723021568 -> 137615233387728
	137615233387728 [label=AccumulateGrad]
	137615233387488 -> 137615233387440
	137614723018768 [label="layer1.2.bn1.weight
 (64)" fillcolor=lightblue]
	137614723018768 -> 137615233387488
	137615233387488 [label=AccumulateGrad]
	137615233387344 -> 137615233387440
	137614723019088 [label="layer1.2.bn1.bias
 (64)" fillcolor=lightblue]
	137614723019088 -> 137615233387344
	137615233387344 [label=AccumulateGrad]
	137615233387248 -> 137615233387104
	137614723019248 [label="layer1.2.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	137614723019248 -> 137615233387248
	137615233387248 [label=AccumulateGrad]
	137615233387056 -> 137615233387008
	137614723021408 [label="layer1.2.bn2.weight
 (64)" fillcolor=lightblue]
	137614723021408 -> 137615233387056
	137615233387056 [label=AccumulateGrad]
	137615233386912 -> 137615233387008
	137614723019168 [label="layer1.2.bn2.bias
 (64)" fillcolor=lightblue]
	137614723019168 -> 137615233386912
	137615233386912 [label=AccumulateGrad]
	137615233386816 -> 137615233386672
	137614723017328 [label="layer1.2.conv3.weight
 (256, 64, 1, 1)" fillcolor=lightblue]
	137614723017328 -> 137615233386816
	137615233386816 [label=AccumulateGrad]
	137615233386624 -> 137615233386528
	137614723017488 [label="layer1.2.bn3.weight
 (256)" fillcolor=lightblue]
	137614723017488 -> 137615233386624
	137615233386624 [label=AccumulateGrad]
	137615233386576 -> 137615233386528
	137614723018288 [label="layer1.2.bn3.bias
 (256)" fillcolor=lightblue]
	137614723018288 -> 137615233386576
	137615233386576 [label=AccumulateGrad]
	137615233386480 -> 137615233386384
	137615233386336 -> 137615233386240
	137615233386336 [label=SigmoidBackward0]
	137615233386768 -> 137615233386336
	137615233386768 [label=AddBackward0]
	137615233387152 -> 137615233386768
	137615233387152 [label=ConvolutionBackward0]
	137615233387584 -> 137615233387152
	137615233387584 [label=ReluBackward0]
	137615233387776 -> 137615233387584
	137615233387776 [label=ConvolutionBackward0]
	137615233391760 -> 137615233387776
	137615233391760 [label=MeanBackward1]
	137615233386384 -> 137615233391760
	137615233391520 -> 137615233387776
	137614705284560 [label="layer1.2.cbam.ca.mlp.0.weight
 (16, 256, 1, 1)" fillcolor=lightblue]
	137614705284560 -> 137615233391520
	137615233391520 [label=AccumulateGrad]
	137615233388400 -> 137615233387776
	137614705284320 [label="layer1.2.cbam.ca.mlp.0.bias
 (16)" fillcolor=lightblue]
	137614705284320 -> 137615233388400
	137615233388400 [label=AccumulateGrad]
	137615233387680 -> 137615233387152
	137614705284400 [label="layer1.2.cbam.ca.mlp.2.weight
 (256, 16, 1, 1)" fillcolor=lightblue]
	137614705284400 -> 137615233387680
	137615233387680 [label=AccumulateGrad]
	137615233386960 -> 137615233387152
	137614705284640 [label="layer1.2.cbam.ca.mlp.2.bias
 (256)" fillcolor=lightblue]
	137614705284640 -> 137615233386960
	137615233386960 [label=AccumulateGrad]
	137615233387200 -> 137615233386768
	137615233387200 [label=ConvolutionBackward0]
	137615233392000 -> 137615233387200
	137615233392000 [label=ReluBackward0]
	137615233391808 -> 137615233392000
	137615233391808 [label=ConvolutionBackward0]
	137615233392096 -> 137615233391808
	137615233392096 [label=AdaptiveMaxPool2DBackward0]
	137615233386384 -> 137615233392096
	137615233391520 -> 137615233391808
	137615233388400 -> 137615233391808
	137615233387680 -> 137615233387200
	137615233386960 -> 137615233387200
	137615233386192 -> 137615233386144
	137615233386192 [label=SigmoidBackward0]
	137615233386432 -> 137615233386192
	137615233386432 [label=NativeBatchNormBackward0]
	137615233392048 -> 137615233386432
	137615233392048 [label=ConvolutionBackward0]
	137615233387392 -> 137615233392048
	137615233387392 [label=CatBackward0]
	137615233392336 -> 137615233387392
	137615233392336 [label=MaxBackward0]
	137615233386240 -> 137615233392336
	137615233392288 -> 137615233387392
	137615233392288 [label=MeanBackward1]
	137615233386240 -> 137615233392288
	137615233392144 -> 137615233392048
	137614705284880 [label="layer1.2.cbam.sa.conv.weight
 (1, 2, 7, 7)" fillcolor=lightblue]
	137614705284880 -> 137615233392144
	137615233392144 [label=AccumulateGrad]
	137615233387632 -> 137615233386432
	137614705284960 [label="layer1.2.cbam.sa.bn.weight
 (1)" fillcolor=lightblue]
	137614705284960 -> 137615233387632
	137615233387632 [label=AccumulateGrad]
	137615233386288 -> 137615233386432
	137614705285040 [label="layer1.2.cbam.sa.bn.bias
 (1)" fillcolor=lightblue]
	137614705285040 -> 137615233386288
	137615233386288 [label=AccumulateGrad]
	137615233385952 -> 137615233385808
	137614723017008 [label="layer2.0.conv1.weight
 (128, 256, 1, 1)" fillcolor=lightblue]
	137614723017008 -> 137615233385952
	137615233385952 [label=AccumulateGrad]
	137615233385760 -> 137615233385712
	137614723015408 [label="layer2.0.bn1.weight
 (128)" fillcolor=lightblue]
	137614723015408 -> 137615233385760
	137615233385760 [label=AccumulateGrad]
	137615233385616 -> 137615233385712
	137614723015328 [label="layer2.0.bn1.bias
 (128)" fillcolor=lightblue]
	137614723015328 -> 137615233385616
	137615233385616 [label=AccumulateGrad]
	137615233385520 -> 137615233385376
	137614723016848 [label="layer2.0.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	137614723016848 -> 137615233385520
	137615233385520 [label=AccumulateGrad]
	137615233385328 -> 137615233385280
	137614723015888 [label="layer2.0.bn2.weight
 (128)" fillcolor=lightblue]
	137614723015888 -> 137615233385328
	137615233385328 [label=AccumulateGrad]
	137615233385184 -> 137615233385280
	137614723015168 [label="layer2.0.bn2.bias
 (128)" fillcolor=lightblue]
	137614723015168 -> 137615233385184
	137615233385184 [label=AccumulateGrad]
	137615233385088 -> 137615233384944
	137614723012448 [label="layer2.0.conv3.weight
 (512, 128, 1, 1)" fillcolor=lightblue]
	137614723012448 -> 137615233385088
	137615233385088 [label=AccumulateGrad]
	137615233384896 -> 137615233384800
	137614723014928 [label="layer2.0.bn3.weight
 (512)" fillcolor=lightblue]
	137614723014928 -> 137615233384896
	137615233384896 [label=AccumulateGrad]
	137615233384848 -> 137615233384800
	137614723009168 [label="layer2.0.bn3.bias
 (512)" fillcolor=lightblue]
	137614723009168 -> 137615233384848
	137615233384848 [label=AccumulateGrad]
	137615233384752 -> 137615233384656
	137615233384752 [label=NativeBatchNormBackward0]
	137615233385472 -> 137615233384752
	137615233385472 [label=ConvolutionBackward0]
	137615233386000 -> 137615233385472
	137615233385856 -> 137615233385472
	137614723021168 [label="layer2.0.downsample.0.weight
 (512, 256, 1, 1)" fillcolor=lightblue]
	137614723021168 -> 137615233385856
	137615233385856 [label=AccumulateGrad]
	137615233385040 -> 137615233384752
	137614723020448 [label="layer2.0.downsample.1.weight
 (512)" fillcolor=lightblue]
	137614723020448 -> 137615233385040
	137615233385040 [label=AccumulateGrad]
	137615233384992 -> 137615233384752
	137614723019408 [label="layer2.0.downsample.1.bias
 (512)" fillcolor=lightblue]
	137614723019408 -> 137615233384992
	137615233384992 [label=AccumulateGrad]
	137615233384608 -> 137615233384400
	137615233384608 [label=SigmoidBackward0]
	137615233385904 -> 137615233384608
	137615233385904 [label=AddBackward0]
	137615233385664 -> 137615233385904
	137615233385664 [label=ConvolutionBackward0]
	137615233386096 -> 137615233385664
	137615233386096 [label=ReluBackward0]
	137615233392384 -> 137615233386096
	137615233392384 [label=ConvolutionBackward0]
	137615233392480 -> 137615233392384
	137615233392480 [label=MeanBackward1]
	137615233384656 -> 137615233392480
	137615233392240 -> 137615233392384
	137614705285680 [label="layer2.0.cbam.ca.mlp.0.weight
 (32, 512, 1, 1)" fillcolor=lightblue]
	137614705285680 -> 137615233392240
	137615233392240 [label=AccumulateGrad]
	137615233392192 -> 137615233392384
	137614705285440 [label="layer2.0.cbam.ca.mlp.0.bias
 (32)" fillcolor=lightblue]
	137614705285440 -> 137615233392192
	137615233392192 [label=AccumulateGrad]
	137615233386048 -> 137615233385664
	137614705285520 [label="layer2.0.cbam.ca.mlp.2.weight
 (512, 32, 1, 1)" fillcolor=lightblue]
	137614705285520 -> 137615233386048
	137615233386048 [label=AccumulateGrad]
	137615233385232 -> 137615233385664
	137614705285760 [label="layer2.0.cbam.ca.mlp.2.bias
 (512)" fillcolor=lightblue]
	137614705285760 -> 137615233385232
	137615233385232 [label=AccumulateGrad]
	137615233386720 -> 137615233385904
	137615233386720 [label=ConvolutionBackward0]
	137615233392528 -> 137615233386720
	137615233392528 [label=ReluBackward0]
	137615233392624 -> 137615233392528
	137615233392624 [label=ConvolutionBackward0]
	137615233392720 -> 137615233392624
	137615233392720 [label=AdaptiveMaxPool2DBackward0]
	137615233384656 -> 137615233392720
	137615233392240 -> 137615233392624
	137615233392192 -> 137615233392624
	137615233386048 -> 137615233386720
	137615233385232 -> 137615233386720
	137615233384160 -> 137615233384352
	137615233384160 [label=SigmoidBackward0]
	137615233384704 -> 137615233384160
	137615233384704 [label=NativeBatchNormBackward0]
	137615233392672 -> 137615233384704
	137615233392672 [label=ConvolutionBackward0]
	137615233391952 -> 137615233392672
	137615233391952 [label=CatBackward0]
	137615233392960 -> 137615233391952
	137615233392960 [label=MaxBackward0]
	137615233384400 -> 137615233392960
	137615233392912 -> 137615233391952
	137615233392912 [label=MeanBackward1]
	137615233384400 -> 137615233392912
	137615233392768 -> 137615233392672
	137614705286000 [label="layer2.0.cbam.sa.conv.weight
 (1, 2, 7, 7)" fillcolor=lightblue]
	137614705286000 -> 137615233392768
	137615233392768 [label=AccumulateGrad]
	137615233392432 -> 137615233384704
	137614705286080 [label="layer2.0.cbam.sa.bn.weight
 (1)" fillcolor=lightblue]
	137614705286080 -> 137615233392432
	137615233392432 [label=AccumulateGrad]
	137615233384560 -> 137615233384704
	137614705286160 [label="layer2.0.cbam.sa.bn.bias
 (1)" fillcolor=lightblue]
	137614705286160 -> 137615233384560
	137615233384560 [label=AccumulateGrad]
	137615233384256 -> 137615233384064
	137614723007328 [label="layer2.1.conv1.weight
 (128, 512, 1, 1)" fillcolor=lightblue]
	137614723007328 -> 137615233384256
	137615233384256 [label=AccumulateGrad]
	137615233384016 -> 137615233383968
	137614723007168 [label="layer2.1.bn1.weight
 (128)" fillcolor=lightblue]
	137614723007168 -> 137615233384016
	137615233384016 [label=AccumulateGrad]
	137615233383872 -> 137615233383968
	137614723007088 [label="layer2.1.bn1.bias
 (128)" fillcolor=lightblue]
	137614723007088 -> 137615233383872
	137615233383872 [label=AccumulateGrad]
	137615233383776 -> 137615233383632
	137614723007408 [label="layer2.1.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	137614723007408 -> 137615233383776
	137615233383776 [label=AccumulateGrad]
	137615233383584 -> 137615233383536
	137614723007568 [label="layer2.1.bn2.weight
 (128)" fillcolor=lightblue]
	137614723007568 -> 137615233383584
	137615233383584 [label=AccumulateGrad]
	137615233383440 -> 137615233383536
	137614723008208 [label="layer2.1.bn2.bias
 (128)" fillcolor=lightblue]
	137614723008208 -> 137615233383440
	137615233383440 [label=AccumulateGrad]
	137615233383344 -> 137615233383200
	137614723010928 [label="layer2.1.conv3.weight
 (512, 128, 1, 1)" fillcolor=lightblue]
	137614723010928 -> 137615233383344
	137615233383344 [label=AccumulateGrad]
	137615233383152 -> 137615233383056
	137614723009568 [label="layer2.1.bn3.weight
 (512)" fillcolor=lightblue]
	137614723009568 -> 137615233383152
	137615233383152 [label=AccumulateGrad]
	137615233383104 -> 137615233383056
	137614723008368 [label="layer2.1.bn3.bias
 (512)" fillcolor=lightblue]
	137614723008368 -> 137615233383104
	137615233383104 [label=AccumulateGrad]
	137615233383008 -> 137615233382912
	137615233382864 -> 137615233382768
	137615233382864 [label=SigmoidBackward0]
	137615233383296 -> 137615233382864
	137615233383296 [label=AddBackward0]
	137615233383680 -> 137615233383296
	137615233383680 [label=ConvolutionBackward0]
	137615233384112 -> 137615233383680
	137615233384112 [label=ReluBackward0]
	137615233384304 -> 137615233384112
	137615233384304 [label=ConvolutionBackward0]
	137615233392816 -> 137615233384304
	137615233392816 [label=MeanBackward1]
	137615233382912 -> 137615233392816
	137615233392576 -> 137615233384304
	137614705286720 [label="layer2.1.cbam.ca.mlp.0.weight
 (32, 512, 1, 1)" fillcolor=lightblue]
	137614705286720 -> 137615233392576
	137615233392576 [label=AccumulateGrad]
	137615233385424 -> 137615233384304
	137614705286800 [label="layer2.1.cbam.ca.mlp.0.bias
 (32)" fillcolor=lightblue]
	137614705286800 -> 137615233385424
	137615233385424 [label=AccumulateGrad]
	137615233384208 -> 137615233383680
	137614705286560 [label="layer2.1.cbam.ca.mlp.2.weight
 (512, 32, 1, 1)" fillcolor=lightblue]
	137614705286560 -> 137615233384208
	137615233384208 [label=AccumulateGrad]
	137615233383488 -> 137615233383680
	137614705286880 [label="layer2.1.cbam.ca.mlp.2.bias
 (512)" fillcolor=lightblue]
	137614705286880 -> 137615233383488
	137615233383488 [label=AccumulateGrad]
	137615233383728 -> 137615233383296
	137615233383728 [label=ConvolutionBackward0]
	137615233383920 -> 137615233383728
	137615233383920 [label=ReluBackward0]
	137615233392864 -> 137615233383920
	137615233392864 [label=ConvolutionBackward0]
	137615233393152 -> 137615233392864
	137615233393152 [label=AdaptiveMaxPool2DBackward0]
	137615233382912 -> 137615233393152
	137615233392576 -> 137615233392864
	137615233385424 -> 137615233392864
	137615233384208 -> 137615233383728
	137615233383488 -> 137615233383728
	137615233382720 -> 137615233382672
	137615233382720 [label=SigmoidBackward0]
	137615233382960 -> 137615233382720
	137615233382960 [label=NativeBatchNormBackward0]
	137615233382816 -> 137615233382960
	137615233382816 [label=ConvolutionBackward0]
	137615233384512 -> 137615233382816
	137615233384512 [label=CatBackward0]
	137615233393392 -> 137615233384512
	137615233393392 [label=MaxBackward0]
	137615233382768 -> 137615233393392
	137615233393344 -> 137615233384512
	137615233393344 [label=MeanBackward1]
	137615233382768 -> 137615233393344
	137615233393200 -> 137615233382816
	137614705287120 [label="layer2.1.cbam.sa.conv.weight
 (1, 2, 7, 7)" fillcolor=lightblue]
	137614705287120 -> 137615233393200
	137615233393200 [label=AccumulateGrad]
	137615233393104 -> 137615233382960
	137614705287200 [label="layer2.1.cbam.sa.bn.weight
 (1)" fillcolor=lightblue]
	137614705287200 -> 137615233393104
	137615233393104 [label=AccumulateGrad]
	137615233393056 -> 137615233382960
	137614705287280 [label="layer2.1.cbam.sa.bn.bias
 (1)" fillcolor=lightblue]
	137614705287280 -> 137615233393056
	137615233393056 [label=AccumulateGrad]
	137615233382576 -> 137615233382384
	137614723011728 [label="layer2.2.conv1.weight
 (128, 512, 1, 1)" fillcolor=lightblue]
	137614723011728 -> 137615233382576
	137615233382576 [label=AccumulateGrad]
	137615233382336 -> 137615233382288
	137614723010288 [label="layer2.2.bn1.weight
 (128)" fillcolor=lightblue]
	137614723010288 -> 137615233382336
	137615233382336 [label=AccumulateGrad]
	137615233382192 -> 137615233382288
	137614723006768 [label="layer2.2.bn1.bias
 (128)" fillcolor=lightblue]
	137614723006768 -> 137615233382192
	137615233382192 [label=AccumulateGrad]
	137615233382096 -> 137615233381952
	137614722889216 [label="layer2.2.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	137614722889216 -> 137615233382096
	137615233382096 [label=AccumulateGrad]
	137615233381904 -> 137615233381856
	137614722888416 [label="layer2.2.bn2.weight
 (128)" fillcolor=lightblue]
	137614722888416 -> 137615233381904
	137615233381904 [label=AccumulateGrad]
	137615233381760 -> 137615233381856
	137614722889536 [label="layer2.2.bn2.bias
 (128)" fillcolor=lightblue]
	137614722889536 -> 137615233381760
	137615233381760 [label=AccumulateGrad]
	137615233381664 -> 137615235067488
	137614722890496 [label="layer2.2.conv3.weight
 (512, 128, 1, 1)" fillcolor=lightblue]
	137614722890496 -> 137615233381664
	137615233381664 [label=AccumulateGrad]
	137615235067440 -> 137615235067392
	137614722889936 [label="layer2.2.bn3.weight
 (512)" fillcolor=lightblue]
	137614722889936 -> 137615235067440
	137615235067440 [label=AccumulateGrad]
	137615233381520 -> 137615235067392
	137614722890336 [label="layer2.2.bn3.bias
 (512)" fillcolor=lightblue]
	137614722890336 -> 137615233381520
	137615233381520 [label=AccumulateGrad]
	137615235067344 -> 137615235067248
	137615235067200 -> 137615235067104
	137615235067200 [label=SigmoidBackward0]
	137615235067296 -> 137615235067200
	137615235067296 [label=AddBackward0]
	137615233382000 -> 137615235067296
	137615233382000 [label=ConvolutionBackward0]
	137615233382432 -> 137615233382000
	137615233382432 [label=ReluBackward0]
	137615233382624 -> 137615233382432
	137615233382624 [label=ConvolutionBackward0]
	137615233383248 -> 137615233382624
	137615233383248 [label=MeanBackward1]
	137615235067248 -> 137615233383248
	137615233393248 -> 137615233382624
	137614705287920 [label="layer2.2.cbam.ca.mlp.0.weight
 (32, 512, 1, 1)" fillcolor=lightblue]
	137614705287920 -> 137615233393248
	137615233393248 [label=AccumulateGrad]
	137615233393008 -> 137615233382624
	137614705287680 [label="layer2.2.cbam.ca.mlp.0.bias
 (32)" fillcolor=lightblue]
	137614705287680 -> 137615233393008
	137615233393008 [label=AccumulateGrad]
	137615233382528 -> 137615233382000
	137614705287760 [label="layer2.2.cbam.ca.mlp.2.weight
 (512, 32, 1, 1)" fillcolor=lightblue]
	137614705287760 -> 137615233382528
	137615233382528 [label=AccumulateGrad]
	137615233381808 -> 137615233382000
	137614705288000 [label="layer2.2.cbam.ca.mlp.2.bias
 (512)" fillcolor=lightblue]
	137614705288000 -> 137615233381808
	137615233381808 [label=AccumulateGrad]
	137615233382048 -> 137615235067296
	137615233382048 [label=ConvolutionBackward0]
	137615233382480 -> 137615233382048
	137615233382480 [label=ReluBackward0]
	137615233393296 -> 137615233382480
	137615233393296 [label=ConvolutionBackward0]
	137615233393584 -> 137615233393296
	137615233393584 [label=AdaptiveMaxPool2DBackward0]
	137615235067248 -> 137615233393584
	137615233393248 -> 137615233393296
	137615233393008 -> 137615233393296
	137615233382528 -> 137615233382048
	137615233381808 -> 137615233382048
	137615235067056 -> 137615235067008
	137615235067056 [label=SigmoidBackward0]
	137615235067152 -> 137615235067056
	137615235067152 [label=NativeBatchNormBackward0]
	137615233382240 -> 137615235067152
	137615233382240 [label=ConvolutionBackward0]
	137615233393488 -> 137615233382240
	137615233393488 [label=CatBackward0]
	137615233393824 -> 137615233393488
	137615233393824 [label=MaxBackward0]
	137615235067104 -> 137615233393824
	137615233393776 -> 137615233393488
	137615233393776 [label=MeanBackward1]
	137615235067104 -> 137615233393776
	137615233393632 -> 137615233382240
	137614705288240 [label="layer2.2.cbam.sa.conv.weight
 (1, 2, 7, 7)" fillcolor=lightblue]
	137614705288240 -> 137615233393632
	137615233393632 [label=AccumulateGrad]
	137615233381616 -> 137615235067152
	137614705288320 [label="layer2.2.cbam.sa.bn.weight
 (1)" fillcolor=lightblue]
	137614705288320 -> 137615233381616
	137615233381616 [label=AccumulateGrad]
	137615233393536 -> 137615235067152
	137614705288400 [label="layer2.2.cbam.sa.bn.bias
 (1)" fillcolor=lightblue]
	137614705288400 -> 137615233393536
	137615233393536 [label=AccumulateGrad]
	137615235066912 -> 137615235066720
	137614722890816 [label="layer2.3.conv1.weight
 (128, 512, 1, 1)" fillcolor=lightblue]
	137614722890816 -> 137615235066912
	137615235066912 [label=AccumulateGrad]
	137615235066672 -> 137615235066624
	137614722890576 [label="layer2.3.bn1.weight
 (128)" fillcolor=lightblue]
	137614722890576 -> 137615235066672
	137615235066672 [label=AccumulateGrad]
	137615235066528 -> 137615235066624
	137614722891616 [label="layer2.3.bn1.bias
 (128)" fillcolor=lightblue]
	137614722891616 -> 137615235066528
	137615235066528 [label=AccumulateGrad]
	137615235066432 -> 137615235066288
	137614722888976 [label="layer2.3.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	137614722888976 -> 137615235066432
	137615235066432 [label=AccumulateGrad]
	137615235066240 -> 137615235066192
	137614722889056 [label="layer2.3.bn2.weight
 (128)" fillcolor=lightblue]
	137614722889056 -> 137615235066240
	137615235066240 [label=AccumulateGrad]
	137615235066096 -> 137615235066192
	137614722888896 [label="layer2.3.bn2.bias
 (128)" fillcolor=lightblue]
	137614722888896 -> 137615235066096
	137615235066096 [label=AccumulateGrad]
	137615235066000 -> 137615235065856
	137614722886416 [label="layer2.3.conv3.weight
 (512, 128, 1, 1)" fillcolor=lightblue]
	137614722886416 -> 137615235066000
	137615235066000 [label=AccumulateGrad]
	137615235065808 -> 137615235065712
	137614722886096 [label="layer2.3.bn3.weight
 (512)" fillcolor=lightblue]
	137614722886096 -> 137615235065808
	137615235065808 [label=AccumulateGrad]
	137615235065760 -> 137615235065712
	137614722886336 [label="layer2.3.bn3.bias
 (512)" fillcolor=lightblue]
	137614722886336 -> 137615235065760
	137615235065760 [label=AccumulateGrad]
	137615235065664 -> 137615235065568
	137615235065520 -> 137615235065424
	137615235065520 [label=SigmoidBackward0]
	137615233381568 -> 137615235065520
	137615233381568 [label=AddBackward0]
	137615235066336 -> 137615233381568
	137615235066336 [label=ConvolutionBackward0]
	137615235066768 -> 137615235066336
	137615235066768 [label=ReluBackward0]
	137615235066960 -> 137615235066768
	137615235066960 [label=ConvolutionBackward0]
	137615235066864 -> 137615235066960
	137615235066864 [label=MeanBackward1]
	137615235065568 -> 137615235066864
	137615233393680 -> 137615235066960
	137614705289040 [label="layer2.3.cbam.ca.mlp.0.weight
 (32, 512, 1, 1)" fillcolor=lightblue]
	137614705289040 -> 137615233393680
	137615233393680 [label=AccumulateGrad]
	137615233393440 -> 137615235066960
	137614705288800 [label="layer2.3.cbam.ca.mlp.0.bias
 (32)" fillcolor=lightblue]
	137614705288800 -> 137615233393440
	137615233393440 [label=AccumulateGrad]
	137615235066384 -> 137615235066336
	137614705288880 [label="layer2.3.cbam.ca.mlp.2.weight
 (512, 32, 1, 1)" fillcolor=lightblue]
	137614705288880 -> 137615235066384
	137615235066384 [label=AccumulateGrad]
	137615235066144 -> 137615235066336
	137614705289120 [label="layer2.3.cbam.ca.mlp.2.bias
 (512)" fillcolor=lightblue]
	137614705289120 -> 137615235066144
	137615235066144 [label=AccumulateGrad]
	137615235065952 -> 137615233381568
	137615235065952 [label=ConvolutionBackward0]
	137615235066816 -> 137615235065952
	137615235066816 [label=ReluBackward0]
	137615233393728 -> 137615235066816
	137615233393728 [label=ConvolutionBackward0]
	137615233394016 -> 137615233393728
	137615233394016 [label=AdaptiveMaxPool2DBackward0]
	137615235065568 -> 137615233394016
	137615233393680 -> 137615233393728
	137615233393440 -> 137615233393728
	137615235066384 -> 137615235065952
	137615235066144 -> 137615235065952
	137615235065376 -> 137615235065328
	137615235065376 [label=SigmoidBackward0]
	137615235065616 -> 137615235065376
	137615235065616 [label=NativeBatchNormBackward0]
	137615235066576 -> 137615235065616
	137615235066576 [label=ConvolutionBackward0]
	137615233393920 -> 137615235066576
	137615233393920 [label=CatBackward0]
	137615233394256 -> 137615233393920
	137615233394256 [label=MaxBackward0]
	137615235065424 -> 137615233394256
	137615233394208 -> 137615233393920
	137615233394208 [label=MeanBackward1]
	137615235065424 -> 137615233394208
	137615233394064 -> 137615235066576
	137614705289280 [label="layer2.3.cbam.sa.conv.weight
 (1, 2, 7, 7)" fillcolor=lightblue]
	137614705289280 -> 137615233394064
	137615233394064 [label=AccumulateGrad]
	137615235065472 -> 137615235065616
	137614705289360 [label="layer2.3.cbam.sa.bn.weight
 (1)" fillcolor=lightblue]
	137614705289360 -> 137615235065472
	137615235065472 [label=AccumulateGrad]
	137615233393968 -> 137615235065616
	137614705289440 [label="layer2.3.cbam.sa.bn.bias
 (1)" fillcolor=lightblue]
	137614705289440 -> 137615233393968
	137615233393968 [label=AccumulateGrad]
	137615235065136 -> 137615235064992
	137614722877856 [label="layer3.0.conv1.weight
 (256, 512, 1, 1)" fillcolor=lightblue]
	137614722877856 -> 137615235065136
	137615235065136 [label=AccumulateGrad]
	137615235064944 -> 137615235064896
	137614722877296 [label="layer3.0.bn1.weight
 (256)" fillcolor=lightblue]
	137614722877296 -> 137615235064944
	137615235064944 [label=AccumulateGrad]
	137615235064800 -> 137615235064896
	137614722876336 [label="layer3.0.bn1.bias
 (256)" fillcolor=lightblue]
	137614722876336 -> 137615235064800
	137615235064800 [label=AccumulateGrad]
	137615235064704 -> 137615235064560
	137614722879456 [label="layer3.0.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	137614722879456 -> 137615235064704
	137615235064704 [label=AccumulateGrad]
	137615235064512 -> 137615235064464
	137614722879056 [label="layer3.0.bn2.weight
 (256)" fillcolor=lightblue]
	137614722879056 -> 137615235064512
	137615235064512 [label=AccumulateGrad]
	137615235064368 -> 137615235064464
	137614722879536 [label="layer3.0.bn2.bias
 (256)" fillcolor=lightblue]
	137614722879536 -> 137615235064368
	137615235064368 [label=AccumulateGrad]
	137615235064272 -> 137615235064128
	137614722879696 [label="layer3.0.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	137614722879696 -> 137615235064272
	137615235064272 [label=AccumulateGrad]
	137615235064080 -> 137615235063984
	137614722880896 [label="layer3.0.bn3.weight
 (1024)" fillcolor=lightblue]
	137614722880896 -> 137615235064080
	137615235064080 [label=AccumulateGrad]
	137615235064032 -> 137615235063984
	137614722882336 [label="layer3.0.bn3.bias
 (1024)" fillcolor=lightblue]
	137614722882336 -> 137615235064032
	137615235064032 [label=AccumulateGrad]
	137615235063936 -> 137615235063840
	137615235063936 [label=NativeBatchNormBackward0]
	137615235064656 -> 137615235063936
	137615235064656 [label=ConvolutionBackward0]
	137615235065184 -> 137615235064656
	137615235065040 -> 137615235064656
	137614722876096 [label="layer3.0.downsample.0.weight
 (1024, 512, 1, 1)" fillcolor=lightblue]
	137614722876096 -> 137615235065040
	137615235065040 [label=AccumulateGrad]
	137615235064224 -> 137615235063936
	137614722875696 [label="layer3.0.downsample.1.weight
 (1024)" fillcolor=lightblue]
	137614722875696 -> 137615235064224
	137615235064224 [label=AccumulateGrad]
	137615235064176 -> 137615235063936
	137614722883456 [label="layer3.0.downsample.1.bias
 (1024)" fillcolor=lightblue]
	137614722883456 -> 137615235064176
	137615235064176 [label=AccumulateGrad]
	137615235063792 -> 137615235063696
	137615235063792 [label=SigmoidBackward0]
	137615235065088 -> 137615235063792
	137615235065088 [label=AddBackward0]
	137615235064848 -> 137615235065088
	137615235064848 [label=ConvolutionBackward0]
	137615235065280 -> 137615235064848
	137615235065280 [label=ReluBackward0]
	137615233394304 -> 137615235065280
	137615233394304 [label=ConvolutionBackward0]
	137615233394400 -> 137615233394304
	137615233394400 [label=MeanBackward1]
	137615235063840 -> 137615233394400
	137615233394160 -> 137615233394304
	137614705290080 [label="layer3.0.cbam.ca.mlp.0.weight
 (64, 1024, 1, 1)" fillcolor=lightblue]
	137614705290080 -> 137615233394160
	137615233394160 [label=AccumulateGrad]
	137615233394112 -> 137615233394304
	137614705289840 [label="layer3.0.cbam.ca.mlp.0.bias
 (64)" fillcolor=lightblue]
	137614705289840 -> 137615233394112
	137615233394112 [label=AccumulateGrad]
	137615235065232 -> 137615235064848
	137614705289920 [label="layer3.0.cbam.ca.mlp.2.weight
 (1024, 64, 1, 1)" fillcolor=lightblue]
	137614705289920 -> 137615235065232
	137615235065232 [label=AccumulateGrad]
	137615235064416 -> 137615235064848
	137614705290160 [label="layer3.0.cbam.ca.mlp.2.bias
 (1024)" fillcolor=lightblue]
	137614705290160 -> 137615235064416
	137615235064416 [label=AccumulateGrad]
	137615235065904 -> 137615235065088
	137615235065904 [label=ConvolutionBackward0]
	137615233394448 -> 137615235065904
	137615233394448 [label=ReluBackward0]
	137615233394544 -> 137615233394448
	137615233394544 [label=ConvolutionBackward0]
	137615233394640 -> 137615233394544
	137615233394640 [label=AdaptiveMaxPool2DBackward0]
	137615235063840 -> 137615233394640
	137615233394160 -> 137615233394544
	137615233394112 -> 137615233394544
	137615235065232 -> 137615235065904
	137615235064416 -> 137615235065904
	137615235063648 -> 137615235063600
	137615235063648 [label=SigmoidBackward0]
	137615235063888 -> 137615235063648
	137615235063888 [label=NativeBatchNormBackward0]
	137615235063744 -> 137615235063888
	137615235063744 [label=ConvolutionBackward0]
	137615233393872 -> 137615235063744
	137615233393872 [label=CatBackward0]
	137615233394880 -> 137615233393872
	137615233394880 [label=MaxBackward0]
	137615235063696 -> 137615233394880
	137615233394832 -> 137615233393872
	137615233394832 [label=MeanBackward1]
	137615235063696 -> 137615233394832
	137615233394688 -> 137615235063744
	137614705290320 [label="layer3.0.cbam.sa.conv.weight
 (1, 2, 7, 7)" fillcolor=lightblue]
	137614705290320 -> 137615233394688
	137615233394688 [label=AccumulateGrad]
	137615233394592 -> 137615235063888
	137614705290400 [label="layer3.0.cbam.sa.bn.weight
 (1)" fillcolor=lightblue]
	137614705290400 -> 137615233394592
	137615233394592 [label=AccumulateGrad]
	137615233394352 -> 137615235063888
	137614705290480 [label="layer3.0.cbam.sa.bn.bias
 (1)" fillcolor=lightblue]
	137614705290480 -> 137615233394352
	137615233394352 [label=AccumulateGrad]
	137615235063504 -> 137615235063312
	137614722879616 [label="layer3.1.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	137614722879616 -> 137615235063504
	137615235063504 [label=AccumulateGrad]
	137615235063264 -> 137615235063216
	137614722883056 [label="layer3.1.bn1.weight
 (256)" fillcolor=lightblue]
	137614722883056 -> 137615235063264
	137615235063264 [label=AccumulateGrad]
	137615235063120 -> 137615235063216
	137614722875776 [label="layer3.1.bn1.bias
 (256)" fillcolor=lightblue]
	137614722875776 -> 137615235063120
	137615235063120 [label=AccumulateGrad]
	137615235063024 -> 137615235062880
	137614722877536 [label="layer3.1.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	137614722877536 -> 137615235063024
	137615235063024 [label=AccumulateGrad]
	137615235062832 -> 137615235062784
	137614722877456 [label="layer3.1.bn2.weight
 (256)" fillcolor=lightblue]
	137614722877456 -> 137615235062832
	137615235062832 [label=AccumulateGrad]
	137615235062688 -> 137615235062784
	137614722878256 [label="layer3.1.bn2.bias
 (256)" fillcolor=lightblue]
	137614722878256 -> 137615235062688
	137615235062688 [label=AccumulateGrad]
	137615235062592 -> 137615235062448
	137614722877216 [label="layer3.1.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	137614722877216 -> 137615235062592
	137615235062592 [label=AccumulateGrad]
	137615235062400 -> 137615235062304
	137614722877776 [label="layer3.1.bn3.weight
 (1024)" fillcolor=lightblue]
	137614722877776 -> 137615235062400
	137615235062400 [label=AccumulateGrad]
	137615235062352 -> 137615235062304
	137614722875456 [label="layer3.1.bn3.bias
 (1024)" fillcolor=lightblue]
	137614722875456 -> 137615235062352
	137615235062352 [label=AccumulateGrad]
	137615235062256 -> 137615235062160
	137615235062112 -> 137615235062016
	137615235062112 [label=SigmoidBackward0]
	137615235062544 -> 137615235062112
	137615235062544 [label=AddBackward0]
	137615235062928 -> 137615235062544
	137615235062928 [label=ConvolutionBackward0]
	137615235063360 -> 137615235062928
	137615235063360 [label=ReluBackward0]
	137615235063552 -> 137615235063360
	137615235063552 [label=ConvolutionBackward0]
	137615235064608 -> 137615235063552
	137615235064608 [label=MeanBackward1]
	137615235062160 -> 137615235064608
	137615233394736 -> 137615235063552
	137614705290960 [label="layer3.1.cbam.ca.mlp.0.weight
 (64, 1024, 1, 1)" fillcolor=lightblue]
	137614705290960 -> 137615233394736
	137615233394736 [label=AccumulateGrad]
	137615233394496 -> 137615235063552
	137614705291040 [label="layer3.1.cbam.ca.mlp.0.bias
 (64)" fillcolor=lightblue]
	137614705291040 -> 137615233394496
	137615233394496 [label=AccumulateGrad]
	137615235063456 -> 137615235062928
	137614705290800 [label="layer3.1.cbam.ca.mlp.2.weight
 (1024, 64, 1, 1)" fillcolor=lightblue]
	137614705290800 -> 137615235063456
	137615235063456 [label=AccumulateGrad]
	137615235062736 -> 137615235062928
	137614705291120 [label="layer3.1.cbam.ca.mlp.2.bias
 (1024)" fillcolor=lightblue]
	137614705291120 -> 137615235062736
	137615235062736 [label=AccumulateGrad]
	137615235062976 -> 137615235062544
	137615235062976 [label=ConvolutionBackward0]
	137615235063408 -> 137615235062976
	137615235063408 [label=ReluBackward0]
	137615233394784 -> 137615235063408
	137615233394784 [label=ConvolutionBackward0]
	137615233395072 -> 137615233394784
	137615233395072 [label=AdaptiveMaxPool2DBackward0]
	137615235062160 -> 137615233395072
	137615233394736 -> 137615233394784
	137615233394496 -> 137615233394784
	137615235063456 -> 137615235062976
	137615235062736 -> 137615235062976
	137615235061968 -> 137615235061920
	137615235061968 [label=SigmoidBackward0]
	137615235062208 -> 137615235061968
	137615235062208 [label=NativeBatchNormBackward0]
	137615235063168 -> 137615235062208
	137615235063168 [label=ConvolutionBackward0]
	137615233394976 -> 137615235063168
	137615233394976 [label=CatBackward0]
	137615233395312 -> 137615233394976
	137615233395312 [label=MaxBackward0]
	137615235062016 -> 137615233395312
	137615233395264 -> 137615233394976
	137615233395264 [label=MeanBackward1]
	137615235062016 -> 137615233395264
	137615233395120 -> 137615235063168
	137614705291360 [label="layer3.1.cbam.sa.conv.weight
 (1, 2, 7, 7)" fillcolor=lightblue]
	137614705291360 -> 137615233395120
	137615233395120 [label=AccumulateGrad]
	137615235062064 -> 137615235062208
	137614705291440 [label="layer3.1.cbam.sa.bn.weight
 (1)" fillcolor=lightblue]
	137614705291440 -> 137615235062064
	137615235062064 [label=AccumulateGrad]
	137615233395024 -> 137615235062208
	137614705291520 [label="layer3.1.cbam.sa.bn.bias
 (1)" fillcolor=lightblue]
	137614705291520 -> 137615233395024
	137615233395024 [label=AccumulateGrad]
	137615235061824 -> 137615235061632
	137614722877056 [label="layer3.2.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	137614722877056 -> 137615235061824
	137615235061824 [label=AccumulateGrad]
	137615235061584 -> 137615235061536
	137614722876656 [label="layer3.2.bn1.weight
 (256)" fillcolor=lightblue]
	137614722876656 -> 137615235061584
	137615235061584 [label=AccumulateGrad]
	137615235061440 -> 137615235061536
	137614722878016 [label="layer3.2.bn1.bias
 (256)" fillcolor=lightblue]
	137614722878016 -> 137615235061440
	137615235061440 [label=AccumulateGrad]
	137615235061344 -> 137615235061200
	137614722886736 [label="layer3.2.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	137614722886736 -> 137615235061344
	137615235061344 [label=AccumulateGrad]
	137615235061152 -> 137615235061104
	137614722886816 [label="layer3.2.bn2.weight
 (256)" fillcolor=lightblue]
	137614722886816 -> 137615235061152
	137615235061152 [label=AccumulateGrad]
	137615235061008 -> 137615235061104
	137614722888336 [label="layer3.2.bn2.bias
 (256)" fillcolor=lightblue]
	137614722888336 -> 137615235061008
	137615235061008 [label=AccumulateGrad]
	137615235060912 -> 137615235060768
	137614721024384 [label="layer3.2.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	137614721024384 -> 137615235060912
	137615235060912 [label=AccumulateGrad]
	137615235060720 -> 137615235060624
	137614721025184 [label="layer3.2.bn3.weight
 (1024)" fillcolor=lightblue]
	137614721025184 -> 137615235060720
	137615235060720 [label=AccumulateGrad]
	137615235060672 -> 137615235060624
	137614721025344 [label="layer3.2.bn3.bias
 (1024)" fillcolor=lightblue]
	137614721025344 -> 137615235060672
	137615235060672 [label=AccumulateGrad]
	137615235060576 -> 137615235060480
	137615235060432 -> 137615235060336
	137615235060432 [label=SigmoidBackward0]
	137615235060864 -> 137615235060432
	137615235060864 [label=AddBackward0]
	137615235061248 -> 137615235060864
	137615235061248 [label=ConvolutionBackward0]
	137615235061680 -> 137615235061248
	137615235061680 [label=ReluBackward0]
	137615235061872 -> 137615235061680
	137615235061872 [label=ConvolutionBackward0]
	137615235062496 -> 137615235061872
	137615235062496 [label=MeanBackward1]
	137615235060480 -> 137615235062496
	137615233395168 -> 137615235061872
	137614705292160 [label="layer3.2.cbam.ca.mlp.0.weight
 (64, 1024, 1, 1)" fillcolor=lightblue]
	137614705292160 -> 137615233395168
	137615233395168 [label=AccumulateGrad]
	137615233394928 -> 137615235061872
	137614705291920 [label="layer3.2.cbam.ca.mlp.0.bias
 (64)" fillcolor=lightblue]
	137614705291920 -> 137615233394928
	137615233394928 [label=AccumulateGrad]
	137615235061776 -> 137615235061248
	137614705292000 [label="layer3.2.cbam.ca.mlp.2.weight
 (1024, 64, 1, 1)" fillcolor=lightblue]
	137614705292000 -> 137615235061776
	137615235061776 [label=AccumulateGrad]
	137615235061056 -> 137615235061248
	137614705292240 [label="layer3.2.cbam.ca.mlp.2.bias
 (1024)" fillcolor=lightblue]
	137614705292240 -> 137615235061056
	137615235061056 [label=AccumulateGrad]
	137615235061296 -> 137615235060864
	137615235061296 [label=ConvolutionBackward0]
	137615235061728 -> 137615235061296
	137615235061728 [label=ReluBackward0]
	137615233395216 -> 137615235061728
	137615233395216 [label=ConvolutionBackward0]
	137615233395504 -> 137615233395216
	137615233395504 [label=AdaptiveMaxPool2DBackward0]
	137615235060480 -> 137615233395504
	137615233395168 -> 137615233395216
	137615233394928 -> 137615233395216
	137615235061776 -> 137615235061296
	137615235061056 -> 137615235061296
	137615235060288 -> 137615235060240
	137615235060288 [label=SigmoidBackward0]
	137615235060528 -> 137615235060288
	137615235060528 [label=NativeBatchNormBackward0]
	137615235061488 -> 137615235060528
	137615235061488 [label=ConvolutionBackward0]
	137615233395408 -> 137615235061488
	137615233395408 [label=CatBackward0]
	137615233395744 -> 137615233395408
	137615233395744 [label=MaxBackward0]
	137615235060336 -> 137615233395744
	137615233395696 -> 137615233395408
	137615233395696 [label=MeanBackward1]
	137615235060336 -> 137615233395696
	137615233395552 -> 137615235061488
	137614705292480 [label="layer3.2.cbam.sa.conv.weight
 (1, 2, 7, 7)" fillcolor=lightblue]
	137614705292480 -> 137615233395552
	137615233395552 [label=AccumulateGrad]
	137615235060384 -> 137615235060528
	137614705292560 [label="layer3.2.cbam.sa.bn.weight
 (1)" fillcolor=lightblue]
	137614705292560 -> 137615235060384
	137615235060384 [label=AccumulateGrad]
	137615233395456 -> 137615235060528
	137614705292640 [label="layer3.2.cbam.sa.bn.bias
 (1)" fillcolor=lightblue]
	137614705292640 -> 137615233395456
	137615233395456 [label=AccumulateGrad]
	137615235060144 -> 137615235059952
	137614721025504 [label="layer3.3.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	137614721025504 -> 137615235060144
	137615235060144 [label=AccumulateGrad]
	137615235059904 -> 137615235059856
	137614721025424 [label="layer3.3.bn1.weight
 (256)" fillcolor=lightblue]
	137614721025424 -> 137615235059904
	137615235059904 [label=AccumulateGrad]
	137615235059760 -> 137615235059856
	137614721024144 [label="layer3.3.bn1.bias
 (256)" fillcolor=lightblue]
	137614721024144 -> 137615235059760
	137615235059760 [label=AccumulateGrad]
	137615235059664 -> 137615235059520
	137614722954832 [label="layer3.3.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	137614722954832 -> 137615235059664
	137615235059664 [label=AccumulateGrad]
	137615235059472 -> 137615235059424
	137614722954512 [label="layer3.3.bn2.weight
 (256)" fillcolor=lightblue]
	137614722954512 -> 137615235059472
	137615235059472 [label=AccumulateGrad]
	137615235059280 -> 137615235059424
	137614722957232 [label="layer3.3.bn2.bias
 (256)" fillcolor=lightblue]
	137614722957232 -> 137615235059280
	137615235059280 [label=AccumulateGrad]
	137615235059232 -> 137615235059040
	137614704489184 [label="layer3.3.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	137614704489184 -> 137615235059232
	137615235059232 [label=AccumulateGrad]
	137615235059088 -> 137615235058848
	137614704488544 [label="layer3.3.bn3.weight
 (1024)" fillcolor=lightblue]
	137614704488544 -> 137615235059088
	137615235059088 [label=AccumulateGrad]
	137615235058896 -> 137615235058848
	137614704490944 [label="layer3.3.bn3.bias
 (1024)" fillcolor=lightblue]
	137614704490944 -> 137615235058896
	137615235058896 [label=AccumulateGrad]
	137615235058944 -> 137615235058752
	137615235058656 -> 137615235058608
	137615235058656 [label=SigmoidBackward0]
	137615235059184 -> 137615235058656
	137615235059184 [label=AddBackward0]
	137615235059568 -> 137615235059184
	137615235059568 [label=ConvolutionBackward0]
	137615235060000 -> 137615235059568
	137615235060000 [label=ReluBackward0]
	137615235060192 -> 137615235060000
	137615235060192 [label=ConvolutionBackward0]
	137615235060816 -> 137615235060192
	137615235060816 [label=MeanBackward1]
	137615235058752 -> 137615235060816
	137615233395600 -> 137615235060192
	137614705293200 [label="layer3.3.cbam.ca.mlp.0.weight
 (64, 1024, 1, 1)" fillcolor=lightblue]
	137614705293200 -> 137615233395600
	137615233395600 [label=AccumulateGrad]
	137615233395360 -> 137615235060192
	137614705292960 [label="layer3.3.cbam.ca.mlp.0.bias
 (64)" fillcolor=lightblue]
	137614705292960 -> 137615233395360
	137615233395360 [label=AccumulateGrad]
	137615235060096 -> 137615235059568
	137614705293040 [label="layer3.3.cbam.ca.mlp.2.weight
 (1024, 64, 1, 1)" fillcolor=lightblue]
	137614705293040 -> 137615235060096
	137615235060096 [label=AccumulateGrad]
	137615235059376 -> 137615235059568
	137614705293280 [label="layer3.3.cbam.ca.mlp.2.bias
 (1024)" fillcolor=lightblue]
	137614705293280 -> 137615235059376
	137615235059376 [label=AccumulateGrad]
	137615235059616 -> 137615235059184
	137615235059616 [label=ConvolutionBackward0]
	137615235060048 -> 137615235059616
	137615235060048 [label=ReluBackward0]
	137615233395648 -> 137615235060048
	137615233395648 [label=ConvolutionBackward0]
	137615233395936 -> 137615233395648
	137615233395936 [label=AdaptiveMaxPool2DBackward0]
	137615235058752 -> 137615233395936
	137615233395600 -> 137615233395648
	137615233395360 -> 137615233395648
	137615235060096 -> 137615235059616
	137615235059376 -> 137615235059616
	137615235058512 -> 137615235058560
	137615235058512 [label=SigmoidBackward0]
	137615235058800 -> 137615235058512
	137615235058800 [label=NativeBatchNormBackward0]
	137615235059808 -> 137615235058800
	137615235059808 [label=ConvolutionBackward0]
	137615233395840 -> 137615235059808
	137615233395840 [label=CatBackward0]
	137615233396176 -> 137615233395840
	137615233396176 [label=MaxBackward0]
	137615235058608 -> 137615233396176
	137615233396128 -> 137615233395840
	137615233396128 [label=MeanBackward1]
	137615235058608 -> 137615233396128
	137615233395984 -> 137615235059808
	137614705293520 [label="layer3.3.cbam.sa.conv.weight
 (1, 2, 7, 7)" fillcolor=lightblue]
	137614705293520 -> 137615233395984
	137615233395984 [label=AccumulateGrad]
	137615235058704 -> 137615235058800
	137614705293600 [label="layer3.3.cbam.sa.bn.weight
 (1)" fillcolor=lightblue]
	137614705293600 -> 137615235058704
	137615235058704 [label=AccumulateGrad]
	137615233395888 -> 137615235058800
	137614705293680 [label="layer3.3.cbam.sa.bn.bias
 (1)" fillcolor=lightblue]
	137614705293680 -> 137615233395888
	137615233395888 [label=AccumulateGrad]
	137615235058416 -> 137615235058224
	137614704481104 [label="layer3.4.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	137614704481104 -> 137615235058416
	137615235058416 [label=AccumulateGrad]
	137615235058176 -> 137615235057984
	137614704491424 [label="layer3.4.bn1.weight
 (256)" fillcolor=lightblue]
	137614704491424 -> 137615235058176
	137615235058176 [label=AccumulateGrad]
	137615235057936 -> 137615235057984
	137614704492304 [label="layer3.4.bn1.bias
 (256)" fillcolor=lightblue]
	137614704492304 -> 137615235057936
	137615235057936 [label=AccumulateGrad]
	137615235058032 -> 137615235057792
	137614704489744 [label="layer3.4.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	137614704489744 -> 137615235058032
	137615235058032 [label=AccumulateGrad]
	137615235057504 -> 137615235057888
	137614704489664 [label="layer3.4.bn2.weight
 (256)" fillcolor=lightblue]
	137614704489664 -> 137615235057504
	137615235057504 [label=AccumulateGrad]
	137615235057648 -> 137615235057888
	137614706029840 [label="layer3.4.bn2.bias
 (256)" fillcolor=lightblue]
	137614706029840 -> 137615235057648
	137615235057648 [label=AccumulateGrad]
	137615235057360 -> 137615235057168
	137614704698976 [label="layer3.4.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	137614704698976 -> 137615235057360
	137615235057360 [label=AccumulateGrad]
	137615235057264 -> 137615235057600
	137614704698816 [label="layer3.4.bn3.weight
 (1024)" fillcolor=lightblue]
	137614704698816 -> 137615235057264
	137615235057264 [label=AccumulateGrad]
	137615235056400 -> 137615235057600
	137614704691136 [label="layer3.4.bn3.bias
 (1024)" fillcolor=lightblue]
	137614704691136 -> 137615235056400
	137615235056400 [label=AccumulateGrad]
	137615235057120 -> 137615235056928
	137615235056976 -> 137615235057072
	137615235056976 [label=SigmoidBackward0]
	137615235057840 -> 137615235056976
	137615235057840 [label=AddBackward0]
	137615235057744 -> 137615235057840
	137615235057744 [label=ConvolutionBackward0]
	137615235058128 -> 137615235057744
	137615235058128 [label=ReluBackward0]
	137615235058464 -> 137615235058128
	137615235058464 [label=ConvolutionBackward0]
	137615235059136 -> 137615235058464
	137615235059136 [label=MeanBackward1]
	137615235056928 -> 137615235059136
	137615233396032 -> 137615235058464
	137614705294320 [label="layer3.4.cbam.ca.mlp.0.weight
 (64, 1024, 1, 1)" fillcolor=lightblue]
	137614705294320 -> 137615233396032
	137615233396032 [label=AccumulateGrad]
	137615233395792 -> 137615235058464
	137614705294080 [label="layer3.4.cbam.ca.mlp.0.bias
 (64)" fillcolor=lightblue]
	137614705294080 -> 137615233395792
	137615233395792 [label=AccumulateGrad]
	137615235058368 -> 137615235057744
	137614705294160 [label="layer3.4.cbam.ca.mlp.2.weight
 (1024, 64, 1, 1)" fillcolor=lightblue]
	137614705294160 -> 137615235058368
	137615235058368 [label=AccumulateGrad]
	137615235057552 -> 137615235057744
	137614705294400 [label="layer3.4.cbam.ca.mlp.2.bias
 (1024)" fillcolor=lightblue]
	137614705294400 -> 137615235057552
	137615235057552 [label=AccumulateGrad]
	137615235057696 -> 137615235057840
	137615235057696 [label=ConvolutionBackward0]
	137615235058272 -> 137615235057696
	137615235058272 [label=ReluBackward0]
	137615233396080 -> 137615235058272
	137615233396080 [label=ConvolutionBackward0]
	137615233396368 -> 137615233396080
	137615233396368 [label=AdaptiveMaxPool2DBackward0]
	137615235056928 -> 137615233396368
	137615233396032 -> 137615233396080
	137615233395792 -> 137615233396080
	137615235058368 -> 137615235057696
	137615235057552 -> 137615235057696
	137615235057216 -> 137615235056352
	137615235057216 [label=SigmoidBackward0]
	137615235057408 -> 137615235057216
	137615235057408 [label=NativeBatchNormBackward0]
	137615235058320 -> 137615235057408
	137615235058320 [label=ConvolutionBackward0]
	137615233396272 -> 137615235058320
	137615233396272 [label=CatBackward0]
	137615233396608 -> 137615233396272
	137615233396608 [label=MaxBackward0]
	137615235057072 -> 137615233396608
	137615233396560 -> 137615233396272
	137615233396560 [label=MeanBackward1]
	137615235057072 -> 137615233396560
	137615233396416 -> 137615235058320
	137614705294640 [label="layer3.4.cbam.sa.conv.weight
 (1, 2, 7, 7)" fillcolor=lightblue]
	137614705294640 -> 137615233396416
	137615233396416 [label=AccumulateGrad]
	137615235057024 -> 137615235057408
	137614705294720 [label="layer3.4.cbam.sa.bn.weight
 (1)" fillcolor=lightblue]
	137614705294720 -> 137615235057024
	137615235057024 [label=AccumulateGrad]
	137615233396320 -> 137615235057408
	137614705294800 [label="layer3.4.cbam.sa.bn.bias
 (1)" fillcolor=lightblue]
	137614705294800 -> 137615233396320
	137615233396320 [label=AccumulateGrad]
	137615235056832 -> 137615235056736
	137614704698336 [label="layer3.5.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	137614704698336 -> 137615235056832
	137615235056832 [label=AccumulateGrad]
	137615235056544 -> 137615235056592
	137614704692496 [label="layer3.5.bn1.weight
 (256)" fillcolor=lightblue]
	137614704692496 -> 137615235056544
	137615235056544 [label=AccumulateGrad]
	137615235056640 -> 137615235056592
	137614704700096 [label="layer3.5.bn1.bias
 (256)" fillcolor=lightblue]
	137614704700096 -> 137615235056640
	137615235056640 [label=AccumulateGrad]
	137615235055920 -> 137615235055776
	137614705625904 [label="layer3.5.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	137614705625904 -> 137615235055920
	137615235055920 [label=AccumulateGrad]
	137615235056304 -> 137615235055680
	137614705628544 [label="layer3.5.bn2.weight
 (256)" fillcolor=lightblue]
	137614705628544 -> 137615235056304
	137615235056304 [label=AccumulateGrad]
	137615235055872 -> 137615235055680
	137614705637504 [label="layer3.5.bn2.bias
 (256)" fillcolor=lightblue]
	137614705637504 -> 137615235055872
	137615235055872 [label=AccumulateGrad]
	137615235055728 -> 137615234719824
	137614704861616 [label="layer3.5.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	137614704861616 -> 137615235055728
	137615235055728 [label=AccumulateGrad]
	137615235056016 -> 137615234720352
	137614704862896 [label="layer3.5.bn3.weight
 (1024)" fillcolor=lightblue]
	137614704862896 -> 137615235056016
	137615235056016 [label=AccumulateGrad]
	137615235056064 -> 137615234720352
	137614704859296 [label="layer3.5.bn3.bias
 (1024)" fillcolor=lightblue]
	137614704859296 -> 137615235056064
	137615235056064 [label=AccumulateGrad]
	137615234719344 -> 137615234719584
	137615234719776 -> 137615234719632
	137615234719776 [label=SigmoidBackward0]
	137615234719728 -> 137615234719776
	137615234719728 [label=AddBackward0]
	137615235056688 -> 137615234719728
	137615235056688 [label=ConvolutionBackward0]
	137615235056112 -> 137615235056688
	137615235056112 [label=ReluBackward0]
	137615235056784 -> 137615235056112
	137615235056784 [label=ConvolutionBackward0]
	137615235057456 -> 137615235056784
	137615235057456 [label=MeanBackward1]
	137615234719584 -> 137615235057456
	137615233396464 -> 137615235056784
	137614705295280 [label="layer3.5.cbam.ca.mlp.0.weight
 (64, 1024, 1, 1)" fillcolor=lightblue]
	137614705295280 -> 137615233396464
	137615233396464 [label=AccumulateGrad]
	137615233396224 -> 137615235056784
	137614703476800 [label="layer3.5.cbam.ca.mlp.0.bias
 (64)" fillcolor=lightblue]
	137614703476800 -> 137615233396224
	137615233396224 [label=AccumulateGrad]
	137615235056448 -> 137615235056688
	137614703476960 [label="layer3.5.cbam.ca.mlp.2.weight
 (1024, 64, 1, 1)" fillcolor=lightblue]
	137614703476960 -> 137615235056448
	137615235056448 [label=AccumulateGrad]
	137615235056208 -> 137615235056688
	137614703477040 [label="layer3.5.cbam.ca.mlp.2.bias
 (1024)" fillcolor=lightblue]
	137614703477040 -> 137615235056208
	137615235056208 [label=AccumulateGrad]
	137615235055824 -> 137615234719728
	137615235055824 [label=ConvolutionBackward0]
	137615235056880 -> 137615235055824
	137615235056880 [label=ReluBackward0]
	137615233396512 -> 137615235056880
	137615233396512 [label=ConvolutionBackward0]
	137615233396800 -> 137615233396512
	137615233396800 [label=AdaptiveMaxPool2DBackward0]
	137615234719584 -> 137615233396800
	137615233396464 -> 137615233396512
	137615233396224 -> 137615233396512
	137615235056448 -> 137615235055824
	137615235056208 -> 137615235055824
	137615234719392 -> 137615234719152
	137615234719392 [label=SigmoidBackward0]
	137615234720256 -> 137615234719392
	137615234720256 [label=NativeBatchNormBackward0]
	137615235056496 -> 137615234720256
	137615235056496 [label=ConvolutionBackward0]
	137615233396704 -> 137615235056496
	137615233396704 [label=CatBackward0]
	137615233397040 -> 137615233396704
	137615233397040 [label=MaxBackward0]
	137615234719632 -> 137615233397040
	137615233396992 -> 137615233396704
	137615233396992 [label=MeanBackward1]
	137615234719632 -> 137615233396992
	137615233396848 -> 137615235056496
	137614703477120 [label="layer3.5.cbam.sa.conv.weight
 (1, 2, 7, 7)" fillcolor=lightblue]
	137614703477120 -> 137615233396848
	137615233396848 [label=AccumulateGrad]
	137615235056256 -> 137615234720256
	137614703477200 [label="layer3.5.cbam.sa.bn.weight
 (1)" fillcolor=lightblue]
	137614703477200 -> 137615235056256
	137615235056256 [label=AccumulateGrad]
	137615233396752 -> 137615234720256
	137614703477280 [label="layer3.5.cbam.sa.bn.bias
 (1)" fillcolor=lightblue]
	137614703477280 -> 137615233396752
	137615233396752 [label=AccumulateGrad]
	137615234719248 -> 137615234719296
	137614704867456 [label="layer4.0.conv1.weight
 (512, 1024, 1, 1)" fillcolor=lightblue]
	137614704867456 -> 137615234719248
	137615234719248 [label=AccumulateGrad]
	137615234719200 -> 137615234719680
	137614704869216 [label="layer4.0.bn1.weight
 (512)" fillcolor=lightblue]
	137614704869216 -> 137615234719200
	137615234719200 [label=AccumulateGrad]
	137615234718720 -> 137615234719680
	137614704868976 [label="layer4.0.bn1.bias
 (512)" fillcolor=lightblue]
	137614704868976 -> 137615234718720
	137615234718720 [label=AccumulateGrad]
	137615234718816 -> 137615234718480
	137614704868256 [label="layer4.0.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	137614704868256 -> 137615234718816
	137615234718816 [label=AccumulateGrad]
	137615234718096 -> 137615234718672
	137614704865536 [label="layer4.0.bn2.weight
 (512)" fillcolor=lightblue]
	137614704865536 -> 137615234718096
	137615234718096 [label=AccumulateGrad]
	137615234718624 -> 137615234718672
	137614704863856 [label="layer4.0.bn2.bias
 (512)" fillcolor=lightblue]
	137614704863856 -> 137615234718624
	137615234718624 [label=AccumulateGrad]
	137615234718960 -> 137615234717712
	137614704860576 [label="layer4.0.conv3.weight
 (2048, 512, 1, 1)" fillcolor=lightblue]
	137614704860576 -> 137615234718960
	137615234718960 [label=AccumulateGrad]
	137615234727792 -> 137615234719104
	137614704869056 [label="layer4.0.bn3.weight
 (2048)" fillcolor=lightblue]
	137614704869056 -> 137615234727792
	137615234727792 [label=AccumulateGrad]
	137615234718192 -> 137615234719104
	137614704869136 [label="layer4.0.bn3.bias
 (2048)" fillcolor=lightblue]
	137614704869136 -> 137615234718192
	137615234718192 [label=AccumulateGrad]
	137615234718336 -> 137615234727072
	137615234718336 [label=NativeBatchNormBackward0]
	137615235055968 -> 137615234718336
	137615235055968 [label=ConvolutionBackward0]
	137615234718768 -> 137615235055968
	137615234719536 -> 137615235055968
	137614704860896 [label="layer4.0.downsample.0.weight
 (2048, 1024, 1, 1)" fillcolor=lightblue]
	137614704860896 -> 137615234719536
	137615234719536 [label=AccumulateGrad]
	137615234717616 -> 137615234718336
	137614704861856 [label="layer4.0.downsample.1.weight
 (2048)" fillcolor=lightblue]
	137614704861856 -> 137615234717616
	137615234717616 [label=AccumulateGrad]
	137615234718384 -> 137615234718336
	137614704862416 [label="layer4.0.downsample.1.bias
 (2048)" fillcolor=lightblue]
	137614704862416 -> 137615234718384
	137615234718384 [label=AccumulateGrad]
	137615234718528 -> 137615234718912
	137615234718528 [label=SigmoidBackward0]
	137615234719488 -> 137615234718528
	137615234719488 [label=AddBackward0]
	137615234719440 -> 137615234719488
	137615234719440 [label=ConvolutionBackward0]
	137615234719056 -> 137615234719440
	137615234719056 [label=ReluBackward0]
	137615233397088 -> 137615234719056
	137615233397088 [label=ConvolutionBackward0]
	137615233397184 -> 137615233397088
	137615233397184 [label=MeanBackward1]
	137615234727072 -> 137615233397184
	137615233396944 -> 137615233397088
	137614703477920 [label="layer4.0.cbam.ca.mlp.0.weight
 (128, 2048, 1, 1)" fillcolor=lightblue]
	137614703477920 -> 137615233396944
	137615233396944 [label=AccumulateGrad]
	137615233396896 -> 137615233397088
	137614703477680 [label="layer4.0.cbam.ca.mlp.0.bias
 (128)" fillcolor=lightblue]
	137614703477680 -> 137615233396896
	137615233396896 [label=AccumulateGrad]
	137615234718576 -> 137615234719440
	137614703477760 [label="layer4.0.cbam.ca.mlp.2.weight
 (2048, 128, 1, 1)" fillcolor=lightblue]
	137614703477760 -> 137615234718576
	137615234718576 [label=AccumulateGrad]
	137615234717952 -> 137615234719440
	137614703478000 [label="layer4.0.cbam.ca.mlp.2.bias
 (2048)" fillcolor=lightblue]
	137614703478000 -> 137615234717952
	137615234717952 [label=AccumulateGrad]
	137615234720304 -> 137615234719488
	137615234720304 [label=ConvolutionBackward0]
	137615233397232 -> 137615234720304
	137615233397232 [label=ReluBackward0]
	137615233397328 -> 137615233397232
	137615233397328 [label=ConvolutionBackward0]
	137615233397424 -> 137615233397328
	137615233397424 [label=AdaptiveMaxPool2DBackward0]
	137615234727072 -> 137615233397424
	137615233396944 -> 137615233397328
	137615233396896 -> 137615233397328
	137615234718576 -> 137615234720304
	137615234717952 -> 137615234720304
	137615234718048 -> 137615234720160
	137615234718048 [label=SigmoidBackward0]
	137615234718240 -> 137615234718048
	137615234718240 [label=NativeBatchNormBackward0]
	137615234718000 -> 137615234718240
	137615234718000 [label=ConvolutionBackward0]
	137615233396656 -> 137615234718000
	137615233396656 [label=CatBackward0]
	137615233397664 -> 137615233396656
	137615233397664 [label=MaxBackward0]
	137615234718912 -> 137615233397664
	137615233397616 -> 137615233396656
	137615233397616 [label=MeanBackward1]
	137615234718912 -> 137615233397616
	137615233397472 -> 137615234718000
	137614703478160 [label="layer4.0.cbam.sa.conv.weight
 (1, 2, 7, 7)" fillcolor=lightblue]
	137614703478160 -> 137615233397472
	137615233397472 [label=AccumulateGrad]
	137615233397376 -> 137615234718240
	137614703478240 [label="layer4.0.cbam.sa.bn.weight
 (1)" fillcolor=lightblue]
	137614703478240 -> 137615233397376
	137615233397376 [label=AccumulateGrad]
	137615233397136 -> 137615234718240
	137614703478320 [label="layer4.0.cbam.sa.bn.bias
 (1)" fillcolor=lightblue]
	137614703478320 -> 137615233397136
	137615233397136 [label=AccumulateGrad]
	137615234727360 -> 137615234727552
	137614704860416 [label="layer4.1.conv1.weight
 (512, 2048, 1, 1)" fillcolor=lightblue]
	137614704860416 -> 137615234727360
	137615234727360 [label=AccumulateGrad]
	137615234720208 -> 137615234717664
	137614705279040 [label="layer4.1.bn1.weight
 (512)" fillcolor=lightblue]
	137614705279040 -> 137615234720208
	137615234720208 [label=AccumulateGrad]
	137615234727696 -> 137615234717664
	137614705279120 [label="layer4.1.bn1.bias
 (512)" fillcolor=lightblue]
	137614705279120 -> 137615234727696
	137615234727696 [label=AccumulateGrad]
	137615234718144 -> 137615234727216
	137614705279680 [label="layer4.1.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	137614705279680 -> 137615234718144
	137615234718144 [label=AccumulateGrad]
	137615234717760 -> 137615234726880
	137614705279600 [label="layer4.1.bn2.weight
 (512)" fillcolor=lightblue]
	137614705279600 -> 137615234717760
	137615234717760 [label=AccumulateGrad]
	137615234717808 -> 137615234726880
	137614705279760 [label="layer4.1.bn2.bias
 (512)" fillcolor=lightblue]
	137614705279760 -> 137615234717808
	137615234717808 [label=AccumulateGrad]
	137615234727456 -> 137615234727840
	137614705280240 [label="layer4.1.conv3.weight
 (2048, 512, 1, 1)" fillcolor=lightblue]
	137614705280240 -> 137615234727456
	137615234727456 [label=AccumulateGrad]
	137615234727408 -> 137615234719920
	137614705280320 [label="layer4.1.bn3.weight
 (2048)" fillcolor=lightblue]
	137614705280320 -> 137615234727408
	137615234727408 [label=AccumulateGrad]
	137615234727120 -> 137615234719920
	137614705280400 [label="layer4.1.bn3.bias
 (2048)" fillcolor=lightblue]
	137614705280400 -> 137615234727120
	137615234727120 [label=AccumulateGrad]
	137615234727648 -> 137615234726928
	137615234726208 -> 137615234719008
	137615234726208 [label=SigmoidBackward0]
	137615234717856 -> 137615234726208
	137615234717856 [label=AddBackward0]
	137615234718432 -> 137615234717856
	137615234718432 [label=ConvolutionBackward0]
	137615234719872 -> 137615234718432
	137615234719872 [label=ReluBackward0]
	137615234717904 -> 137615234719872
	137615234717904 [label=ConvolutionBackward0]
	137615234720112 -> 137615234717904
	137615234720112 [label=MeanBackward1]
	137615234726928 -> 137615234720112
	137615233397520 -> 137615234717904
	137614703478800 [label="layer4.1.cbam.ca.mlp.0.weight
 (128, 2048, 1, 1)" fillcolor=lightblue]
	137614703478800 -> 137615233397520
	137615233397520 [label=AccumulateGrad]
	137615233397280 -> 137615234717904
	137614703478880 [label="layer4.1.cbam.ca.mlp.0.bias
 (128)" fillcolor=lightblue]
	137614703478880 -> 137615233397280
	137615233397280 [label=AccumulateGrad]
	137615234719968 -> 137615234718432
	137614703478640 [label="layer4.1.cbam.ca.mlp.2.weight
 (2048, 128, 1, 1)" fillcolor=lightblue]
	137614703478640 -> 137615234719968
	137615234719968 [label=AccumulateGrad]
	137615234720064 -> 137615234718432
	137614703478960 [label="layer4.1.cbam.ca.mlp.2.bias
 (2048)" fillcolor=lightblue]
	137614703478960 -> 137615234720064
	137615234720064 [label=AccumulateGrad]
	137615234727312 -> 137615234717856
	137615234727312 [label=ConvolutionBackward0]
	137615234717568 -> 137615234727312
	137615234717568 [label=ReluBackward0]
	137615233397568 -> 137615234717568
	137615233397568 [label=ConvolutionBackward0]
	137615233397856 -> 137615233397568
	137615233397856 [label=AdaptiveMaxPool2DBackward0]
	137615234726928 -> 137615233397856
	137615233397520 -> 137615233397568
	137615233397280 -> 137615233397568
	137615234719968 -> 137615234727312
	137615234720064 -> 137615234727312
	137615234726304 -> 137615234726496
	137615234726304 [label=SigmoidBackward0]
	137615234727024 -> 137615234726304
	137615234727024 [label=NativeBatchNormBackward0]
	137615234727600 -> 137615234727024
	137615234727600 [label=ConvolutionBackward0]
	137615233397760 -> 137615234727600
	137615233397760 [label=CatBackward0]
	137615233398096 -> 137615233397760
	137615233398096 [label=MaxBackward0]
	137615234719008 -> 137615233398096
	137615233398048 -> 137615233397760
	137615233398048 [label=MeanBackward1]
	137615234719008 -> 137615233398048
	137615233397904 -> 137615234727600
	137614703479200 [label="layer4.1.cbam.sa.conv.weight
 (1, 2, 7, 7)" fillcolor=lightblue]
	137614703479200 -> 137615233397904
	137615233397904 [label=AccumulateGrad]
	137615234727504 -> 137615234727024
	137614703479280 [label="layer4.1.cbam.sa.bn.weight
 (1)" fillcolor=lightblue]
	137614703479280 -> 137615234727504
	137615234727504 [label=AccumulateGrad]
	137615233397808 -> 137615234727024
	137614703479360 [label="layer4.1.cbam.sa.bn.bias
 (1)" fillcolor=lightblue]
	137614703479360 -> 137615233397808
	137615233397808 [label=AccumulateGrad]
	137615234726688 -> 137615234727744
	137614705280800 [label="layer4.2.conv1.weight
 (512, 2048, 1, 1)" fillcolor=lightblue]
	137614705280800 -> 137615234726688
	137615234726688 [label=AccumulateGrad]
	137615234726976 -> 137615234727264
	137614705280880 [label="layer4.2.bn1.weight
 (512)" fillcolor=lightblue]
	137614705280880 -> 137615234726976
	137615234726976 [label=AccumulateGrad]
	137615234726160 -> 137615234727264
	137614705280960 [label="layer4.2.bn1.bias
 (512)" fillcolor=lightblue]
	137614705280960 -> 137615234726160
	137615234726160 [label=AccumulateGrad]
	137615234725968 -> 137614695627936
	137614705281520 [label="layer4.2.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	137614705281520 -> 137615234725968
	137615234725968 [label=AccumulateGrad]
	137614695627456 -> 137614695628224
	137614705281440 [label="layer4.2.bn2.weight
 (512)" fillcolor=lightblue]
	137614705281440 -> 137614695627456
	137614695627456 [label=AccumulateGrad]
	137614695628368 -> 137614695628224
	137614705281600 [label="layer4.2.bn2.bias
 (512)" fillcolor=lightblue]
	137614705281600 -> 137614695628368
	137614695628368 [label=AccumulateGrad]
	137614695628608 -> 137614695638272
	137614705282080 [label="layer4.2.conv3.weight
 (2048, 512, 1, 1)" fillcolor=lightblue]
	137614705282080 -> 137614695628608
	137614695628608 [label=AccumulateGrad]
	137614695639568 -> 137614695638464
	137614705282160 [label="layer4.2.bn3.weight
 (2048)" fillcolor=lightblue]
	137614705282160 -> 137614695639568
	137614695639568 [label=AccumulateGrad]
	137614695639376 -> 137614695638464
	137614705282240 [label="layer4.2.bn3.bias
 (2048)" fillcolor=lightblue]
	137614705282240 -> 137614695639376
	137614695639376 [label=AccumulateGrad]
	137614695640000 -> 137614695638368
	137614695638512 -> 137614695638848
	137614695638512 [label=SigmoidBackward0]
	137614695639952 -> 137614695638512
	137614695639952 [label=AddBackward0]
	137614695628128 -> 137614695639952
	137614695628128 [label=ConvolutionBackward0]
	137615234726736 -> 137614695628128
	137615234726736 [label=ReluBackward0]
	137615234725920 -> 137615234726736
	137615234725920 [label=ConvolutionBackward0]
	137615234720016 -> 137615234725920
	137615234720016 [label=MeanBackward1]
	137614695638368 -> 137615234720016
	137615233397952 -> 137615234725920
	137614703479920 [label="layer4.2.cbam.ca.mlp.0.weight
 (128, 2048, 1, 1)" fillcolor=lightblue]
	137614703479920 -> 137615233397952
	137615233397952 [label=AccumulateGrad]
	137615233397712 -> 137615234725920
	137614703479680 [label="layer4.2.cbam.ca.mlp.0.bias
 (128)" fillcolor=lightblue]
	137614703479680 -> 137615233397712
	137615233397712 [label=AccumulateGrad]
	137615234726592 -> 137614695628128
	137614703479760 [label="layer4.2.cbam.ca.mlp.2.weight
 (2048, 128, 1, 1)" fillcolor=lightblue]
	137614703479760 -> 137615234726592
	137615234726592 [label=AccumulateGrad]
	137615234726832 -> 137614695628128
	137614703480000 [label="layer4.2.cbam.ca.mlp.2.bias
 (2048)" fillcolor=lightblue]
	137614703480000 -> 137615234726832
	137615234726832 [label=AccumulateGrad]
	137614695626400 -> 137614695639952
	137614695626400 [label=ConvolutionBackward0]
	137615234726016 -> 137614695626400
	137615234726016 [label=ReluBackward0]
	137615233398000 -> 137615234726016
	137615233398000 [label=ConvolutionBackward0]
	137615233398288 -> 137615233398000
	137615233398288 [label=AdaptiveMaxPool2DBackward0]
	137614695638368 -> 137615233398288
	137615233397952 -> 137615233398000
	137615233397712 -> 137615233398000
	137615234726592 -> 137614695626400
	137615234726832 -> 137614695626400
	137614695638896 -> 137614695638944
	137614695638896 [label=SigmoidBackward0]
	137614695627264 -> 137614695638896
	137614695627264 [label=NativeBatchNormBackward0]
	137615234726448 -> 137614695627264
	137615234726448 [label=ConvolutionBackward0]
	137615233398192 -> 137615234726448
	137615233398192 [label=CatBackward0]
	137615233398528 -> 137615233398192
	137615233398528 [label=MaxBackward0]
	137614695638848 -> 137615233398528
	137615233398480 -> 137615233398192
	137615233398480 [label=MeanBackward1]
	137614695638848 -> 137615233398480
	137615233398336 -> 137615234726448
	137614703480160 [label="layer4.2.cbam.sa.conv.weight
 (1, 2, 7, 7)" fillcolor=lightblue]
	137614703480160 -> 137615233398336
	137615233398336 [label=AccumulateGrad]
	137614695639616 -> 137614695627264
	137614703480240 [label="layer4.2.cbam.sa.bn.weight
 (1)" fillcolor=lightblue]
	137614703480240 -> 137614695639616
	137614695639616 [label=AccumulateGrad]
	137615233398240 -> 137614695627264
	137614703480080 [label="layer4.2.cbam.sa.bn.bias
 (1)" fillcolor=lightblue]
	137614703480080 -> 137615233398240
	137615233398240 [label=AccumulateGrad]
	137614695638752 -> 137614695639904
	137614695638752 [label=TBackward0]
	137614695639712 -> 137614695638752
	137614705630624 [label="fc.weight
 (10, 2048)" fillcolor=lightblue]
	137614705630624 -> 137614695639712
	137614695639712 [label=AccumulateGrad]
	137614695639904 -> 137615231157328
}
