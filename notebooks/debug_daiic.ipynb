{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d696db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29f549f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "repo_root = /home/user/abin_ref_papers/project_structure_demo/dnn_template\n",
      "Inserted repo_root into sys.path\n",
      "models_dir exists: True  losses_dir exists: True\n",
      "Creating in-memory 'models' package module (so relative imports work)\n",
      "Imported models.backbone OK\n",
      "Registry keys now: ['resnet34', 'daiic_resnet34', 'resnet50', 'daiic_resnet50', 'densenet121']\n",
      "Using factory: daiic_resnet34 function: <function daiic_resnet34 at 0x79350d2c0ee0>\n",
      "Loaded ALRLoss from /home/user/abin_ref_papers/project_structure_demo/dnn_template/losses/alr.py\n",
      "Device: cuda\n",
      "Instantiated model type: <class 'models.backbone.DAIICModule'>\n",
      "\n",
      "=== Shapes ===\n",
      "logits: torch.Size([32, 100])\n",
      "probs: torch.Size([32, 100])\n",
      "W: torch.Size([32, 100])\n",
      "F shape: torch.Size([32, 512, 4, 4])\n",
      "Fprime shape: torch.Size([32, 512, 4, 4])\n",
      "\n",
      "=== Basic stats ===\n",
      "logits min/max/mean: -26.30319595336914 26.12763023376465 -1.1395820379257202\n",
      "probs min/max/mean: 3.772828837539377e-12 1.0 0.42649194598197937\n",
      "W min/max/mean: 5.881404874230611e-08 0.18396776914596558 0.009999999776482582\n",
      "W per-sample sums (first 8): [1.        1.        1.        1.        1.        1.0000001 1.\n",
      " 1.0000001]\n",
      "preds (first 16): [ 7 18  7  7  7  7  7 18  7 18  7 18 18  7 18  7]\n",
      "labels (first 16): [39 98 69 10 57 83  6 96 66 90  9 57  1 18 14 18]\n",
      "\n",
      "ALR loss: 1.6384718418121338\n",
      "loss_per_class_mean first8: [1.0077030e-03 1.4755622e-04 3.1575101e-07 3.9893200e-04 4.4120368e-02\n",
      " 1.7052108e-07 3.1827661e-04 9.8818310e-02]\n",
      "W_mean_per_class first8: [1.0752375e-03 1.7415811e-04 2.7597898e-03 7.8866002e-04 2.6360409e-02\n",
      " 3.8421062e-05 2.0967147e-03 6.9788247e-03]\n",
      "pos_per_class first8: [0. 2. 0. 0. 1. 1. 1. 0.]\n",
      "CrossEntropy loss: 19.16866683959961\n",
      "\n",
      "--- CE grad-flow check ---\n",
      "CE loss (backward): 4.692339897155762\n",
      "grad for se.fc1.weight norm: 0.2564271092414856\n",
      "\n",
      "Diagnostic run complete.\n"
     ]
    }
   ],
   "source": [
    "# Paste this cell into your Jupyter notebook and run it.\n",
    "\n",
    "import sys, pathlib, importlib, traceback, types\n",
    "import importlib.util\n",
    "\n",
    "repo_root = pathlib.Path(\"/home/user/abin_ref_papers/project_structure_demo/dnn_template\").resolve()\n",
    "print(\"repo_root =\", repo_root)\n",
    "if str(repo_root) not in sys.path:\n",
    "    sys.path.insert(0, str(repo_root))\n",
    "    print(\"Inserted repo_root into sys.path\")\n",
    "\n",
    "models_dir = repo_root / \"models\"\n",
    "losses_dir = repo_root / \"losses\"\n",
    "\n",
    "print(\"models_dir exists:\", models_dir.exists(), \" losses_dir exists:\", losses_dir.exists())\n",
    "\n",
    "# --- Create an in-memory 'models' package module so relative imports inside models/*.py work ---\n",
    "if \"models\" not in sys.modules:\n",
    "    print(\"Creating in-memory 'models' package module (so relative imports work)\")\n",
    "    models_pkg = types.ModuleType(\"models\")\n",
    "    # package must provide __path__ so importlib can locate submodules (models.<name>)\n",
    "    models_pkg.__path__ = [str(models_dir)]\n",
    "    # Add a REGISTRY and register decorator (backbone.py expects `from . import register` -> register())\n",
    "    models_pkg.REGISTRY = {}\n",
    "    def _register(name):\n",
    "        def deco(fn):\n",
    "            models_pkg.REGISTRY[name] = fn\n",
    "            return fn\n",
    "        return deco\n",
    "    models_pkg.register = _register\n",
    "    # Insert into sys.modules\n",
    "    sys.modules[\"models\"] = models_pkg\n",
    "else:\n",
    "    print(\"'models' already in sys.modules (skipping in-memory package creation)\")\n",
    "\n",
    "# Now import models.backbone (and other models.* files will be loadable via normal imports)\n",
    "try:\n",
    "    m_backbone = importlib.import_module(\"models.backbone\")\n",
    "    print(\"Imported models.backbone OK\")\n",
    "except Exception as e:\n",
    "    print(\"Failed to import models.backbone via package import. Traceback:\")\n",
    "    traceback.print_exc()\n",
    "    raise\n",
    "\n",
    "# After importing backbone and any other modules that used @register decorator,\n",
    "# the in-memory models.REGISTRY should be populated with factories (if files used @register).\n",
    "import models as models_pkg  # the module we put in sys.modules\n",
    "print(\"Registry keys now:\", list(models_pkg.REGISTRY.keys())[:200])\n",
    "\n",
    "# try to find the factory for daiic_resnet34\n",
    "factory_name = None\n",
    "factory_fn = None\n",
    "for candidate in (\"daiic_resnet34\", \"daiic_resnet\", \"daiic\", \"resnet34\"):\n",
    "    if candidate in models_pkg.REGISTRY:\n",
    "        factory_name = candidate\n",
    "        factory_fn = models_pkg.REGISTRY[candidate]\n",
    "        break\n",
    "\n",
    "# also check attributes on imported modules (sometimes factories are defined as module-level functions)\n",
    "if factory_fn is None:\n",
    "    # attempt to find on loaded modules inside 'models' package\n",
    "    for name, mod in list(sys.modules.items()):\n",
    "        if name.startswith(\"models.\") and hasattr(mod, \"daiic_resnet34\"):\n",
    "            factory_fn = getattr(mod, \"daiic_resnet34\")\n",
    "            factory_name = \"daiic_resnet34 (module-level)\"\n",
    "            break\n",
    "\n",
    "if factory_fn is None:\n",
    "    # try to scan 'models' dir for a plausible file and load it (last resort)\n",
    "    print(\"Could not find a registered factory in models.REGISTRY. Scanning models directory for candidate python files...\")\n",
    "    for p in sorted(models_dir.glob(\"*.py\")):\n",
    "        try:\n",
    "            spec = importlib.util.spec_from_file_location(f\"models_scan_{p.stem}\", str(p))\n",
    "            mod = importlib.util.module_from_spec(spec)\n",
    "            spec.loader.exec_module(mod)\n",
    "            if hasattr(mod, \"daiic_resnet34\"):\n",
    "                factory_fn = getattr(mod, \"daiic_resnet34\")\n",
    "                factory_name = f\"daiic_resnet34 (from file {p.name})\"\n",
    "                print(\"Found factory in\", p)\n",
    "                break\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "if factory_fn is None:\n",
    "    raise RuntimeError(\"Failed to locate a daiic_resnet34 factory in models package. Please paste the output of: `ls models` and the first 200 chars of models/backbone.py`.\")\n",
    "\n",
    "print(\"Using factory:\", factory_name, \"function:\", factory_fn)\n",
    "\n",
    "# --- Load ALRLoss from losses/alr.py ---\n",
    "alr_path = losses_dir / \"alr.py\"\n",
    "if not alr_path.exists():\n",
    "    raise FileNotFoundError(f\"Couldn't find {alr_path}; adjust path (I saw earlier ALRLoss in losses/alr.py).\")\n",
    "spec = importlib.util.spec_from_file_location(\"local_losses_alr\", str(alr_path))\n",
    "mod_alr = importlib.util.module_from_spec(spec)\n",
    "spec.loader.exec_module(mod_alr)\n",
    "if not hasattr(mod_alr, \"ALRLoss\"):\n",
    "    raise RuntimeError(f\"{alr_path} loaded but ALRLoss symbol not found. Available names: {[n for n in dir(mod_alr) if not n.startswith('_')][:200]}\")\n",
    "ALRLoss = getattr(mod_alr, \"ALRLoss\")\n",
    "print(\"Loaded ALRLoss from\", alr_path)\n",
    "\n",
    "# === instantiate model and run a one-batch diagnostic ===\n",
    "import torch, torch.nn as nn, torchvision, torchvision.transforms as T\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "\n",
    "# instantiate model\n",
    "model = factory_fn(num_classes=100, pretrained=False, cifar_stem=True, in_channels=3)\n",
    "model = model.to(device)\n",
    "print(\"Instantiated model type:\", type(model))\n",
    "\n",
    "# instantiate ALR and optimizer\n",
    "criterion_alr = ALRLoss(class_weights=None, reduction=\"mean\")\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=1e-4)\n",
    "\n",
    "# tiny dataloader\n",
    "transform = T.Compose([T.ToTensor()])\n",
    "train_ds = torchvision.datasets.CIFAR100(root=str(repo_root / \"data\"), train=True, download=True, transform=transform)\n",
    "train_loader = DataLoader(train_ds, batch_size=32, shuffle=True, num_workers=2)\n",
    "\n",
    "# fetch batch\n",
    "x, y = next(iter(train_loader))\n",
    "x = x.to(device); y = y.to(device)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(x)\n",
    "\n",
    "# normalize outputs as Trainer would\n",
    "def normalize_outputs(outputs):\n",
    "    logits = None; W = None\n",
    "    if isinstance(outputs, dict):\n",
    "        logits = outputs.get(\"logits\", None)\n",
    "        if logits is None and \"probs\" in outputs:\n",
    "            probs = outputs[\"probs\"].clamp(1e-6, 1-1e-6)\n",
    "            logits = torch.log(probs / (1.0 - probs))\n",
    "        W = outputs.get(\"W\", None)\n",
    "    elif torch.is_tensor(outputs):\n",
    "        logits = outputs\n",
    "    else:\n",
    "        raise RuntimeError(\"Unsupported outputs type\")\n",
    "    logits = logits.to(device).float()\n",
    "    if W is not None:\n",
    "        W = W.to(device).float()\n",
    "        if not torch.isfinite(W).all():\n",
    "            W = torch.ones_like(logits)\n",
    "        W = W.clamp(min=0.0)\n",
    "        rs = W.sum(dim=1, keepdim=True)\n",
    "        zero_rows = rs == 0\n",
    "        if zero_rows.any():\n",
    "            W[zero_rows.expand_as(W)] = 1.0\n",
    "            rs = W.sum(dim=1, keepdim=True)\n",
    "        W = W / (rs + 1e-12)\n",
    "        if not torch.isfinite(W).all() or (W.abs().max() < 1e-12):\n",
    "            W = torch.ones_like(logits) / float(logits.size(1))\n",
    "    return logits, W, outputs\n",
    "\n",
    "logits, W, full = normalize_outputs(outputs)\n",
    "\n",
    "print(\"\\n=== Shapes ===\")\n",
    "print(\"logits:\", logits.shape)\n",
    "print(\"probs:\", torch.sigmoid(logits).shape)\n",
    "print(\"W:\", None if W is None else W.shape)\n",
    "if isinstance(full, dict):\n",
    "    for k in (\"F\",\"Fprime\"):\n",
    "        if k in full:\n",
    "            try:\n",
    "                print(k, \"shape:\", full[k].shape)\n",
    "            except Exception:\n",
    "                print(k, \"present but couldn't read shape\")\n",
    "\n",
    "probs = torch.sigmoid(logits)\n",
    "print(\"\\n=== Basic stats ===\")\n",
    "print(\"logits min/max/mean:\", float(logits.min()), float(logits.max()), float(logits.mean()))\n",
    "print(\"probs min/max/mean:\", float(probs.min()), float(probs.max()), float(probs.mean()))\n",
    "if W is not None:\n",
    "    print(\"W min/max/mean:\", float(W.min()), float(W.max()), float(W.mean()))\n",
    "    print(\"W per-sample sums (first 8):\", W.sum(dim=1)[:8].detach().cpu().numpy())\n",
    "\n",
    "preds = logits.argmax(dim=1)\n",
    "y_idx = y.argmax(dim=1) if y.dim()>1 else y.view(-1)\n",
    "print(\"preds (first 16):\", preds[:16].cpu().numpy())\n",
    "print(\"labels (first 16):\", y_idx[:16].cpu().numpy())\n",
    "\n",
    "# ALR loss\n",
    "if W is None:\n",
    "    print(\"Model did not return W -> using uniform W\")\n",
    "    W = torch.ones_like(logits) / logits.size(1)\n",
    "\n",
    "try:\n",
    "    res = criterion_alr(logits, y, W)\n",
    "    if isinstance(res, (tuple,list)):\n",
    "        alr_loss = res[0]; alr_info = res[1] if len(res)>1 else {}\n",
    "    else:\n",
    "        alr_loss = res; alr_info = {}\n",
    "    print(\"\\nALR loss:\", float(alr_loss.item()))\n",
    "    if alr_info:\n",
    "        for k in (\"loss_per_class_mean\",\"W_mean_per_class\",\"pos_per_class\"):\n",
    "            if k in alr_info:\n",
    "                v = alr_info[k]\n",
    "                try:\n",
    "                    print(k, \"first8:\", v[:8].detach().cpu().numpy())\n",
    "                except Exception:\n",
    "                    print(k, \":\", v)\n",
    "except Exception as e:\n",
    "    print(\"ALR call failed:\", e)\n",
    "    traceback.print_exc()\n",
    "\n",
    "# CE comparison\n",
    "try:\n",
    "    ce_loss = nn.CrossEntropyLoss()(logits, y_idx)\n",
    "    print(\"CrossEntropy loss:\", float(ce_loss.item()))\n",
    "except Exception as e:\n",
    "    print(\"CE failed:\", e)\n",
    "\n",
    "# CE grad-flow check\n",
    "print(\"\\n--- CE grad-flow check ---\")\n",
    "model.train()\n",
    "optimizer.zero_grad()\n",
    "outputs2 = model(x)\n",
    "logits2, W2, _ = normalize_outputs(outputs2)\n",
    "ce_loss2 = nn.CrossEntropyLoss()(logits2, y_idx)\n",
    "ce_loss2.backward()\n",
    "print(\"CE loss (backward):\", float(ce_loss2.item()))\n",
    "for name,p in model.named_parameters():\n",
    "    if ('fc' in name or 'classifier' in name or 'head' in name) and p.requires_grad:\n",
    "        print(\"grad for\", name, \"norm:\", 0.0 if p.grad is None else float(p.grad.detach().norm()))\n",
    "        break\n",
    "optimizer.step()\n",
    "\n",
    "print(\"\\nDiagnostic run complete.\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48fb9d89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a77e787e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Re-init CostAttention.conv1x1 -> std=0.001, zero_bias=True\n",
      "Re-init CostAttention.conv1x1 -> std=0.001, zero_bias=True\n"
     ]
    }
   ],
   "source": [
    "# Run inside your notebook (after you created `model` via daiic factory)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def reinit_cost_attention(model, init_std=1e-3, zero_bias=True, verbose=True):\n",
    "    \"\"\"\n",
    "    Re-initialize CostAttention conv weights small and zero bias.\n",
    "    - model: the DAIICModule instance (or container holding it)\n",
    "    - init_std: std dev for normal init (try 1e-3, 1e-4)\n",
    "    \"\"\"\n",
    "    # try common attribute paths\n",
    "    candidates = []\n",
    "    if hasattr(model, \"cost_att\"):\n",
    "        candidates.append(model.cost_att)\n",
    "    # sometimes model may be wrapper: try deeper search\n",
    "    for name, module in model.named_modules():\n",
    "        # detect by attribute/class name\n",
    "        if module.__class__.__name__.lower().startswith(\"costattention\") or getattr(module, \"conv1x1\", None) is not None:\n",
    "            candidates.append(module)\n",
    "\n",
    "    touched = 0\n",
    "    for module in candidates:\n",
    "        conv = getattr(module, \"conv1x1\", None)\n",
    "        if conv is None:\n",
    "            continue\n",
    "        if isinstance(conv, nn.Conv2d):\n",
    "            nn.init.normal_(conv.weight, mean=0.0, std=float(init_std))\n",
    "            if zero_bias and conv.bias is not None:\n",
    "                nn.init.constant_(conv.bias, 0.0)\n",
    "            touched += 1\n",
    "            if verbose:\n",
    "                print(f\"Re-init {module.__class__.__name__}.conv1x1 -> std={init_std}, zero_bias={zero_bias}\")\n",
    "    if touched == 0 and verbose:\n",
    "        print(\"No CostAttention.conv1x1 found on model. Inspect model.named_modules() to find proper path.\")\n",
    "    return touched\n",
    "\n",
    "# Example usage (choose small std)\n",
    "reinit_cost_attention(model, init_std=1e-3, zero_bias=True)\n",
    "# Optionally combine with increased softmax temperature if implemented:\n",
    "if hasattr(model, \"cost_att\") and hasattr(model.cost_att, \"tau\"):\n",
    "    model.cost_att.tau = 8.0\n",
    "    print(\"Set cost_att.tau =\", model.cost_att.tau)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae91b6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea8a0e43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W mean: 0.009999999776482582 W top1 mean: 0.9621791839599609 W entropy mean: 0.17686697840690613\n",
      "W rows (first 4, first 10 cols):\n",
      "[[2.8333790e-04 1.7396493e-10 1.7888036e-08 8.3736307e-07 1.1745243e-06\n",
      "  4.2493301e-10 7.0379946e-07 1.9076278e-09 5.4415160e-08 3.6905321e-09]\n",
      " [2.3198091e-04 2.8960329e-10 1.9363863e-08 6.2880554e-07 1.5742845e-06\n",
      "  3.7805695e-10 7.7095638e-07 1.5517209e-09 4.9369703e-08 4.4073589e-09]\n",
      " [2.2140451e-04 1.2493841e-10 1.4881882e-08 4.2759348e-07 7.4848361e-07\n",
      "  2.5877844e-10 5.5893832e-07 9.8466801e-10 5.5105289e-08 2.9031730e-09]\n",
      " [2.6897390e-04 2.8882699e-10 2.7090564e-08 1.1810084e-06 2.5095926e-06\n",
      "  4.9874666e-10 1.0366307e-06 3.7599381e-09 7.1720621e-08 5.0822404e-09]]\n"
     ]
    }
   ],
   "source": [
    "# Run after re-init or after changing file + reloading model\n",
    "import torch, torch.nn.functional as F, numpy as np\n",
    "# create or re-create model:\n",
    "model = models_pkg.REGISTRY[\"daiic_resnet34\"](num_classes=100, pretrained=False, cifar_stem=True, in_channels=3).to('cuda')\n",
    "# if you used monkeypatch reinit on previous model instance, use that; otherwise it's a fresh new model\n",
    "model.eval()\n",
    "x = torch.randn(8,3,32,32).cuda()\n",
    "with torch.no_grad():\n",
    "    out = model(x)\n",
    "W = out.get(\"W\", None)\n",
    "if W is None:\n",
    "    print(\"Model did not return W\")\n",
    "else:\n",
    "    probs = out.get(\"probs\", None)\n",
    "    logits = out.get(\"logits\", None)\n",
    "    # compute entropy\n",
    "    def entropy(p):\n",
    "        p = p / (p.sum(dim=1, keepdim=True)+1e-12)\n",
    "        return -(p * (p.clamp(1e-12).log())).sum(dim=1)\n",
    "    W_top1_val, W_top1_idx = W.max(dim=1)\n",
    "    print(\"W mean:\", float(W.mean()), \"W top1 mean:\", float(W_top1_val.mean()), \"W entropy mean:\", float(entropy(W).mean()))\n",
    "    print(\"W rows (first 4, first 10 cols):\")\n",
    "    print(W[:4,:10].cpu().numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1814cd6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patched cost_att (class CostAttention): init_std=0.0001, tau=None\n",
      "W mean: 0.009999999776482582 W top1 mean: 0.010759033262729645 W entropy mean: 4.60485315322876\n",
      "Example W rows (first 4, first 10 cols):\n",
      "[[0.01024134 0.00991709 0.01001475 0.00944132 0.00979228 0.00963728\n",
      "  0.00999563 0.00955502 0.00984503 0.00963696]\n",
      " [0.01025479 0.00991656 0.01002092 0.00943761 0.00977195 0.00961552\n",
      "  0.00999963 0.00952794 0.00982326 0.00963264]\n",
      " [0.01026538 0.00992124 0.0100203  0.00944282 0.00978252 0.00962164\n",
      "  0.00999355 0.0095411  0.00985025 0.0096305 ]\n",
      " [0.01025944 0.00991534 0.01001347 0.0094259  0.00979426 0.00961505\n",
      "  0.00998644 0.00952348 0.00983254 0.00962814]]\n"
     ]
    }
   ],
   "source": [
    "# MONKEYPATCH: reinit conv smaller and set tau larger, then test W\n",
    "import torch, torch.nn as nn, importlib\n",
    "import numpy as np\n",
    "\n",
    "# get fresh model (or reuse 'model' if you have one)\n",
    "# model = models_pkg.REGISTRY[\"daiic_resnet34\"](num_classes=100, pretrained=False, cifar_stem=True).cuda()\n",
    "# If you already have `model`, skip creating new one.\n",
    "\n",
    "def apply_costatt_fix(model, init_std=1e-4, tau=10.0, zero_bias=True, verbose=True):\n",
    "    touched = 0\n",
    "    for name, module in model.named_modules():\n",
    "        if module.__class__.__name__.lower().startswith(\"costattention\") or hasattr(module, \"conv1x1\"):\n",
    "            conv = getattr(module, \"conv1x1\", None)\n",
    "            if isinstance(conv, nn.Conv2d):\n",
    "                nn.init.normal_(conv.weight, mean=0.0, std=float(init_std))\n",
    "                if zero_bias and conv.bias is not None:\n",
    "                    nn.init.constant_(conv.bias, 0.0)\n",
    "                # set tau if available\n",
    "                if hasattr(module, \"tau\"):\n",
    "                    module.tau = float(tau)\n",
    "                # store a flag if needed\n",
    "                setattr(module, \"_patched_init_std\", float(init_std))\n",
    "                touched += 1\n",
    "                if verbose:\n",
    "                    print(f\"Patched {name} (class {module.__class__.__name__}): init_std={init_std}, tau={getattr(module,'tau',None)}\")\n",
    "    if touched == 0:\n",
    "        print(\"No CostAttention-like module found. Inspect model.named_modules().\")\n",
    "    return touched\n",
    "\n",
    "# Apply to your current model (or create/recreate model then apply)\n",
    "apply_costatt_fix(model, init_std=1e-4, tau=10.0)\n",
    "\n",
    "# quick W diagnostic on a random minibatch (or reuse your x,y)\n",
    "model.eval()\n",
    "x = torch.randn(8,3,32,32).cuda()\n",
    "with torch.no_grad():\n",
    "    out = model(x)\n",
    "W = out.get(\"W\")\n",
    "def entropy(p):\n",
    "    p = p / (p.sum(dim=1, keepdim=True)+1e-12)\n",
    "    return -(p * (p.clamp(1e-12).log())).sum(dim=1)\n",
    "W_top1_val = W.max(dim=1).values\n",
    "print(\"W mean:\", float(W.mean()), \"W top1 mean:\", float(W_top1_val.mean()), \"W entropy mean:\", float(entropy(W).mean()))\n",
    "print(\"Example W rows (first 4, first 10 cols):\")\n",
    "print(W[:4,:10].cpu().numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "49c5a023",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "Starting CE warmup for 2 epochs\n",
      "  step 100 | loss 5.2701 acc 0.0148\n",
      "  step 200 | loss 4.8016 acc 0.0287\n",
      "  step 300 | loss 4.5323 acc 0.0449\n",
      "CE epoch 1/2 | train_loss 4.3671 train_acc 0.0586 | val_loss 3.7816 val_acc 0.1060 | time 22.1s\n",
      "  W stats: {'W_mean': 0.009999999776482582, 'W_top1_mean': 0.010772624984383583, 'W_entropy_mean': 4.604591369628906}\n",
      "  step 100 | loss 3.6845 acc 0.1230\n",
      "  step 200 | loss 3.6238 acc 0.1350\n",
      "  step 300 | loss 3.5465 acc 0.1471\n",
      "CE epoch 2/2 | train_loss 3.4905 train_acc 0.1575 | val_loss 3.1812 val_acc 0.2135 | time 22.5s\n",
      "  W stats: {'W_mean': 0.009999999776482582, 'W_top1_mean': 0.010868331417441368, 'W_entropy_mean': 4.60463809967041}\n",
      "Starting ALR phase for 6 epochs\n",
      "  step 100 | loss 8.2516 acc 0.0151\n",
      "  step 200 | loss 6.9176 acc 0.0163\n",
      "  step 300 | loss 7.1837 acc 0.0156\n",
      "ALR epoch 1/6 | train_loss 6.8876 train_acc 0.0158 | val_loss 5.2087 val_acc 0.0162 | time 24.0s\n",
      "  W stats: {'W_mean': 0.009999999776482582, 'W_top1_mean': 0.13355275988578796, 'W_entropy_mean': 4.077324867248535}\n",
      "  step 100 | loss 5.2129 acc 0.0152\n",
      "  step 200 | loss 6.0244 acc 0.0130\n",
      "  step 300 | loss 5.4283 acc 0.0130\n",
      "ALR epoch 2/6 | train_loss 5.1637 train_acc 0.0120 | val_loss 7.1162 val_acc 0.0033 | time 24.2s\n",
      "  W stats: {'W_mean': 0.009999999776482582, 'W_top1_mean': 0.919564425945282, 'W_entropy_mean': 0.44442200660705566}\n",
      "  step 100 | loss 4.4566 acc 0.0075\n",
      "  step 200 | loss 4.0516 acc 0.0084\n",
      "  step 300 | loss 4.1660 acc 0.0084\n",
      "ALR epoch 3/6 | train_loss 3.4378 train_acc 0.0091 | val_loss 0.6771 val_acc 0.0102 | time 24.1s\n",
      "  W stats: {'W_mean': 0.009999999776482582, 'W_top1_mean': 0.9808962941169739, 'W_entropy_mean': 0.10544949769973755}\n",
      "  step 100 | loss 0.5703 acc 0.0102\n",
      "  step 200 | loss 0.8702 acc 0.0095\n",
      "  step 300 | loss 0.6063 acc 0.0102\n",
      "ALR epoch 4/6 | train_loss 0.5812 train_acc 0.0106 | val_loss 0.3783 val_acc 0.0123 | time 24.3s\n",
      "  W stats: {'W_mean': 0.009999999776482582, 'W_top1_mean': 0.9883874654769897, 'W_entropy_mean': 0.04414065554738045}\n",
      "  step 100 | loss 0.2881 acc 0.0110\n",
      "  step 200 | loss 0.4866 acc 0.0113\n",
      "  step 300 | loss 0.4272 acc 0.0115\n",
      "ALR epoch 5/6 | train_loss 0.4419 train_acc 0.0117 | val_loss 1.1124 val_acc 0.0116 | time 24.4s\n",
      "  W stats: {'W_mean': 0.009999999776482582, 'W_top1_mean': 0.9825274348258972, 'W_entropy_mean': 0.05045834928750992}\n",
      "  step 100 | loss 0.2917 acc 0.0125\n",
      "  step 200 | loss 0.2329 acc 0.0123\n",
      "  step 300 | loss 0.2996 acc 0.0114\n",
      "ALR epoch 6/6 | train_loss 0.3404 train_acc 0.0109 | val_loss 0.1909 val_acc 0.0075 | time 24.0s\n",
      "  W stats: {'W_mean': 0.009999999776482582, 'W_top1_mean': 0.9902662038803101, 'W_entropy_mean': 0.02548365667462349}\n"
     ]
    }
   ],
   "source": [
    "# Paste & run in your notebook\n",
    "import time, math\n",
    "import torch, torch.nn as nn, torchvision, torchvision.transforms as T\n",
    "from torch.utils.data import DataLoader\n",
    "import models as models_pkg\n",
    "import importlib, importlib.util, pathlib\n",
    "\n",
    "repo_root = pathlib.Path(\"/home/user/abin_ref_papers/project_structure_demo/dnn_template\").resolve()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "\n",
    "# factory and ALR\n",
    "daiic_factory = models_pkg.REGISTRY[\"daiic_resnet34\"]\n",
    "\n",
    "# load ALRLoss from losses\n",
    "spec = importlib.util.spec_from_file_location(\"local_alr\", str(repo_root / \"losses\" / \"alr.py\"))\n",
    "mod_alr = importlib.util.module_from_spec(spec)\n",
    "spec.loader.exec_module(mod_alr)\n",
    "ALRLoss = mod_alr.ALRLoss\n",
    "\n",
    "# hyperparams (feel free to tune)\n",
    "num_classes = 100\n",
    "batch_size = 128\n",
    "warmup_epochs = 2           # CE warmup\n",
    "alr_epochs = 6              # ALR training after warmup (increase later)\n",
    "lr_base = 0.01\n",
    "lr_head = 0.1\n",
    "weight_decay = 1e-4\n",
    "lambda_ent = 0.0            # 0.0 unless you want entropy reg\n",
    "alr_scale = float(num_classes)  # scale ALR to match CE magnitude (tweak)\n",
    "\n",
    "# dataloaders (small quick transforms)\n",
    "transform = T.Compose([T.ToTensor(), T.Normalize((0.5071,0.4867,0.4408),(0.2675,0.2565,0.2761))])\n",
    "train_ds = torchvision.datasets.CIFAR100(root=str(repo_root / \"data\"), train=True, download=True, transform=transform)\n",
    "val_ds = torchvision.datasets.CIFAR100(root=str(repo_root / \"data\"), train=False, download=True, transform=transform)\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=256, shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "# utils\n",
    "def instantiate_model():\n",
    "    model = daiic_factory(num_classes=num_classes, pretrained=False, cifar_stem=True, in_channels=3).to(device)\n",
    "    # if model has cost_att and tau attr, you can set initial tau here:\n",
    "    for name, m in model.named_modules():\n",
    "        if m.__class__.__name__.lower().startswith(\"costattention\") and hasattr(m, \"tau\"):\n",
    "            # start with high tau (very soft), will anneal later\n",
    "            m.tau = 8.0\n",
    "    return model\n",
    "\n",
    "def make_optimizer(model):\n",
    "    head_params, base_params = [], []\n",
    "    for n, p in model.named_parameters():\n",
    "        if not p.requires_grad: continue\n",
    "        if (\"classifier\" in n) or (\"fc\" in n) or (\"head\" in n):\n",
    "            head_params.append(p)\n",
    "        else:\n",
    "            base_params.append(p)\n",
    "    groups = []\n",
    "    if base_params: groups.append({'params': base_params, 'lr': lr_base})\n",
    "    if head_params: groups.append({'params': head_params, 'lr': lr_head})\n",
    "    if not groups: groups = [{'params': model.parameters(), 'lr': lr_base}]\n",
    "    opt = torch.optim.SGD(groups, momentum=0.9, weight_decay=weight_decay)\n",
    "    return opt\n",
    "\n",
    "def W_stats(W):\n",
    "    if W is None: return {}\n",
    "    top1 = float(W.max(dim=1).values.mean())\n",
    "    entropy = float((-(W*(W.clamp(1e-12).log())).sum(dim=1)).mean())\n",
    "    return {'W_mean': float(W.mean()), 'W_top1_mean': top1, 'W_entropy_mean': entropy}\n",
    "\n",
    "# simple train loop for a given loss mode\n",
    "def train_one_epoch(model, opt, loss_mode='ce', epoch=0, total_epochs=1, alr_loss_fn=None, scheduler_tau=None):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    running_acc = 0.0\n",
    "    n = 0\n",
    "    for i, (x,y) in enumerate(train_loader):\n",
    "        x = x.to(device); y = y.to(device)\n",
    "        opt.zero_grad()\n",
    "        out = model(x)\n",
    "        logits = out.get('logits')\n",
    "        W = out.get('W')\n",
    "        # normalize if necessary (but assume W already normalized by module)\n",
    "        if loss_mode == 'ce':\n",
    "            y_idx = y.view(-1)\n",
    "            loss = nn.CrossEntropyLoss()(logits, y_idx)\n",
    "        elif loss_mode == 'alr':\n",
    "            # scale ALR to have comparable magnitude to CE\n",
    "            raw = alr_loss_fn(logits, y, W)\n",
    "            if isinstance(raw, (tuple, list)):\n",
    "                raw = raw[0]\n",
    "            loss = raw * alr_scale\n",
    "            # optional entropy reg (encourage non-peaky)\n",
    "            if lambda_ent > 0.0:\n",
    "                ent = (-(W * (W.clamp(1e-12).log())).sum(dim=1)).mean()\n",
    "                loss = loss - lambda_ent * ent\n",
    "        else:\n",
    "            raise ValueError\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        # optionally anneal tau per step if scheduler provided\n",
    "        if scheduler_tau is not None:\n",
    "            scheduler_tau(step=(epoch + i/len(train_loader)), total_epochs=total_epochs)\n",
    "\n",
    "        bs = x.size(0)\n",
    "        running_loss += float(loss.item()) * bs\n",
    "        pred = logits.argmax(dim=1)\n",
    "        y_idx = y.view(-1)\n",
    "        running_acc += int((pred == y_idx).sum().item())\n",
    "        n += bs\n",
    "\n",
    "        if (i+1) % 100 == 0:\n",
    "            print(f\"  step {i+1} | loss {running_loss/n:.4f} acc {running_acc/n:.4f}\")\n",
    "\n",
    "    return running_loss / max(1,n), running_acc / max(1,n)\n",
    "\n",
    "def eval_one_epoch(model, loss_mode='ce', alr_loss_fn=None, max_batches=50):\n",
    "    model.eval()\n",
    "    loss_sum = 0.0; correct = 0; n=0\n",
    "    with torch.no_grad():\n",
    "        for i,(x,y) in enumerate(val_loader):\n",
    "            x = x.to(device); y = y.to(device)\n",
    "            out = model(x)\n",
    "            logits = out.get('logits'); W = out.get('W')\n",
    "            if loss_mode=='ce':\n",
    "                l = nn.CrossEntropyLoss()(logits, y.view(-1)).item()\n",
    "            else:\n",
    "                raw = alr_loss_fn(logits,y,W)\n",
    "                if isinstance(raw, (tuple,list)): raw = raw[0]\n",
    "                l = float(raw.item())*alr_scale\n",
    "            loss_sum += l * x.size(0)\n",
    "            pred = logits.argmax(dim=1)\n",
    "            correct += int((pred == y.view(-1)).sum().item())\n",
    "            n += x.size(0)\n",
    "            if i >= max_batches: break\n",
    "    return loss_sum/max(1,n), correct/max(1,n)\n",
    "\n",
    "# instantiate\n",
    "model = instantiate_model()\n",
    "opt = make_optimizer(model)\n",
    "alr = ALRLoss(class_weights=None, reduction='mean')\n",
    "\n",
    "# simple tau annealing scheduler function (optional)\n",
    "def make_tau_scheduler(module, tau_start=8.0, tau_end=1.0, total_epochs=10):\n",
    "    def scheduler(step, total_epochs=total_epochs):\n",
    "        # step: fractional epoch (e.g., epoch + iter/len)\n",
    "        frac = min(1.0, max(0.0, step/float(total_epochs)))\n",
    "        tau = tau_start + (tau_end - tau_start) * frac\n",
    "        # apply to all cost_att modules\n",
    "        for name, m in module.named_modules():\n",
    "            if m.__class__.__name__.lower().startswith(\"costattention\") and hasattr(m, 'tau'):\n",
    "                m.tau = tau\n",
    "    return scheduler\n",
    "\n",
    "tau_sched = make_tau_scheduler(model, tau_start=8.0, tau_end=1.0, total_epochs=(warmup_epochs+alr_epochs))\n",
    "\n",
    "# === Warmup (CE) ===\n",
    "print(\"Starting CE warmup for\", warmup_epochs, \"epochs\")\n",
    "for e in range(warmup_epochs):\n",
    "    t0 = time.time()\n",
    "    train_loss, train_acc = train_one_epoch(model, opt, loss_mode='ce', epoch=e, total_epochs=warmup_epochs+alr_epochs)\n",
    "    val_loss, val_acc = eval_one_epoch(model, loss_mode='ce')\n",
    "    print(f\"CE epoch {e+1}/{warmup_epochs} | train_loss {train_loss:.4f} train_acc {train_acc:.4f} | val_loss {val_loss:.4f} val_acc {val_acc:.4f} | time {(time.time()-t0):.1f}s\")\n",
    "    # print W stats\n",
    "    with torch.no_grad():\n",
    "        out = model(next(iter(val_loader))[0].to(device))\n",
    "        W = out.get('W')\n",
    "        print(\"  W stats:\", W_stats(W))\n",
    "\n",
    "# === ALR training with tau anneal ===\n",
    "print(\"Starting ALR phase for\", alr_epochs, \"epochs\")\n",
    "for e in range(alr_epochs):\n",
    "    t0 = time.time()\n",
    "    train_loss, train_acc = train_one_epoch(model, opt, loss_mode='alr', epoch=warmup_epochs+e, total_epochs=warmup_epochs+alr_epochs, alr_loss_fn=alr, scheduler_tau=tau_sched)\n",
    "    val_loss, val_acc = eval_one_epoch(model, loss_mode='alr', alr_loss_fn=alr)\n",
    "    print(f\"ALR epoch {e+1}/{alr_epochs} | train_loss {train_loss:.4f} train_acc {train_acc:.4f} | val_loss {val_loss:.4f} val_acc {val_acc:.4f} | time {(time.time()-t0):.1f}s\")\n",
    "    with torch.no_grad():\n",
    "        out = model(next(iter(val_loader))[0].to(device))\n",
    "        print(\"  W stats:\", W_stats(out.get('W')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3eab385",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "040a36af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda repo_root: /home/user/abin_ref_papers/project_structure_demo/dnn_template\n",
      "set_costatt_requires_grad -> touched: 1 req: False\n",
      "\n",
      "=== CE Warmup ===\n",
      "  step 200 | avg_loss 4.9911 | avg_acc 0.0212\n",
      "CE epoch 1/3 | train_loss 4.5498 train_acc 0.0417 | val_loss 3.9484 val_acc 0.0855 | time 23.0s\n",
      "  W stats: {'W_mean': 0.009999999776482582, 'W_top1_mean': 0.010767602361738682, 'W_entropy_mean': 4.604740619659424}\n",
      "  step 200 | avg_loss 3.7797 | avg_acc 0.1093\n",
      "CE epoch 2/3 | train_loss 3.6698 train_acc 0.1259 | val_loss 3.5143 val_acc 0.1570 | time 23.2s\n",
      "  W stats: {'W_mean': 0.009999999776482582, 'W_top1_mean': 0.01086423359811306, 'W_entropy_mean': 4.604649543762207}\n",
      "  step 200 | avg_loss 3.2888 | avg_acc 0.1921\n",
      "CE epoch 3/3 | train_loss 3.1943 train_acc 0.2086 | val_loss 3.0100 val_acc 0.2446 | time 24.5s\n",
      "  W stats: {'W_mean': 0.009999999776482582, 'W_top1_mean': 0.010853853076696396, 'W_entropy_mean': 4.60462760925293}\n",
      "set_costatt_requires_grad -> touched: 1 req: True\n",
      "\n",
      "=== ALR Phase ===\n",
      "  step 200 | avg_loss 6.5998 | avg_acc 0.0199\n",
      "ALR epoch 1/10 | tau 20.000 | force_uniform True | train_loss 5.8529 train_acc 0.0234 | val_loss 5.3937 val_acc 0.0334 | time 25.0s\n",
      "  W stats: {'W_mean': 0.009999999776482582, 'W_top1_mean': 0.010586801916360855, 'W_entropy_mean': 4.6049699783325195}\n",
      "  step 200 | avg_loss 4.6713 | avg_acc 0.0413\n",
      "ALR epoch 2/10 | tau 17.889 | force_uniform False | train_loss 3.1092 train_acc 0.0379 | val_loss 0.3086 val_acc 0.0205 | time 25.3s\n",
      "  W stats: {'W_mean': 0.009999999776482582, 'W_top1_mean': 0.9766806364059448, 'W_entropy_mean': 0.13746079802513123}\n",
      "  step 200 | avg_loss 0.2187 | avg_acc 0.0219\n",
      "ALR epoch 3/10 | tau 15.778 | force_uniform False | train_loss 0.2902 train_acc 0.0209 | val_loss 0.3379 val_acc 0.0116 | time 25.5s\n",
      "  W stats: {'W_mean': 0.009999999776482582, 'W_top1_mean': 0.9720065593719482, 'W_entropy_mean': 0.16308732330799103}\n",
      "  step 200 | avg_loss 0.1642 | avg_acc 0.0138\n",
      "ALR epoch 4/10 | tau 13.667 | force_uniform False | train_loss 0.3812 train_acc 0.0124 | val_loss 1.1529 val_acc 0.0153 | time 25.3s\n",
      "  W stats: {'W_mean': 0.009999999776482582, 'W_top1_mean': 0.9765932559967041, 'W_entropy_mean': 0.14457657933235168}\n",
      "  step 200 | avg_loss 1.0490 | avg_acc 0.0139\n",
      "ALR epoch 5/10 | tau 11.556 | force_uniform False | train_loss 0.7490 train_acc 0.0145 | val_loss 0.1862 val_acc 0.0135 | time 25.3s\n",
      "  W stats: {'W_mean': 0.009999999776482582, 'W_top1_mean': 0.9981817603111267, 'W_entropy_mean': 0.005699391011148691}\n",
      "  step 200 | avg_loss 0.5044 | avg_acc 0.0139\n",
      "ALR epoch 6/10 | tau 9.444 | force_uniform False | train_loss 0.3689 train_acc 0.0146 | val_loss 0.1850 val_acc 0.0140 | time 25.3s\n",
      "  W stats: {'W_mean': 0.009999999776482582, 'W_top1_mean': 0.9993529915809631, 'W_entropy_mean': 0.0025778692215681076}\n",
      "  step 200 | avg_loss 0.3136 | avg_acc 0.0138\n",
      "ALR epoch 7/10 | tau 7.333 | force_uniform False | train_loss 0.5627 train_acc 0.0141 | val_loss 0.1945 val_acc 0.0140 | time 25.2s\n",
      "  W stats: {'W_mean': 0.009999999776482582, 'W_top1_mean': 0.9985806941986084, 'W_entropy_mean': 0.011071646586060524}\n",
      "  step 200 | avg_loss 0.3824 | avg_acc 0.0150\n",
      "ALR epoch 8/10 | tau 5.222 | force_uniform False | train_loss 0.2336 train_acc 0.0148 | val_loss 0.1785 val_acc 0.0142 | time 24.6s\n",
      "  W stats: {'W_mean': 0.009999999776482582, 'W_top1_mean': 0.9970260262489319, 'W_entropy_mean': 0.009015051648020744}\n",
      "  step 200 | avg_loss 0.0729 | avg_acc 0.0152\n",
      "ALR epoch 9/10 | tau 3.111 | force_uniform False | train_loss 0.1483 train_acc 0.0142 | val_loss 0.0039 val_acc 0.0144 | time 24.9s\n",
      "  W stats: {'W_mean': 0.009999999776482582, 'W_top1_mean': 0.999836802482605, 'W_entropy_mean': 0.001342226518318057}\n",
      "  step 200 | avg_loss 0.0727 | avg_acc 0.0144\n",
      "ALR epoch 10/10 | tau 1.000 | force_uniform False | train_loss 0.1112 train_acc 0.0142 | val_loss 0.0170 val_acc 0.0143 | time 25.2s\n",
      "  W stats: {'W_mean': 0.009999999776482582, 'W_top1_mean': 0.9999669194221497, 'W_entropy_mean': 0.0004315774713177234}\n",
      "\n",
      "Training run complete. Monitor W_top1_mean and W_entropy_mean; if W_top1 climbs too fast, increase tau or lambda_ent, or force longer uniform W.\n"
     ]
    }
   ],
   "source": [
    "# === Paste & run this entire cell in your Jupyter notebook ===\n",
    "import time, math, pathlib, importlib, importlib.util\n",
    "import torch, torch.nn as nn, torchvision, torchvision.transforms as T\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "repo_root = pathlib.Path(\"/home/user/abin_ref_papers/project_structure_demo/dnn_template\").resolve()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device, \"repo_root:\", repo_root)\n",
    "\n",
    "# === Hyperparameters - tune these ===\n",
    "num_classes = 100\n",
    "batch_size = 128\n",
    "warmup_epochs = 3               # CE warmup epochs (freeze cost-att during these)\n",
    "alr_epochs = 10                 # ALR training epochs (after warmup)\n",
    "lr_base = 0.01\n",
    "lr_head = 0.1\n",
    "weight_decay = 1e-4\n",
    "\n",
    "# ALR specifics\n",
    "alr_scale = float(num_classes)   # scale ALR to make magnitudes comparable to CE\n",
    "lambda_ent = 0.1                 # entropy regularizer weight (try 0.05 - 0.2)\n",
    "force_uniform_W_for_alr_epochs = 1  # number of ALR epochs to force uniform W (diagnostic)\n",
    "initial_tau = 20.0               # starting softmax temperature in CostAttention\n",
    "final_tau = 1.0                  # final tau after anneal\n",
    "tau_anneal_epochs = max(1, alr_epochs)  # over how many ALR epochs to anneal tau\n",
    "\n",
    "# dataset / transforms (CIFAR-100)\n",
    "transform = T.Compose([T.ToTensor(), T.Normalize((0.5071,0.4867,0.4408),(0.2675,0.2565,0.2761))])\n",
    "train_ds = torchvision.datasets.CIFAR100(root=str(repo_root / \"data\"), train=True, download=True, transform=transform)\n",
    "val_ds = torchvision.datasets.CIFAR100(root=str(repo_root / \"data\"), train=False, download=True, transform=transform)\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=256, shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "# === Load model factory & ALR loss ===\n",
    "import models as models_pkg  # should already be importable in your notebook\n",
    "assert \"daiic_resnet34\" in models_pkg.REGISTRY, \"daiic_resnet34 not found in models.REGISTRY\"\n",
    "daiic_factory = models_pkg.REGISTRY[\"daiic_resnet34\"]\n",
    "\n",
    "spec = importlib.util.spec_from_file_location(\"local_alr\", str(repo_root / \"losses\" / \"alr.py\"))\n",
    "mod_alr = importlib.util.module_from_spec(spec)\n",
    "spec.loader.exec_module(mod_alr)\n",
    "ALRLoss = mod_alr.ALRLoss\n",
    "\n",
    "# === Helpers ===\n",
    "def instantiate_model():\n",
    "    m = daiic_factory(num_classes=num_classes, pretrained=False, cifar_stem=True, in_channels=3).to(device)\n",
    "    # set initial tau if module supports it\n",
    "    for _, mm in m.named_modules():\n",
    "        if mm.__class__.__name__.lower().startswith(\"costattention\") and hasattr(mm, \"tau\"):\n",
    "            mm.tau = float(initial_tau)\n",
    "    return m\n",
    "\n",
    "def make_optimizer(model):\n",
    "    head_params, base_params = [], []\n",
    "    for n,p in model.named_parameters():\n",
    "        if not p.requires_grad: \n",
    "            continue\n",
    "        if (\"classifier\" in n) or (\"fc\" in n) or (\"head\" in n):\n",
    "            head_params.append(p)\n",
    "        else:\n",
    "            base_params.append(p)\n",
    "    groups = []\n",
    "    if base_params: groups.append({'params': base_params, 'lr': lr_base})\n",
    "    if head_params: groups.append({'params': head_params, 'lr': lr_head})\n",
    "    if not groups:\n",
    "        groups = [{'params': model.parameters(), 'lr': lr_base}]\n",
    "    opt = torch.optim.SGD(groups, momentum=0.9, weight_decay=weight_decay)\n",
    "    return opt\n",
    "\n",
    "def set_costatt_requires_grad(model, req=False):\n",
    "    touched = 0\n",
    "    for name, m in model.named_modules():\n",
    "        if m.__class__.__name__.lower().startswith(\"costattention\") or hasattr(m, \"conv1x1\"):\n",
    "            conv = getattr(m, \"conv1x1\", None)\n",
    "            if conv is not None:\n",
    "                for p in conv.parameters():\n",
    "                    p.requires_grad = bool(req)\n",
    "                touched += 1\n",
    "    print(\"set_costatt_requires_grad -> touched:\", touched, \"req:\", req)\n",
    "\n",
    "def normalize_outputs(outputs):\n",
    "    # same normalization used earlier\n",
    "    logits = None; W = None\n",
    "    if isinstance(outputs, dict):\n",
    "        logits = outputs.get(\"logits\", None)\n",
    "        if logits is None and \"probs\" in outputs:\n",
    "            probs = outputs[\"probs\"].clamp(1e-6, 1-1e-6)\n",
    "            logits = torch.log(probs / (1.0 - probs))\n",
    "        W = outputs.get(\"W\", None)\n",
    "    elif torch.is_tensor(outputs):\n",
    "        logits = outputs\n",
    "    else:\n",
    "        raise RuntimeError(\"Unsupported model outputs type\")\n",
    "    logits = logits.to(device).float()\n",
    "    if W is not None:\n",
    "        W = W.to(device).float()\n",
    "        if not torch.isfinite(W).all():\n",
    "            W = torch.ones_like(logits)\n",
    "        W = W.clamp(min=0.0)\n",
    "        rs = W.sum(dim=1, keepdim=True)\n",
    "        zero_rows = rs == 0\n",
    "        if zero_rows.any():\n",
    "            W[zero_rows.expand_as(W)] = 1.0\n",
    "            rs = W.sum(dim=1, keepdim=True)\n",
    "        W = W / (rs + 1e-12)\n",
    "        if not torch.isfinite(W).all() or (W.abs().max() < 1e-12):\n",
    "            W = torch.ones_like(logits) / float(logits.size(1))\n",
    "    return logits, W, outputs\n",
    "\n",
    "def W_stats(W):\n",
    "    if W is None:\n",
    "        return {}\n",
    "    top1 = float(W.max(dim=1).values.mean())\n",
    "    ent = float((-(W * (W.clamp(1e-12).log())).sum(dim=1)).mean())\n",
    "    return {'W_mean': float(W.mean()), 'W_top1_mean': top1, 'W_entropy_mean': ent}\n",
    "\n",
    "# === Training/Eval loops ===\n",
    "ce_loss_fn = nn.CrossEntropyLoss()\n",
    "alr_loss_fn = ALRLoss(class_weights=None, reduction=\"mean\")\n",
    "\n",
    "def train_one_epoch(model, opt, epoch_idx, total_epochs, mode='ce', force_uniform_W=False, lambda_ent=0.0, alr_scale=1.0):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    running_correct = 0\n",
    "    seen = 0\n",
    "    for step, (x,y) in enumerate(train_loader, 1):\n",
    "        x = x.to(device); y = y.to(device)\n",
    "        opt.zero_grad()\n",
    "        outputs = model(x)\n",
    "        logits, W, _ = normalize_outputs(outputs)\n",
    "        if mode == 'ce':\n",
    "            loss = ce_loss_fn(logits, y.view(-1))\n",
    "        elif mode == 'alr':\n",
    "            if force_uniform_W:\n",
    "                W_use = torch.ones_like(logits, device=logits.device) / float(logits.size(1))\n",
    "            else:\n",
    "                W_use = W\n",
    "            raw = alr_loss_fn(logits, y, W_use)\n",
    "            if isinstance(raw, (tuple, list)):\n",
    "                raw = raw[0]\n",
    "            loss = raw * alr_scale\n",
    "            if lambda_ent > 0.0:\n",
    "                ent = (-(W_use * (W_use.clamp(1e-12).log())).sum(dim=1)).mean()\n",
    "                # subtract ent (we want to maximize entropy) with small weight\n",
    "                loss = loss - lambda_ent * ent\n",
    "        else:\n",
    "            raise ValueError(\"mode must be 'ce' or 'alr'\")\n",
    "\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        preds = logits.argmax(dim=1)\n",
    "        running_correct += int((preds == y.view(-1)).sum().item())\n",
    "        running_loss += float(loss.item()) * x.size(0)\n",
    "        seen += x.size(0)\n",
    "\n",
    "        # optional per-step logging\n",
    "        if step % 200 == 0:\n",
    "            print(f\"  step {step} | avg_loss {running_loss/seen:.4f} | avg_acc {running_correct/seen:.4f}\")\n",
    "\n",
    "    return running_loss / max(1, seen), running_correct / max(1, seen)\n",
    "\n",
    "def eval_model(model, mode='ce', alr_scale=1.0, max_batches=50):\n",
    "    model.eval()\n",
    "    loss_sum = 0.0; correct = 0; n = 0\n",
    "    with torch.no_grad():\n",
    "        for i, (x,y) in enumerate(val_loader):\n",
    "            x = x.to(device); y = y.to(device)\n",
    "            outputs = model(x)\n",
    "            logits, W, _ = normalize_outputs(outputs)\n",
    "            if mode == 'ce':\n",
    "                l = float(ce_loss_fn(logits, y.view(-1)).item())\n",
    "            else:\n",
    "                raw = alr_loss_fn(logits, y, W)\n",
    "                if isinstance(raw, (tuple,list)): raw = raw[0]\n",
    "                l = float(raw.item()) * alr_scale\n",
    "            loss_sum += l * x.size(0)\n",
    "            preds = logits.argmax(dim=1)\n",
    "            correct += int((preds == y.view(-1)).sum().item())\n",
    "            n += x.size(0)\n",
    "            if i >= max_batches: break\n",
    "    return loss_sum / max(1, n), correct / max(1, n)\n",
    "\n",
    "# === Instantiate model, freeze cost-att, and run CE warmup ===\n",
    "model = instantiate_model()\n",
    "set_costatt_requires_grad(model, req=False)  # freeze cost-att during warmup\n",
    "opt = make_optimizer(model)\n",
    "\n",
    "print(\"\\n=== CE Warmup ===\")\n",
    "for e in range(warmup_epochs):\n",
    "    t0 = time.time()\n",
    "    train_loss, train_acc = train_one_epoch(model, opt, epoch_idx=e, total_epochs=warmup_epochs, mode='ce')\n",
    "    val_loss, val_acc = eval_model(model, mode='ce')\n",
    "    # print W stats on a small val batch\n",
    "    with torch.no_grad():\n",
    "        out = model(next(iter(val_loader))[0].to(device))\n",
    "        _, W_val, _ = normalize_outputs(out)\n",
    "    print(f\"CE epoch {e+1}/{warmup_epochs} | train_loss {train_loss:.4f} train_acc {train_acc:.4f} | val_loss {val_loss:.4f} val_acc {val_acc:.4f} | time {(time.time()-t0):.1f}s\")\n",
    "    print(\"  W stats:\", W_stats(W_val))\n",
    "\n",
    "# === Prepare for ALR phase: unfreeze cost-att, recreate optimizer ===\n",
    "set_costatt_requires_grad(model, req=True)\n",
    "opt = make_optimizer(model)\n",
    "\n",
    "# tau annealing schedule across ALR epochs\n",
    "def tau_for_alr_epoch(epoch_idx):\n",
    "    frac = min(1.0, max(0.0, float(epoch_idx) / float(max(1, tau_anneal_epochs-1))))\n",
    "    return initial_tau + (final_tau - initial_tau) * frac\n",
    "\n",
    "print(\"\\n=== ALR Phase ===\")\n",
    "for alr_epoch in range(alr_epochs):\n",
    "    # set tau for this epoch\n",
    "    tau_val = tau_for_alr_epoch(alr_epoch)\n",
    "    for _, mm in model.named_modules():\n",
    "        if mm.__class__.__name__.lower().startswith(\"costattention\") and hasattr(mm, \"tau\"):\n",
    "            mm.tau = float(tau_val)\n",
    "    # choose whether to force uniform W during first few ALR epochs\n",
    "    force_uniform = (alr_epoch < force_uniform_W_for_alr_epochs)\n",
    "    t0 = time.time()\n",
    "    train_loss, train_acc = train_one_epoch(model, opt, epoch_idx=alr_epoch, total_epochs=alr_epochs, mode='alr',\n",
    "                                            force_uniform_W=force_uniform, lambda_ent=lambda_ent, alr_scale=alr_scale)\n",
    "    val_loss, val_acc = eval_model(model, mode='alr', alr_scale=alr_scale)\n",
    "    # sample W stats from val_loader\n",
    "    with torch.no_grad():\n",
    "        out = model(next(iter(val_loader))[0].to(device))\n",
    "        _, W_val, _ = normalize_outputs(out)\n",
    "    print(f\"ALR epoch {alr_epoch+1}/{alr_epochs} | tau {tau_val:.3f} | force_uniform {force_uniform} | train_loss {train_loss:.4f} train_acc {train_acc:.4f} | val_loss {val_loss:.4f} val_acc {val_acc:.4f} | time {(time.time()-t0):.1f}s\")\n",
    "    print(\"  W stats:\", W_stats(W_val))\n",
    "\n",
    "print(\"\\nTraining run complete. Monitor W_top1_mean and W_entropy_mean; if W_top1 climbs too fast, increase tau or lambda_ent, or force longer uniform W.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1fcaee8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08758036",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "454aaabe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recreated optimizer with param groups: [(108, 0.01), (6, 0.1), (2, 1e-06)]\n"
     ]
    }
   ],
   "source": [
    "# Identity-safe optimizer creator: compare params by id() to avoid tensor-equality checks\n",
    "import torch, torch.nn as nn\n",
    "\n",
    "def make_optimizer_with_costatt_lr(model, base_lr=1e-2, head_lr=1e-1, costatt_lr=None, weight_decay=1e-4):\n",
    "    head_params = []\n",
    "    base_params = []\n",
    "    costatt_params = []\n",
    "\n",
    "    # Collect base/head params heuristically by name\n",
    "    for n, p in model.named_parameters():\n",
    "        if not p.requires_grad:\n",
    "            continue\n",
    "        if (\"classifier\" in n) or (\"fc\" in n) or (\"head\" in n):\n",
    "            head_params.append(p)\n",
    "        else:\n",
    "            base_params.append(p)\n",
    "\n",
    "    # If costatt_lr is specified, collect conv params from cost-att modules\n",
    "    if costatt_lr is not None:\n",
    "        conv_params = []\n",
    "        for name, m in model.named_modules():\n",
    "            if m.__class__.__name__.lower().startswith(\"costattention\") or hasattr(m, \"conv1x1\"):\n",
    "                conv = getattr(m, \"conv1x1\", None)\n",
    "                if conv is not None:\n",
    "                    for p in conv.parameters():\n",
    "                        conv_params.append(p)\n",
    "\n",
    "        # use identity comparison via id()\n",
    "        conv_ids = {id(p) for p in conv_params}\n",
    "        base_params = [p for p in base_params if id(p) not in conv_ids]\n",
    "        head_params = [p for p in head_params if id(p) not in conv_ids]\n",
    "        costatt_params = conv_params\n",
    "\n",
    "    # Build param groups\n",
    "    param_groups = []\n",
    "    if base_params:\n",
    "        param_groups.append({'params': base_params, 'lr': base_lr})\n",
    "    if head_params:\n",
    "        param_groups.append({'params': head_params, 'lr': head_lr})\n",
    "    if costatt_params:\n",
    "        param_groups.append({'params': costatt_params, 'lr': costatt_lr})\n",
    "    if not param_groups:\n",
    "        param_groups = [{'params': model.parameters(), 'lr': base_lr}]\n",
    "\n",
    "    opt = torch.optim.SGD(param_groups, momentum=0.9, weight_decay=weight_decay)\n",
    "    return opt\n",
    "\n",
    "# Recreate optimizer using your chosen LRs\n",
    "costatt_low_lr = 1e-6\n",
    "lr_base = 0.01\n",
    "lr_head = 0.1\n",
    "weight_decay = 1e-4\n",
    "\n",
    "opt = make_optimizer_with_costatt_lr(model, base_lr=lr_base, head_lr=lr_head, costatt_lr=costatt_low_lr, weight_decay=weight_decay)\n",
    "print(\"Recreated optimizer with param groups:\", [(len(g['params']), g['lr']) for g in opt.param_groups])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ee0b1335",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (3) must match the size of tensor b (512) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 122\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;66;03m# instantiate and initial optimizer: during warmup we don't want costatt to learn -> costatt_lr=costatt_low_lr\u001b[39;00m\n\u001b[1;32m    121\u001b[0m model \u001b[38;5;241m=\u001b[39m instantiate_model()\n\u001b[0;32m--> 122\u001b[0m opt \u001b[38;5;241m=\u001b[39m \u001b[43mmake_optimizer_with_costatt_lr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbase_lr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr_base\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhead_lr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr_head\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcostatt_lr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcostatt_low_lr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    124\u001b[0m ce_loss_fn \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n\u001b[1;32m    125\u001b[0m alr_loss_fn \u001b[38;5;241m=\u001b[39m ALRLoss(class_weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, reduction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[7], line 77\u001b[0m, in \u001b[0;36mmake_optimizer_with_costatt_lr\u001b[0;34m(model, base_lr, head_lr, costatt_lr)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m conv \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m conv\u001b[38;5;241m.\u001b[39mparameters():\n\u001b[1;32m     76\u001b[0m         \u001b[38;5;66;03m# remove p from base/head if accidentally included\u001b[39;00m\n\u001b[0;32m---> 77\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mp\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbase_params\u001b[49m: base_params\u001b[38;5;241m.\u001b[39mremove(p)\n\u001b[1;32m     78\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m head_params: head_params\u001b[38;5;241m.\u001b[39mremove(p)\n\u001b[1;32m     79\u001b[0m         costatt_params\u001b[38;5;241m.\u001b[39mappend(p)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (3) must match the size of tensor b (512) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "# Paste & run this cell in your notebook (replacement / tuned training)\n",
    "import time, math, pathlib, importlib, importlib.util\n",
    "import torch, torch.nn as nn, torchvision, torchvision.transforms as T\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "repo_root = pathlib.Path(\"/home/user/abin_ref_papers/project_structure_demo/dnn_template\").resolve()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "\n",
    "# === Conservative hyperparams (more cautious than before) ===\n",
    "num_classes = 100\n",
    "batch_size = 128\n",
    "warmup_epochs = 3           # CE warmup epochs\n",
    "alr_epochs = 12             # ALR epochs after warmup\n",
    "lr_base = 0.01\n",
    "lr_head = 0.1\n",
    "weight_decay = 1e-4\n",
    "\n",
    "# ALR specifics (more conservative)\n",
    "alr_scale = float(num_classes)\n",
    "lambda_ent = 0.2                    # stronger entropy regularizer\n",
    "force_uniform_alr_epochs = 3        # force uniform W for first 3 ALR epochs\n",
    "initial_tau = 50.0                  # very soft initial tau\n",
    "final_tau = 5.0                     # don't anneal too quickly\n",
    "tau_anneal_epochs = max(1, alr_epochs)  # slow anneal across ALR epochs\n",
    "costatt_low_lr = 1e-6               # extremely small LR for cost_att (instead of full freeze)\n",
    "w_top1_alert_thresh = 0.5           # if W_top1_mean exceeds this, we will re-apply conservative measures\n",
    "\n",
    "# dataset\n",
    "transform = T.Compose([T.ToTensor(), T.Normalize((0.5071,0.4867,0.4408),(0.2675,0.2565,0.2761))])\n",
    "train_ds = torchvision.datasets.CIFAR100(root=str(repo_root / \"data\"), train=True, download=True, transform=transform)\n",
    "val_ds = torchvision.datasets.CIFAR100(root=str(repo_root / \"data\"), train=False, download=True, transform=transform)\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=256, shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "# load factory + ALR\n",
    "import models as models_pkg\n",
    "assert \"daiic_resnet34\" in models_pkg.REGISTRY\n",
    "daiic_factory = models_pkg.REGISTRY[\"daiic_resnet34\"]\n",
    "spec = importlib.util.spec_from_file_location(\"local_alr\", str(repo_root / \"losses\" / \"alr.py\"))\n",
    "mod_alr = importlib.util.module_from_spec(spec)\n",
    "spec.loader.exec_module(mod_alr)\n",
    "ALRLoss = mod_alr.ALRLoss\n",
    "\n",
    "# helpers\n",
    "def instantiate_model():\n",
    "    m = daiic_factory(num_classes=num_classes, pretrained=False, cifar_stem=True, in_channels=3).to(device)\n",
    "    for _, mm in m.named_modules():\n",
    "        if mm.__class__.__name__.lower().startswith(\"costattention\") and hasattr(mm, \"tau\"):\n",
    "            mm.tau = float(initial_tau)\n",
    "    return m\n",
    "\n",
    "def get_costatt_modules(model):\n",
    "    mods = []\n",
    "    for name,m in model.named_modules():\n",
    "        if m.__class__.__name__.lower().startswith(\"costattention\") or hasattr(m, \"conv1x1\"):\n",
    "            mods.append((name,m))\n",
    "    return mods\n",
    "\n",
    "def make_optimizer_with_costatt_lr(model, base_lr=lr_base, head_lr=lr_head, costatt_lr=None):\n",
    "    head_params, base_params, costatt_params = [], [], []\n",
    "    for n,p in model.named_parameters():\n",
    "        if not p.requires_grad:\n",
    "            continue\n",
    "        if (\"classifier\" in n) or (\"fc\" in n) or (\"head\" in n):\n",
    "            head_params.append(p)\n",
    "        else:\n",
    "            base_params.append(p)\n",
    "    # gather costatt parameters separately if costatt_lr provided\n",
    "    if costatt_lr is not None:\n",
    "        # find costatt modules\n",
    "        for name,m in get_costatt_modules(model):\n",
    "            conv = getattr(m, \"conv1x1\", None)\n",
    "            if conv is not None:\n",
    "                for p in conv.parameters():\n",
    "                    # remove p from base/head if accidentally included\n",
    "                    if p in base_params: base_params.remove(p)\n",
    "                    if p in head_params: head_params.remove(p)\n",
    "                    costatt_params.append(p)\n",
    "    param_groups = []\n",
    "    if base_params: param_groups.append({'params': base_params, 'lr': base_lr})\n",
    "    if head_params: param_groups.append({'params': head_params, 'lr': head_lr})\n",
    "    if costatt_params: param_groups.append({'params': costatt_params, 'lr': costatt_lr})\n",
    "    if not param_groups:\n",
    "        param_groups = [{'params': model.parameters(), 'lr': base_lr}]\n",
    "    opt = torch.optim.SGD(param_groups, momentum=0.9, weight_decay=weight_decay)\n",
    "    return opt\n",
    "\n",
    "def normalize_outputs(outputs):\n",
    "    logits=None; W=None\n",
    "    if isinstance(outputs, dict):\n",
    "        logits = outputs.get(\"logits\", None)\n",
    "        if logits is None and \"probs\" in outputs:\n",
    "            probs = outputs[\"probs\"].clamp(1e-6,1-1e-6)\n",
    "            logits = torch.log(probs/(1-probs))\n",
    "        W = outputs.get(\"W\", None)\n",
    "    elif torch.is_tensor(outputs):\n",
    "        logits = outputs\n",
    "    logits = logits.to(device).float()\n",
    "    if W is not None:\n",
    "        W = W.to(device).float()\n",
    "        if not torch.isfinite(W).all(): W = torch.ones_like(logits)\n",
    "        W = W.clamp(min=0.0)\n",
    "        rs = W.sum(dim=1, keepdim=True)\n",
    "        zero_rows = rs == 0\n",
    "        if zero_rows.any():\n",
    "            W[zero_rows.expand_as(W)] = 1.0\n",
    "            rs = W.sum(dim=1, keepdim=True)\n",
    "        W = W / (rs + 1e-12)\n",
    "        if not torch.isfinite(W).all() or (W.abs().max() < 1e-12):\n",
    "            W = torch.ones_like(logits)/float(logits.size(1))\n",
    "    return logits, W, outputs\n",
    "\n",
    "def W_stats(W):\n",
    "    if W is None: return {}\n",
    "    t1 = float(W.max(dim=1).values.mean())\n",
    "    ent = float((-(W*(W.clamp(1e-12).log())).sum(dim=1)).mean())\n",
    "    return {'W_mean': float(W.mean()), 'W_top1_mean': t1, 'W_entropy_mean': ent}\n",
    "\n",
    "# instantiate and initial optimizer: during warmup we don't want costatt to learn -> costatt_lr=costatt_low_lr\n",
    "model = instantiate_model()\n",
    "opt = make_optimizer_with_costatt_lr(model, base_lr=lr_base, head_lr=lr_head, costatt_lr=costatt_low_lr)\n",
    "\n",
    "ce_loss_fn = nn.CrossEntropyLoss()\n",
    "alr_loss_fn = ALRLoss(class_weights=None, reduction='mean')\n",
    "\n",
    "# training loops\n",
    "def train_epoch(model, opt, loader, mode='ce', force_uniform_W=False, lambda_ent=0.0, alr_scale=1.0):\n",
    "    model.train()\n",
    "    total_loss=0.0; total_correct=0; total_n=0\n",
    "    for i,(x,y) in enumerate(loader,1):\n",
    "        x = x.to(device); y = y.to(device)\n",
    "        opt.zero_grad()\n",
    "        out = model(x)\n",
    "        logits, W, _ = normalize_outputs(out)\n",
    "        if mode=='ce':\n",
    "            loss = ce_loss_fn(logits, y.view(-1))\n",
    "        else:\n",
    "            if force_uniform_W:\n",
    "                W_use = torch.ones_like(logits, device=logits.device) / float(logits.size(1))\n",
    "            else:\n",
    "                W_use = W\n",
    "            raw = alr_loss_fn(logits, y, W_use)\n",
    "            if isinstance(raw, (tuple,list)): raw = raw[0]\n",
    "            loss = raw * alr_scale\n",
    "            if lambda_ent > 0.0:\n",
    "                ent = (-(W_use * (W_use.clamp(1e-12).log())).sum(dim=1)).mean()\n",
    "                loss = loss - lambda_ent * ent\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        preds = logits.argmax(dim=1)\n",
    "        total_correct += int((preds == y.view(-1)).sum().item())\n",
    "        total_loss += float(loss.item()) * x.size(0)\n",
    "        total_n += x.size(0)\n",
    "    return total_loss/max(1,total_n), total_correct/max(1,total_n)\n",
    "\n",
    "def eval_model(model, loader, mode='ce', alr_scale=1.0, max_batches=50):\n",
    "    model.eval()\n",
    "    loss_sum=0.0; correct=0; n=0\n",
    "    with torch.no_grad():\n",
    "        for i,(x,y) in enumerate(loader):\n",
    "            x = x.to(device); y = y.to(device)\n",
    "            out = model(x)\n",
    "            logits, W, _ = normalize_outputs(out)\n",
    "            if mode=='ce':\n",
    "                l = float(ce_loss_fn(logits, y.view(-1)).item())\n",
    "            else:\n",
    "                raw = alr_loss_fn(logits, y, W)\n",
    "                if isinstance(raw,(tuple,list)): raw = raw[0]\n",
    "                l = float(raw.item())*alr_scale\n",
    "            loss_sum += l * x.size(0)\n",
    "            preds = logits.argmax(dim=1)\n",
    "            correct += int((preds == y.view(-1)).sum().item())\n",
    "            n += x.size(0)\n",
    "            if i>=max_batches: break\n",
    "    return loss_sum/max(1,n), correct/max(1,n)\n",
    "\n",
    "print(\"\\n=== CE warmup (cost_att very low LR) ===\")\n",
    "for e in range(warmup_epochs):\n",
    "    t0=time.time()\n",
    "    train_loss, train_acc = train_epoch(model, opt, train_loader, mode='ce')\n",
    "    val_loss, val_acc = eval_model(model, val_loader, mode='ce')\n",
    "    # sample W stats\n",
    "    with torch.no_grad():\n",
    "        _, W_val, _ = normalize_outputs(model(next(iter(val_loader))[0].to(device)))\n",
    "    print(f\"CE epoch {e+1}/{warmup_epochs} | train_loss {train_loss:.4f} train_acc {train_acc:.4f} | val_loss {val_loss:.4f} val_acc {val_acc:.4f} | time {(time.time()-t0):.1f}s\")\n",
    "    print(\"  W stats:\", W_stats(W_val))\n",
    "\n",
    "# Now prepare ALR: we'll increase costatt lr slightly (still small) and optionally force uniform first few ALR epochs\n",
    "print(\"\\nReconfiguring optimizer for ALR phase: costatt lr will be small but nonzero.\")\n",
    "# set costatt lr to small positive value to allow slow adaptation\n",
    "opt = make_optimizer_with_costatt_lr(model, base_lr=lr_base, head_lr=lr_head, costatt_lr=1e-5)\n",
    "\n",
    "def tau_for_epoch(idx):\n",
    "    frac = min(1.0, float(idx) / float(max(1,tau_anneal_epochs-1)))\n",
    "    return initial_tau + (final_tau - initial_tau) * frac\n",
    "\n",
    "print(\"\\n=== ALR phase (conservative) ===\")\n",
    "for ae in range(alr_epochs):\n",
    "    # set tau for costatt modules\n",
    "    tau_val = tau_for_epoch(ae)\n",
    "    for _, mm in model.named_modules():\n",
    "        if mm.__class__.__name__.lower().startswith(\"costattention\") and hasattr(mm, \"tau\"):\n",
    "            mm.tau = float(tau_val)\n",
    "    force_uniform = (ae < force_uniform_alr_epochs)\n",
    "    t0=time.time()\n",
    "    train_loss, train_acc = train_epoch(model, opt, train_loader, mode='alr',\n",
    "                                        force_uniform_W=force_uniform, lambda_ent=lambda_ent, alr_scale=alr_scale)\n",
    "    val_loss, val_acc = eval_model(model, val_loader, mode='alr', alr_scale=alr_scale)\n",
    "    # sample W stats\n",
    "    with torch.no_grad():\n",
    "        _, W_val, _ = normalize_outputs(model(next(iter(val_loader))[0].to(device)))\n",
    "    stats = W_stats(W_val)\n",
    "    print(f\"ALR epoch {ae+1}/{alr_epochs} | tau {tau_val:.3f} | force_uniform {force_uniform} | train_loss {train_loss:.4f} train_acc {train_acc:.4f} | val_loss {val_loss:.4f} val_acc {val_acc:.4f} | time {(time.time()-t0):.1f}s\")\n",
    "    print(\"  W stats:\", stats)\n",
    "\n",
    "    # Auto-safety: if W becomes too peaky early, re-apply conservative measures\n",
    "    if stats.get('W_top1_mean', 0.0) > w_top1_alert_thresh and ae < (force_uniform_alr_epochs + 2):\n",
    "        print(\"  WARNING: W collapsed early (W_top1_mean>%.2f). Re-applying conservative measures: increasing tau and forcing uniform W next epoch.\" % w_top1_alert_thresh)\n",
    "        # increase tau to soften more\n",
    "        for _, mm in model.named_modules():\n",
    "            if mm.__class__.__name__.lower().startswith(\"costattention\") and hasattr(mm, \"tau\"):\n",
    "                mm.tau = float(max(mm.tau * 1.5, initial_tau))\n",
    "        # enforce that the next ALR epoch uses uniform W by bumping counter\n",
    "        force_uniform_alr_epochs = max(force_uniform_alr_epochs, ae + 2)\n",
    "        # optionally reduce costatt lr further for stability\n",
    "        opt = make_optimizer_with_costatt_lr(model, base_lr=lr_base, head_lr=lr_head, costatt_lr=1e-6)\n",
    "\n",
    "print(\"\\nConservative ALR training complete. Inspect W stats and accuracies; if W still collapses, increase force_uniform_alr_epochs or lambda_ent and re-run.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fc89e620",
   "metadata": {},
   "outputs": [],
   "source": [
    "from losses.alr import ALRLoss\n",
    "\n",
    "# ALR loss (module version with diagnostics)\n",
    "alr_loss_fn = ALRLoss(class_weights=None, reduction=\"mean\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "54a6b7bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Continue ALR epoch 1/5 ===\n",
      "train_loss 73.6660 train_acc 0.0102 | val_loss 74.2384 val_acc 0.0101 | time 22.8s\n",
      " W stats: {'W_mean': 0.009999999776482582, 'W_top1_mean': 0.01275566965341568, 'W_entropy_mean': 4.598027229309082}\n",
      "\n",
      "=== Continue ALR epoch 2/5 ===\n",
      "train_loss 73.6689 train_acc 0.0101 | val_loss 74.1307 val_acc 0.0099 | time 23.3s\n",
      " W stats: {'W_mean': 0.009999999776482582, 'W_top1_mean': 0.012720715254545212, 'W_entropy_mean': 4.5982513427734375}\n",
      "\n",
      "=== Continue ALR epoch 3/5 ===\n",
      "train_loss 73.7928 train_acc 0.0099 | val_loss 74.1156 val_acc 0.0099 | time 24.9s\n",
      " W stats: {'W_mean': 0.009999999776482582, 'W_top1_mean': 0.012717755511403084, 'W_entropy_mean': 4.598291873931885}\n",
      "\n",
      "=== Continue ALR epoch 4/5 ===\n",
      "train_loss 73.7745 train_acc 0.0098 | val_loss 74.1295 val_acc 0.0102 | time 24.7s\n",
      " W stats: {'W_mean': 0.009999999776482582, 'W_top1_mean': 0.012717355042696, 'W_entropy_mean': 4.5982537269592285}\n",
      "\n",
      "=== Continue ALR epoch 5/5 ===\n",
      "train_loss 73.7902 train_acc 0.0104 | val_loss 74.1591 val_acc 0.0101 | time 24.8s\n",
      " W stats: {'W_mean': 0.009999999776482582, 'W_top1_mean': 0.012731881812214851, 'W_entropy_mean': 4.598208904266357}\n"
     ]
    }
   ],
   "source": [
    "# Continue conservative ALR training for a few epochs (run after optimizer creation)\n",
    "import time, torch\n",
    "\n",
    "# Settings for continuation\n",
    "continue_epochs = 5\n",
    "force_uniform_epochs = 2         # force uniform W for first N continuation epochs\n",
    "lambda_ent = 0.2                 # entropy regularizer\n",
    "alr_scale = float(100)           # adjust if you changed num_classes\n",
    "alert_thresh = 0.5               # if W_top1_mean > alert_thresh -> take action\n",
    "max_val_batches = 50\n",
    "\n",
    "# helper (reuse normalize_outputs and W_stats from earlier cell; redefine if not present)\n",
    "def normalize_outputs(outputs):\n",
    "    logits=None; W=None\n",
    "    if isinstance(outputs, dict):\n",
    "        logits = outputs.get(\"logits\", None)\n",
    "        if logits is None and \"probs\" in outputs:\n",
    "            probs = outputs[\"probs\"].clamp(1e-6,1-1e-6)\n",
    "            logits = torch.log(probs/(1-probs))\n",
    "        W = outputs.get(\"W\", None)\n",
    "    elif torch.is_tensor(outputs):\n",
    "        logits = outputs\n",
    "    logits = logits.to(device).float()\n",
    "    if W is not None:\n",
    "        W = W.to(device).float()\n",
    "        if not torch.isfinite(W).all(): W = torch.ones_like(logits)\n",
    "        W = W.clamp(min=0.0)\n",
    "        rs = W.sum(dim=1, keepdim=True)\n",
    "        zero_rows = rs == 0\n",
    "        if zero_rows.any():\n",
    "            W[zero_rows.expand_as(W)] = 1.0\n",
    "            rs = W.sum(dim=1, keepdim=True)\n",
    "        W = W / (rs + 1e-12)\n",
    "        if not torch.isfinite(W).all() or (W.abs().max() < 1e-12):\n",
    "            W = torch.ones_like(logits)/float(logits.size(1))\n",
    "    return logits, W, outputs\n",
    "\n",
    "def W_stats(W):\n",
    "    if W is None: return {}\n",
    "    t1 = float(W.max(dim=1).values.mean())\n",
    "    ent = float((-(W*(W.clamp(1e-12).log())).sum(dim=1)).mean())\n",
    "    return {'W_mean': float(W.mean()), 'W_top1_mean': t1, 'W_entropy_mean': ent}\n",
    "\n",
    "# training and eval loops (small, reuse your CE/ALR functions if present)\n",
    "ce_loss_fn = torch.nn.CrossEntropyLoss()\n",
    "# assume alr_loss_fn exists (ALRLoss instance). If not, re-create: alr_loss_fn = ALRLoss(class_weights=None, reduction='mean')\n",
    "\n",
    "def train_one_epoch_alr(model, opt, loader, force_uniform=False, lambda_ent=0.0, alr_scale=1.0):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    running_correct = 0\n",
    "    seen = 0\n",
    "    for i,(x,y) in enumerate(loader, 1):\n",
    "        x = x.to(device); y = y.to(device)\n",
    "        opt.zero_grad()\n",
    "        outputs = model(x)\n",
    "        logits, W, _ = normalize_outputs(outputs)\n",
    "        if force_uniform:\n",
    "            W_use = torch.ones_like(logits, device=logits.device) / float(logits.size(1))\n",
    "        else:\n",
    "            W_use = W\n",
    "        raw = alr_loss_fn(logits, y, W_use)\n",
    "        if isinstance(raw, (tuple, list)):\n",
    "            raw = raw[0]\n",
    "        loss = raw * alr_scale\n",
    "        if lambda_ent > 0.0:\n",
    "            ent = (-(W_use * (W_use.clamp(1e-12).log())).sum(dim=1)).mean()\n",
    "            loss = loss - lambda_ent * ent\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        preds = logits.argmax(dim=1)\n",
    "        running_correct += int((preds == y.view(-1)).sum().item())\n",
    "        running_loss += float(loss.item()) * x.size(0)\n",
    "        seen += x.size(0)\n",
    "    return running_loss / max(1, seen), running_correct / max(1, seen)\n",
    "\n",
    "def eval_alr(model, loader, alr_scale=1.0, max_batches=50):\n",
    "    model.eval()\n",
    "    loss_sum=0.0; correct=0; n=0\n",
    "    with torch.no_grad():\n",
    "        for i,(x,y) in enumerate(loader):\n",
    "            x = x.to(device); y = y.to(device)\n",
    "            outputs = model(x)\n",
    "            logits, W, _ = normalize_outputs(outputs)\n",
    "            raw = alr_loss_fn(logits, y, W)\n",
    "            if isinstance(raw,(tuple,list)): raw = raw[0]\n",
    "            l = float(raw.item()) * alr_scale\n",
    "            loss_sum += l * x.size(0)\n",
    "            preds = logits.argmax(dim=1)\n",
    "            correct += int((preds == y.view(-1)).sum().item())\n",
    "            n += x.size(0)\n",
    "            if i >= max_batches: break\n",
    "    return loss_sum / max(1,n), correct / max(1,n)\n",
    "\n",
    "# Run continuation\n",
    "for epoch in range(1, continue_epochs+1):\n",
    "    print(f\"\\n=== Continue ALR epoch {epoch}/{continue_epochs} ===\")\n",
    "    force_uniform = (epoch <= force_uniform_epochs)\n",
    "    t0=time.time()\n",
    "    tr_loss, tr_acc = train_one_epoch_alr(model, opt, train_loader, force_uniform=force_uniform, lambda_ent=lambda_ent, alr_scale=alr_scale)\n",
    "    val_loss, val_acc = eval_alr(model, val_loader, alr_scale=alr_scale)\n",
    "    # sample W stats on a val batch\n",
    "    with torch.no_grad():\n",
    "        _, W_sample, _ = normalize_outputs(model(next(iter(val_loader))[0].to(device)))\n",
    "    stats = W_stats(W_sample)\n",
    "    print(f\"train_loss {tr_loss:.4f} train_acc {tr_acc:.4f} | val_loss {val_loss:.4f} val_acc {val_acc:.4f} | time {(time.time()-t0):.1f}s\")\n",
    "    print(\" W stats:\", stats)\n",
    "    # safety: if W collapsed, strengthen defense\n",
    "    if stats.get('W_top1_mean', 0.0) > alert_thresh:\n",
    "        print(\"  ALERT: W_top1_mean > %.2f  increasing cost-att LR suppression and forcing uniform W next epoch.\" % alert_thresh)\n",
    "        # set optimizer cost-att lr to very small by rebuilding opt\n",
    "        try:\n",
    "            opt = make_optimizer_with_costatt_lr(model, base_lr=lr_base, head_lr=lr_head, costatt_lr=1e-6, weight_decay=weight_decay)\n",
    "            force_uniform_epochs = max(force_uniform_epochs, epoch+1)\n",
    "        except Exception as e:\n",
    "            print(\"  Failed to rebuild optimizer for safety:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dad6ac4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f58b143b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw ALR mean/std: 0.7393078804016113 0.031193485483527184\n",
      "bce_per_entry mean/std: 0.7381960153579712 0.2565256357192993\n",
      "W top1 mean: 0.012673519551753998\n"
     ]
    }
   ],
   "source": [
    "# Run in notebook to inspect raw ALR scale for one batch\n",
    "import torch, torch.nn.functional as F\n",
    "model.eval()\n",
    "x,y = next(iter(train_loader))\n",
    "x = x.to(device); y = y.to(device)\n",
    "with torch.no_grad():\n",
    "    out = model(x)\n",
    "logits = out['logits']\n",
    "W = out.get('W', torch.ones_like(logits)/logits.size(1))\n",
    "probs = torch.sigmoid(logits).clamp(1e-8, 1-1e-8)\n",
    "\n",
    "# BCE per entry and raw ALR\n",
    "B,K = logits.shape\n",
    "bce_per_entry = -( (y.view(-1,1)==torch.arange(K, device=device)).float() * torch.log(probs) +\n",
    "                   (1 - (y.view(-1,1)==torch.arange(K, device=device)).float()) * torch.log(1-probs) )\n",
    "# if y is one-hot multi-hot adapt above; this approximates single-label BCE matrix\n",
    "raw_alr = (W * bce_per_entry).sum(dim=1)   # [B]\n",
    "print(\"raw ALR mean/std:\", float(raw_alr.mean()), float(raw_alr.std()))\n",
    "print(\"bce_per_entry mean/std:\", float(bce_per_entry.mean()), float(bce_per_entry.std()))\n",
    "print(\"W top1 mean:\", float(W.max(dim=1).values.mean()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "abb83eae",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (3) must match the size of tensor b (512) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m opt \u001b[38;5;241m=\u001b[39m \u001b[43mmake_optimizer_with_costatt_lr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbase_lr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.001\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhead_lr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcostatt_lr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-6\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Combined CE + small ALR continuation (safe)\u001b[39;00m\n\u001b[1;32m      5\u001b[0m alpha_alr \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.1\u001b[39m          \u001b[38;5;66;03m# weight for ALR term (try 0.05-0.2)\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[7], line 77\u001b[0m, in \u001b[0;36mmake_optimizer_with_costatt_lr\u001b[0;34m(model, base_lr, head_lr, costatt_lr)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m conv \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m conv\u001b[38;5;241m.\u001b[39mparameters():\n\u001b[1;32m     76\u001b[0m         \u001b[38;5;66;03m# remove p from base/head if accidentally included\u001b[39;00m\n\u001b[0;32m---> 77\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mp\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbase_params\u001b[49m: base_params\u001b[38;5;241m.\u001b[39mremove(p)\n\u001b[1;32m     78\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m head_params: head_params\u001b[38;5;241m.\u001b[39mremove(p)\n\u001b[1;32m     79\u001b[0m         costatt_params\u001b[38;5;241m.\u001b[39mappend(p)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (3) must match the size of tensor b (512) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "opt = make_optimizer_with_costatt_lr(model, base_lr=0.001, head_lr=0.01, costatt_lr=1e-6)\n",
    "\n",
    "\n",
    "# Combined CE + small ALR continuation (safe)\n",
    "alpha_alr = 0.1          # weight for ALR term (try 0.05-0.2)\n",
    "alr_scale = 1.0          # do NOT multiply by 100; keep 1.0 or small\n",
    "lambda_ent = 0.1         # keep small entropy reg\n",
    "epochs = 5\n",
    "\n",
    "# If your optimizer still uses head_lr=0.1, rebuild with smaller head LR:\n",
    "opt = make_optimizer_with_costatt_lr(model, base_lr=0.001, head_lr=0.01, costatt_lr=1e-6, weight_decay=1e-4)\n",
    "print(\"Recreated safer optimizer groups:\", [(len(g['params']), g['lr']) for g in opt.param_groups])\n",
    "\n",
    "ce_loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "for ep in range(epochs):\n",
    "    model.train()\n",
    "    tloss = 0.0; tacc = 0; n=0\n",
    "    for x,y in train_loader:\n",
    "        x = x.to(device); y = y.to(device)\n",
    "        opt.zero_grad()\n",
    "        out = model(x)\n",
    "        logits, W, _ = normalize_outputs(out)\n",
    "        # CE\n",
    "        loss_ce = ce_loss_fn(logits, y.view(-1))\n",
    "        # ALR (raw)\n",
    "        raw = alr_loss_fn(logits, y, W)\n",
    "        if isinstance(raw, (tuple,list)): raw = raw[0]\n",
    "        loss_alr = raw * alr_scale\n",
    "        # entropy reg\n",
    "        ent = (-(W * (W.clamp(1e-12).log())).sum(dim=1)).mean()\n",
    "        loss = (1.0 - alpha_alr) * loss_ce + alpha_alr * loss_alr - 0.0 * lambda_ent * ent\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        preds = logits.argmax(dim=1)\n",
    "        tacc += int((preds == y.view(-1)).sum().item())\n",
    "        tloss += float(loss.item()) * x.size(0)\n",
    "        n += x.size(0)\n",
    "    print(f\"Epoch {ep+1}/{epochs} | train_loss {tloss/n:.4f} train_acc {tacc/n:.4f}\")\n",
    "    # quick eval\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        out = model(next(iter(val_loader))[0].to(device))\n",
    "        _, Wv, _ = normalize_outputs(out)\n",
    "    print(\"  sample W stats:\", W_stats(Wv))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4fc9de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7c2f958c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_optimizer_with_costatt_lr(model, base_lr=0.01, head_lr=0.1, costatt_lr=1e-6, weight_decay=1e-4):\n",
    "    base_params, head_params, conv_params = [], [], []\n",
    "    for name, p in model.named_parameters():\n",
    "        if not p.requires_grad:\n",
    "            continue\n",
    "        if \"classifier\" in name:\n",
    "            head_params.append(p)\n",
    "        else:\n",
    "            base_params.append(p)\n",
    "        if \"cost_att.conv1x1\" in name:  # or whatever your module name is\n",
    "            conv_params.append(p)\n",
    "    base_params = [p for p in base_params if p not in conv_params]\n",
    "    head_params = [p for p in head_params if p not in conv_params]\n",
    "    costatt_params = conv_params\n",
    "    return torch.optim.SGD(\n",
    "        [\n",
    "            {\"params\": base_params, \"lr\": base_lr, \"weight_decay\": weight_decay},\n",
    "            {\"params\": head_params, \"lr\": head_lr, \"weight_decay\": weight_decay},\n",
    "            {\"params\": costatt_params, \"lr\": costatt_lr, \"weight_decay\": 0.0},  # cost-att safe\n",
    "        ],\n",
    "        momentum=0.9,\n",
    "        nesterov=True,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ae6623a3",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (512) must match the size of tensor b (3) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m lambda_ent \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.2\u001b[39m\n\u001b[1;32m      4\u001b[0m epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m\n\u001b[0;32m----> 5\u001b[0m opt \u001b[38;5;241m=\u001b[39m \u001b[43mmake_optimizer_with_costatt_lr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbase_lr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.001\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhead_lr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcostatt_lr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-6\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-4\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ep \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[1;32m      8\u001b[0m     model\u001b[38;5;241m.\u001b[39mtrain()\n",
      "Cell \u001b[0;32mIn[16], line 12\u001b[0m, in \u001b[0;36mmake_optimizer_with_costatt_lr\u001b[0;34m(model, base_lr, head_lr, costatt_lr, weight_decay)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcost_att.conv1x1\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m name:  \u001b[38;5;66;03m# or whatever your module name is\u001b[39;00m\n\u001b[1;32m     11\u001b[0m         conv_params\u001b[38;5;241m.\u001b[39mappend(p)\n\u001b[0;32m---> 12\u001b[0m base_params \u001b[38;5;241m=\u001b[39m [p \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m base_params \u001b[38;5;28;01mif\u001b[39;00m p \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m conv_params]\n\u001b[1;32m     13\u001b[0m head_params \u001b[38;5;241m=\u001b[39m [p \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m head_params \u001b[38;5;28;01mif\u001b[39;00m p \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m conv_params]\n\u001b[1;32m     14\u001b[0m costatt_params \u001b[38;5;241m=\u001b[39m conv_params\n",
      "Cell \u001b[0;32mIn[16], line 12\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcost_att.conv1x1\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m name:  \u001b[38;5;66;03m# or whatever your module name is\u001b[39;00m\n\u001b[1;32m     11\u001b[0m         conv_params\u001b[38;5;241m.\u001b[39mappend(p)\n\u001b[0;32m---> 12\u001b[0m base_params \u001b[38;5;241m=\u001b[39m [p \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m base_params \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mp\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mconv_params\u001b[49m]\n\u001b[1;32m     13\u001b[0m head_params \u001b[38;5;241m=\u001b[39m [p \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m head_params \u001b[38;5;28;01mif\u001b[39;00m p \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m conv_params]\n\u001b[1;32m     14\u001b[0m costatt_params \u001b[38;5;241m=\u001b[39m conv_params\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (512) must match the size of tensor b (3) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "# ALR-only continuation (no big scaling)\n",
    "alr_scale = 1.0\n",
    "lambda_ent = 0.2\n",
    "epochs = 5\n",
    "opt = make_optimizer_with_costatt_lr(model, base_lr=0.001, head_lr=0.01, costatt_lr=1e-6, weight_decay=1e-4)\n",
    "\n",
    "for ep in range(epochs):\n",
    "    model.train()\n",
    "    tloss = 0.0; tacc = 0; n=0\n",
    "    for x,y in train_loader:\n",
    "        x=x.to(device); y=y.to(device)\n",
    "        opt.zero_grad()\n",
    "        out = model(x)\n",
    "        logits, W, _ = normalize_outputs(out)\n",
    "        raw = alr_loss_fn(logits, y, W)\n",
    "        if isinstance(raw,(list,tuple)): raw = raw[0]\n",
    "        loss = raw * alr_scale - lambda_ent * (-(W*(W.clamp(1e-12).log())).sum(dim=1)).mean()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        preds = logits.argmax(dim=1)\n",
    "        tacc += int((preds == y.view(-1)).sum().item())\n",
    "        tloss += float(loss.item()) * x.size(0)\n",
    "        n += x.size(0)\n",
    "    print(f\"ALR epoch {ep+1}/{epochs} | train_loss {tloss/n:.4f} train_acc {tacc/n:.4f}\")\n",
    "    # check W stats\n",
    "    with torch.no_grad():\n",
    "        _, Wv, _ = normalize_outputs(model(next(iter(val_loader))[0].to(device)))\n",
    "    print(\"  sample W stats:\", W_stats(Wv))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02db2a7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9f08ab32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizer groups: [(108, 0.001, 0.0001), (6, 0.01, 0.0001), (2, 1e-06, 0.0)]\n",
      "Epoch 1/5 | train_loss 3.6987 train_acc 0.0449 | sample_val_ce 4.0024\n",
      "  W stats: {'W_mean': 0.009999999776482582, 'W_top1_mean': 0.012885664589703083, 'W_entropy_mean': 4.597731590270996}\n",
      "Epoch 2/5 | train_loss 3.2088 train_acc 0.1110 | sample_val_ce 3.6298\n",
      "  W stats: {'W_mean': 0.009999999776482582, 'W_top1_mean': 0.012905037961900234, 'W_entropy_mean': 4.597824573516846}\n",
      "Epoch 3/5 | train_loss 2.9220 train_acc 0.1626 | sample_val_ce 3.2485\n",
      "  W stats: {'W_mean': 0.009999999776482582, 'W_top1_mean': 0.012911481782793999, 'W_entropy_mean': 4.598141670227051}\n",
      "Epoch 4/5 | train_loss 2.6861 train_acc 0.2113 | sample_val_ce 3.2091\n",
      "  W stats: {'W_mean': 0.009999999776482582, 'W_top1_mean': 0.013038859702646732, 'W_entropy_mean': 4.59785795211792}\n",
      "Epoch 5/5 | train_loss 2.4785 train_acc 0.2518 | sample_val_ce 2.9636\n",
      "  W stats: {'W_mean': 0.009999999776482582, 'W_top1_mean': 0.01300949975848198, 'W_entropy_mean': 4.597658157348633}\n",
      "Done continuation. If acc improves slowly, gradually increase alpha_alr or decrease costatt_lr.\n"
     ]
    }
   ],
   "source": [
    "# ===== Fix helper + run CE+small-ALR continuation =====\n",
    "import time, torch, torch.nn as nn\n",
    "\n",
    "# Identity-safe optimizer creator that accepts weight_decay\n",
    "def make_optimizer_with_costatt_lr(model, base_lr=1e-3, head_lr=1e-2, costatt_lr=None, weight_decay=1e-4):\n",
    "    base_params = []\n",
    "    head_params = []\n",
    "    # collect base/head by name heuristics\n",
    "    for n, p in model.named_parameters():\n",
    "        if not p.requires_grad:\n",
    "            continue\n",
    "        if (\"classifier\" in n) or (\"fc\" in n) or (\"head\" in n):\n",
    "            head_params.append(p)\n",
    "        else:\n",
    "            base_params.append(p)\n",
    "\n",
    "    costatt_params = []\n",
    "    if costatt_lr is not None:\n",
    "        conv_params = []\n",
    "        for name, m in model.named_modules():\n",
    "            if m.__class__.__name__.lower().startswith(\"costattention\") or hasattr(m, \"conv1x1\"):\n",
    "                conv = getattr(m, \"conv1x1\", None)\n",
    "                if conv is not None:\n",
    "                    for p in conv.parameters():\n",
    "                        conv_params.append(p)\n",
    "        conv_ids = {id(p) for p in conv_params}\n",
    "        # filter out conv params by identity (id) to avoid tensor-equality comparisons\n",
    "        base_params = [p for p in base_params if id(p) not in conv_ids]\n",
    "        head_params = [p for p in head_params if id(p) not in conv_ids]\n",
    "        costatt_params = conv_params\n",
    "\n",
    "    param_groups = []\n",
    "    if base_params:\n",
    "        param_groups.append({\"params\": base_params, \"lr\": base_lr, \"weight_decay\": weight_decay})\n",
    "    if head_params:\n",
    "        param_groups.append({\"params\": head_params, \"lr\": head_lr, \"weight_decay\": weight_decay})\n",
    "    if costatt_params:\n",
    "        # usually no weight decay for small convs; set to 0.0 or keep weight_decay\n",
    "        param_groups.append({\"params\": costatt_params, \"lr\": costatt_lr, \"weight_decay\": 0.0})\n",
    "\n",
    "    if not param_groups:\n",
    "        param_groups = [{\"params\": model.parameters(), \"lr\": base_lr, \"weight_decay\": weight_decay}]\n",
    "\n",
    "    return torch.optim.SGD(param_groups, momentum=0.9)\n",
    "\n",
    "# Recreate optimizer with safer LRs for CE+ALR continuation\n",
    "# (adjust base/head lr if you prefer)\n",
    "opt = make_optimizer_with_costatt_lr(model, base_lr=1e-3, head_lr=1e-2, costatt_lr=1e-6, weight_decay=1e-4)\n",
    "print(\"Optimizer groups:\", [(len(g[\"params\"]), g[\"lr\"], g.get(\"weight_decay\")) for g in opt.param_groups])\n",
    "\n",
    "# === CE + small ALR continuation ===\n",
    "alpha_alr = 0.1        # weight for ALR term (0 => pure CE; 0.1 => 10% ALR)\n",
    "alr_scale = 1.0        # do NOT scale by num_classes here\n",
    "lambda_ent = 0.05      # small entropy regularizer\n",
    "epochs = 5\n",
    "\n",
    "ce_loss_fn = nn.CrossEntropyLoss()\n",
    "# Ensure alr_loss_fn exists; recreate if needed:\n",
    "try:\n",
    "    alr_loss_fn\n",
    "except NameError:\n",
    "    from losses.alr import ALRLoss\n",
    "    alr_loss_fn = ALRLoss(class_weights=None, reduction=\"mean\")\n",
    "\n",
    "# quick helper (reuse normalize_outputs if in scope; else define small one)\n",
    "def normalize_outputs(outputs):\n",
    "    logits=None; W=None\n",
    "    if isinstance(outputs, dict):\n",
    "        logits = outputs.get(\"logits\", None)\n",
    "        if logits is None and \"probs\" in outputs:\n",
    "            probs = outputs[\"probs\"].clamp(1e-6,1-1e-6)\n",
    "            logits = torch.log(probs/(1-probs))\n",
    "        W = outputs.get(\"W\", None)\n",
    "    elif torch.is_tensor(outputs):\n",
    "        logits = outputs\n",
    "    logits = logits.to(device).float()\n",
    "    if W is not None:\n",
    "        W = W.to(device).float()\n",
    "        if not torch.isfinite(W).all(): W = torch.ones_like(logits)\n",
    "        W = W.clamp(min=0.0)\n",
    "        rs = W.sum(dim=1, keepdim=True)\n",
    "        zero_rows = rs == 0\n",
    "        if zero_rows.any():\n",
    "            W[zero_rows.expand_as(W)] = 1.0\n",
    "            rs = W.sum(dim=1, keepdim=True)\n",
    "        W = W / (rs + 1e-12)\n",
    "        if not torch.isfinite(W).all() or (W.abs().max() < 1e-12):\n",
    "            W = torch.ones_like(logits)/float(logits.size(1))\n",
    "    return logits, W, outputs\n",
    "\n",
    "def W_stats(W):\n",
    "    if W is None: return {}\n",
    "    t1 = float(W.max(dim=1).values.mean())\n",
    "    ent = float((-(W*(W.clamp(1e-12).log())).sum(dim=1)).mean())\n",
    "    return {'W_mean': float(W.mean()), 'W_top1_mean': t1, 'W_entropy_mean': ent}\n",
    "\n",
    "# Training loop\n",
    "for ep in range(1, epochs+1):\n",
    "    model.train()\n",
    "    total_loss=0.0; total_correct=0; total_n=0\n",
    "    for xb,yb in train_loader:\n",
    "        xb = xb.to(device); yb = yb.to(device)\n",
    "        opt.zero_grad()\n",
    "        out = model(xb)\n",
    "        logits, W, _ = normalize_outputs(out)\n",
    "        loss_ce = ce_loss_fn(logits, yb.view(-1))\n",
    "        raw = alr_loss_fn(logits, yb, W)\n",
    "        if isinstance(raw, (tuple,list)): raw = raw[0]\n",
    "        loss_alr = raw * alr_scale\n",
    "        ent = (-(W * (W.clamp(1e-12).log())).sum(dim=1)).mean()\n",
    "        loss = (1.0 - alpha_alr) * loss_ce + alpha_alr * loss_alr - lambda_ent * ent\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        preds = logits.argmax(dim=1)\n",
    "        total_correct += int((preds == yb.view(-1)).sum().item())\n",
    "        total_loss += float(loss.item()) * xb.size(0)\n",
    "        total_n += xb.size(0)\n",
    "    # eval quick\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outv = model(next(iter(val_loader))[0].to(device))\n",
    "        lv, Wv, _ = normalize_outputs(outv)\n",
    "        val_ce = float(ce_loss_fn(lv, next(iter(val_loader))[1].to(device).view(-1)))\n",
    "    print(f\"Epoch {ep}/{epochs} | train_loss {total_loss/total_n:.4f} train_acc {total_correct/total_n:.4f} | sample_val_ce {val_ce:.4f}\")\n",
    "    print(\"  W stats:\", W_stats(Wv))\n",
    "\n",
    "print(\"Done continuation. If acc improves slowly, gradually increase alpha_alr or decrease costatt_lr.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a21a16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "46d88de6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[cont ep 1/20] alpha_alr=0.100 | train_loss 1.7586 train_acc 0.4076 | sample_val_ce 2.5989 | time 21.6s\n",
      "   W stats: {'W_mean': 0.009999999776482582, 'W_top1_mean': 0.01319112628698349, 'W_entropy_mean': 4.597421169281006}\n",
      "   -> New best sample_val_ce: 2.598945379257202\n",
      "[cont ep 2/20] alpha_alr=0.111 | train_loss 1.5431 train_acc 0.4548 | sample_val_ce 2.5885 | time 22.0s\n",
      "   W stats: {'W_mean': 0.009999999776482582, 'W_top1_mean': 0.013182642869651318, 'W_entropy_mean': 4.597250938415527}\n",
      "   -> New best sample_val_ce: 2.588477849960327\n",
      "[cont ep 3/20] alpha_alr=0.122 | train_loss 1.3150 train_acc 0.5093 | sample_val_ce 2.6740 | time 21.9s\n",
      "   W stats: {'W_mean': 0.009999999776482582, 'W_top1_mean': 0.013111312873661518, 'W_entropy_mean': 4.597574710845947}\n",
      "[cont ep 4/20] alpha_alr=0.133 | train_loss 1.0714 train_acc 0.5700 | sample_val_ce 2.8547 | time 22.4s\n",
      "   W stats: {'W_mean': 0.009999999776482582, 'W_top1_mean': 0.01308935135602951, 'W_entropy_mean': 4.597599983215332}\n",
      "[cont ep 5/20] alpha_alr=0.144 | train_loss 0.8250 train_acc 0.6395 | sample_val_ce 2.7459 | time 21.6s\n",
      "   W stats: {'W_mean': 0.009999999776482582, 'W_top1_mean': 0.013178316876292229, 'W_entropy_mean': 4.5974955558776855}\n",
      "[cont ep 6/20] alpha_alr=0.156 | train_loss 0.5849 train_acc 0.7127 | sample_val_ce 3.1202 | time 22.0s\n",
      "   W stats: {'W_mean': 0.009999999776482582, 'W_top1_mean': 0.013148455880582333, 'W_entropy_mean': 4.597430229187012}\n",
      "[cont ep 7/20] alpha_alr=0.167 | train_loss 0.3677 train_acc 0.7838 | sample_val_ce 3.1888 | time 22.0s\n",
      "   W stats: {'W_mean': 0.009999999776482582, 'W_top1_mean': 0.013363955542445183, 'W_entropy_mean': 4.597179412841797}\n",
      "[cont ep 8/20] alpha_alr=0.178 | train_loss 0.1584 train_acc 0.8621 | sample_val_ce 3.2635 | time 22.5s\n",
      "   W stats: {'W_mean': 0.009999999776482582, 'W_top1_mean': 0.013237801380455494, 'W_entropy_mean': 4.597439765930176}\n",
      "[cont ep 9/20] alpha_alr=0.189 | train_loss 0.0073 train_acc 0.9229 | sample_val_ce 3.4420 | time 22.1s\n",
      "   W stats: {'W_mean': 0.009999999776482582, 'W_top1_mean': 0.013189985416829586, 'W_entropy_mean': 4.597470283508301}\n",
      ">>> Increasing cost-att lr to 1e-05  (rebuilding optimizer )\n",
      "[cont ep 10/20] alpha_alr=0.200 | train_loss 0.0804 train_acc 0.8854 | sample_val_ce 4.0543 | time 22.0s\n",
      "   W stats: {'W_mean': 0.009999999776482582, 'W_top1_mean': 0.013197064399719238, 'W_entropy_mean': 4.597434043884277}\n",
      "[cont ep 11/20] alpha_alr=0.200 | train_loss 0.1622 train_acc 0.8473 | sample_val_ce 3.9626 | time 22.8s\n",
      "   W stats: {'W_mean': 0.009999999776482582, 'W_top1_mean': 0.013284532353281975, 'W_entropy_mean': 4.5972089767456055}\n",
      "[cont ep 12/20] alpha_alr=0.200 | train_loss 0.0489 train_acc 0.8927 | sample_val_ce 4.1357 | time 23.0s\n",
      "   W stats: {'W_mean': 0.009999999776482582, 'W_top1_mean': 0.013222567737102509, 'W_entropy_mean': 4.597135543823242}\n",
      "[cont ep 13/20] alpha_alr=0.200 | train_loss -0.0897 train_acc 0.9524 | sample_val_ce 3.7224 | time 22.9s\n",
      "   W stats: {'W_mean': 0.009999999776482582, 'W_top1_mean': 0.013170304708182812, 'W_entropy_mean': 4.5975446701049805}\n",
      "[cont ep 14/20] alpha_alr=0.200 | train_loss -0.1564 train_acc 0.9798 | sample_val_ce 3.9581 | time 22.8s\n",
      "   W stats: {'W_mean': 0.009999999776482582, 'W_top1_mean': 0.013241128996014595, 'W_entropy_mean': 4.597376346588135}\n",
      "[cont ep 15/20] alpha_alr=0.200 | train_loss -0.1903 train_acc 0.9928 | sample_val_ce 3.8285 | time 22.7s\n",
      "   W stats: {'W_mean': 0.009999999776482582, 'W_top1_mean': 0.013247234746813774, 'W_entropy_mean': 4.597359657287598}\n",
      "[cont ep 16/20] alpha_alr=0.200 | train_loss -0.2045 train_acc 0.9965 | sample_val_ce 3.6728 | time 22.5s\n",
      "   W stats: {'W_mean': 0.009999999776482582, 'W_top1_mean': 0.013200619257986546, 'W_entropy_mean': 4.597496032714844}\n",
      "[cont ep 17/20] alpha_alr=0.200 | train_loss -0.2095 train_acc 0.9978 | sample_val_ce 3.6714 | time 22.4s\n",
      "   W stats: {'W_mean': 0.009999999776482582, 'W_top1_mean': 0.013181297108530998, 'W_entropy_mean': 4.597484588623047}\n",
      "[cont ep 18/20] alpha_alr=0.200 | train_loss -0.2123 train_acc 0.9982 | sample_val_ce 3.6914 | time 22.4s\n",
      "   W stats: {'W_mean': 0.009999999776482582, 'W_top1_mean': 0.013190224766731262, 'W_entropy_mean': 4.597513675689697}\n",
      "[cont ep 19/20] alpha_alr=0.200 | train_loss -0.2142 train_acc 0.9988 | sample_val_ce 3.6287 | time 22.3s\n",
      "   W stats: {'W_mean': 0.009999999776482582, 'W_top1_mean': 0.01320564839988947, 'W_entropy_mean': 4.59743595123291}\n",
      "[cont ep 20/20] alpha_alr=0.200 | train_loss -0.2151 train_acc 0.9988 | sample_val_ce 3.6630 | time 22.7s\n",
      "   W stats: {'W_mean': 0.009999999776482582, 'W_top1_mean': 0.013180233538150787, 'W_entropy_mean': 4.597501277923584}\n",
      "Restored best sample_val_ce checkpoint state.\n",
      "Continuation finished. Final W stats: {'W_mean': 0.009999999776482582, 'W_top1_mean': 0.013180233538150787, 'W_entropy_mean': 4.597501277923584}\n"
     ]
    }
   ],
   "source": [
    "# Continue training with cautious schedule + checkpointing\n",
    "import time, copy, torch\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "# Hyperparams for continuation\n",
    "extra_epochs = 20\n",
    "alpha_start = 0.10       # initial CE/ALR blend weight (ALR fraction)\n",
    "alpha_end = 0.20         # target ALR weight after ramp\n",
    "alpha_ramp_epochs = 10   # ramp ALR weight over this many epochs\n",
    "alr_scale = 1.0\n",
    "lambda_ent = 0.05        # small entropy reg\n",
    "enable_costatt_epoch = 10   # at this epoch (1-indexed in continuation) increase costatt_lr\n",
    "new_costatt_lr = 1e-5      # new small lr for cost-att once we allow it slightly adapt\n",
    "# scheduler params\n",
    "T_max = extra_epochs\n",
    "# checkpoint\n",
    "best_val_ce = float('inf')\n",
    "best_state = None\n",
    "\n",
    "# Recreate optimizer with the current conservative costatt_lr (should be 1e-6 now)\n",
    "opt = make_optimizer_with_costatt_lr(model, base_lr=1e-3, head_lr=1e-2, costatt_lr=1e-6, weight_decay=1e-4)\n",
    "# identify param group indices for scheduler (we'll attach scheduler to groups 0..n-1 but PyTorch handles all groups)\n",
    "scheduler = CosineAnnealingLR(opt, T_max=T_max, eta_min=1e-6)\n",
    "\n",
    "def compute_val_ce_sample(model):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        xb,yb = next(iter(val_loader))\n",
    "        xb = xb.to(device); yb = yb.to(device)\n",
    "        out = model(xb)\n",
    "        logits, W, _ = normalize_outputs(out)\n",
    "        ce = torch.nn.functional.cross_entropy(logits, yb.view(-1)).item()\n",
    "    return ce, W\n",
    "\n",
    "for e in range(1, extra_epochs+1):\n",
    "    # linear ramp for alpha\n",
    "    if e <= alpha_ramp_epochs:\n",
    "        alpha_alr = alpha_start + (alpha_end - alpha_start) * ( (e-1) / max(1,(alpha_ramp_epochs-1)) )\n",
    "    else:\n",
    "        alpha_alr = alpha_end\n",
    "\n",
    "    # optionally increase costatt_lr at scheduled epoch by rebuilding optimizer\n",
    "    if e == enable_costatt_epoch:\n",
    "        print(\">>> Increasing cost-att lr to\", new_costatt_lr, \" (rebuilding optimizer )\")\n",
    "        opt = make_optimizer_with_costatt_lr(model, base_lr=1e-3, head_lr=1e-2, costatt_lr=new_costatt_lr, weight_decay=1e-4)\n",
    "        scheduler = CosineAnnealingLR(opt, T_max=max(1, extra_epochs-e+1), eta_min=1e-6)\n",
    "\n",
    "    # one epoch training (combined CE + ALR)\n",
    "    model.train()\n",
    "    total_loss = 0.0; total_correct = 0; total_n = 0\n",
    "    t0 = time.time()\n",
    "    for xb, yb in train_loader:\n",
    "        xb = xb.to(device); yb = yb.to(device)\n",
    "        opt.zero_grad()\n",
    "        out = model(xb)\n",
    "        logits, W, _ = normalize_outputs(out)\n",
    "        loss_ce = torch.nn.functional.cross_entropy(logits, yb.view(-1))\n",
    "        raw = alr_loss_fn(logits, yb, W)\n",
    "        if isinstance(raw, (tuple,list)): raw = raw[0]\n",
    "        loss_alr = raw * alr_scale\n",
    "        ent = (-(W * (W.clamp(1e-12).log())).sum(dim=1)).mean()\n",
    "        loss = (1.0 - alpha_alr) * loss_ce + alpha_alr * loss_alr - lambda_ent * ent\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        total_loss += float(loss.item()) * xb.size(0)\n",
    "        preds = logits.argmax(dim=1)\n",
    "        total_correct += int((preds == yb.view(-1)).sum().item())\n",
    "        total_n += xb.size(0)\n",
    "\n",
    "    # scheduler step (per-epoch)\n",
    "    try:\n",
    "        scheduler.step()\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # eval CE on small sample as proxy and compute W stats\n",
    "    val_ce_sample, W_sample = compute_val_ce_sample(model)\n",
    "    stats = W_stats(W_sample)\n",
    "    epoch_time = time.time() - t0\n",
    "    train_loss = total_loss / max(1,total_n)\n",
    "    train_acc  = total_correct / max(1,total_n)\n",
    "    print(f\"[cont ep {e}/{extra_epochs}] alpha_alr={alpha_alr:.3f} | train_loss {train_loss:.4f} train_acc {train_acc:.4f} | sample_val_ce {val_ce_sample:.4f} | time {epoch_time:.1f}s\")\n",
    "    print(\"   W stats:\", stats)\n",
    "\n",
    "    # checkpoint best sample val CE\n",
    "    if val_ce_sample < best_val_ce:\n",
    "        best_val_ce = val_ce_sample\n",
    "        best_state = copy.deepcopy(model.state_dict())\n",
    "        print(\"   -> New best sample_val_ce:\", best_val_ce)\n",
    "\n",
    "    # safety: if W collapses too quickly, roll back cost-att aggressiveness\n",
    "    if stats.get('W_top1_mean', 0.0) > 0.5 and e < enable_costatt_epoch + 3:\n",
    "        print(\"   !!! W collapsed early (top1>0.5). Reverting cost-att lr to tiny and forcing uniform for next epoch\")\n",
    "        opt = make_optimizer_with_costatt_lr(model, base_lr=1e-3, head_lr=1e-2, costatt_lr=1e-6, weight_decay=1e-4)\n",
    "        # optionally reduce alpha_alr for next epoch (not implemented here; you can restart loop)\n",
    "\n",
    "# after loop: restore best if you want\n",
    "if best_state is not None:\n",
    "    model.load_state_dict(best_state)\n",
    "    print(\"Restored best sample_val_ce checkpoint state.\")\n",
    "\n",
    "print(\"Continuation finished. Final W stats:\", W_stats(W_sample))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "acf265df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  step 200 | avg_loss 1.5239 | avg_acc 0.5164\n",
      "Epoch 1/5 | train_loss 1.5230 train_acc 0.5170 | val_loss 2.6366 val_acc 0.3390\n",
      "  W stats: {'W_mean': 0.010000000707805157, 'W_top1_mean': 0.013088570907711983, 'W_entropy_mean': 4.597934246063232}\n",
      "  step 200 | avg_loss 1.4668 | avg_acc 0.5266\n",
      "Epoch 2/5 | train_loss 1.4592 train_acc 0.5257 | val_loss 2.6064 val_acc 0.3417\n",
      "  W stats: {'W_mean': 0.010000000707805157, 'W_top1_mean': 0.013099893927574158, 'W_entropy_mean': 4.5977067947387695}\n",
      "  step 200 | avg_loss 1.3991 | avg_acc 0.5343\n",
      "Epoch 3/5 | train_loss 1.4024 train_acc 0.5342 | val_loss 2.5936 val_acc 0.3462\n",
      "  W stats: {'W_mean': 0.010000000707805157, 'W_top1_mean': 0.013130143284797668, 'W_entropy_mean': 4.597745895385742}\n",
      "  step 200 | avg_loss 1.3486 | avg_acc 0.5425\n",
      "Epoch 4/5 | train_loss 1.3469 train_acc 0.5431 | val_loss 2.5815 val_acc 0.3487\n",
      "  W stats: {'W_mean': 0.010000000707805157, 'W_top1_mean': 0.013044561259448528, 'W_entropy_mean': 4.597859859466553}\n",
      "  step 200 | avg_loss 1.3167 | avg_acc 0.5388\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 53\u001b[0m\n\u001b[1;32m     50\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     51\u001b[0m opt\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m---> 53\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m*\u001b[39m x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     54\u001b[0m preds \u001b[38;5;241m=\u001b[39m logits\u001b[38;5;241m.\u001b[39margmax(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     55\u001b[0m total_correct \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (preds \u001b[38;5;241m==\u001b[39m y)\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39mitem()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch, time\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def evaluate_ce(model, val_loader, device=\"cuda\"):\n",
    "    \"\"\"Evaluate with plain CE on full val set.\"\"\"\n",
    "    model.eval()\n",
    "    total_loss, total_correct, total_samples = 0.0, 0, 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in val_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            outputs = model(x)\n",
    "            logits = outputs[\"logits\"] if isinstance(outputs, dict) else outputs\n",
    "            loss = F.cross_entropy(logits, y)\n",
    "            total_loss += loss.item() * x.size(0)\n",
    "            preds = logits.argmax(1)\n",
    "            total_correct += (preds == y).sum().item()\n",
    "            total_samples += x.size(0)\n",
    "    return total_loss / total_samples, total_correct / total_samples\n",
    "\n",
    "# === Continuation training with full val eval ===\n",
    "extra_epochs = 5\n",
    "alpha_start, alpha_end, alpha_ramp_epochs = 0.1, 0.2, 5\n",
    "lambda_ent = 0.1\n",
    "alr_scale = 1.0\n",
    "\n",
    "for ep in range(1, extra_epochs + 1):\n",
    "    # Ramp ALR weight\n",
    "    alpha_alr = min(alpha_end, alpha_start + (alpha_end - alpha_start) * (ep / alpha_ramp_epochs))\n",
    "\n",
    "    model.train()\n",
    "    total_loss, total_correct, total_samples = 0.0, 0, 0\n",
    "    t0 = time.time()\n",
    "\n",
    "    for step, (x, y) in enumerate(train_loader, 1):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        opt.zero_grad()\n",
    "\n",
    "        outputs = model(x)\n",
    "        logits = outputs[\"logits\"]\n",
    "        W = outputs.get(\"W\", None)\n",
    "\n",
    "        ce = F.cross_entropy(logits, y)\n",
    "\n",
    "        if W is not None:\n",
    "            alr, _ = alr_loss_fn(logits, y, W)\n",
    "            loss = (1 - alpha_alr) * ce + alpha_alr * alr\n",
    "        else:\n",
    "            loss = ce\n",
    "\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        total_loss += loss.item() * x.size(0)\n",
    "        preds = logits.argmax(1)\n",
    "        total_correct += (preds == y).sum().item()\n",
    "        total_samples += x.size(0)\n",
    "\n",
    "        if step % 200 == 0:\n",
    "            print(f\"  step {step} | avg_loss {total_loss/total_samples:.4f} | avg_acc {total_correct/total_samples:.4f}\")\n",
    "\n",
    "    tr_loss, tr_acc = total_loss / total_samples, total_correct / total_samples\n",
    "    val_loss, val_acc = evaluate_ce(model, val_loader, device=device)\n",
    "\n",
    "    W_stats_dict = W_stats(outputs[\"W\"]) if \"W\" in outputs else {}\n",
    "\n",
    "    print(f\"Epoch {ep}/{extra_epochs} | \"\n",
    "          f\"train_loss {tr_loss:.4f} train_acc {tr_acc:.4f} | \"\n",
    "          f\"val_loss {val_loss:.4f} val_acc {val_acc:.4f}\")\n",
    "    print(\"  W stats:\", {k: float(v) for k, v in W_stats_dict.items()})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eef0db8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "snn_stdp_poisson (3.10.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
