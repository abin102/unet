# --------------------------------------------------------
# Fine-tune ResNet-50 (torchvision) on CIFAR-10
# with backbone freeze for first few epochs (SGD optimizer)
# --------------------------------------------------------
exp_name: "isic2018_mobilenetv2_freeze_lt_3_experts"
seed: 42
output_dir: "runs/uger/isic2018_experts"

# -------------------------
# dataset / dataloader
# -------------------------
dataset: "isic2018"
data_dir: "./data/isic2018"
batch_size: 64
num_workers: 4
pin_memory: true
persistent_workers: true

image_size: 224
to_rgb: false
imagenet_norm: true          # keep true since using pretrained ImageNet weights

data_args: 
  difficulty_map_path: "data/isic2018/difficulty_map_numeric.json"

  # imb_factor: 0.01
  # imb_type: "exp"
  # class_balance: false

# -------------------------
# model
# -------------------------
# model: "plug_and_play_expert"
# model_args:
#   backbone: "resnet50_torch"
#   backbone_args: { num_classes: 7, pretrained: true, cifar_stem: false }
#   feature_layer: "layer3"
#   adapter_channels: 256
#   expert_dims: 256
#   thresholds: [0.33, 0.66]
#   num_classes: 7


model: "plug_and_play_expert"
model_args:
  backbone: "mobilenetv2_backbone"         # <-- use the backbone factory by name
  backbone_args: 
    pretrained: true
    in_channels: 3
                       # optional; leave empty or pass args for backbone
  probe_input_size: [1, 3, 224, 224]
  feature_layer: "layer3"
  adapter_channels: 256
  expert_dims: 256
  # thresholds: [0.33, 0.66]
  num_classes: 7
  uncertainty_dropout: 0.1
  curriculum_loss_multiplier: 0.01


# -------------------------
# loss
# -------------------------
loss: "ce"
loss_args: {}
  # head_classes: [0, 1, 2]      # indices of head classes
  # medium_classes: [3, 4, 5]    # indices of medium classes
  # tail_classes: [6, 7, 8, 9]   # indices of tail classes


# -------------------------
# metrics & evaluation
# -------------------------
metrics:
  - "acc"


epochs: 200
amp: false                   # disable mixed precision for stable fine-tuning
grad_clip: 5.0

# -------------------------
# optimizer / scheduler
# -------------------------
optim: "sgd"
optim_args:
  lr: 0.005
  momentum: 0.9
  weight_decay: 1e-5

# scheduler: "cosine"
# scheduler_args:
#   T_max: 1250
#   eta_min: 0.0

# scheduler: "multistep"
# scheduler_args:
#   milestones: [160, 180]
#   gamma: 0.1
#   warmup_epochs: 5     # optional â€” add this to enable the 5-epoch warmup

scheduler: "multistep"
scheduler_args:
  milestones: [30, 60, 90]   
  gamma: 0.2
  warmup_epochs: 0  

# -------------------------
# training runtime
# -------------------------

# freeze backbone for initial epochs, then unfreeze and fine-tune full model
# freeze_backbone_epochs: 5    # number of epochs to freeze backbone
# unfreeze_reduce_lr: 1       # multiply LR by this when unfreezing (<=1.0 typical)
# head_only_lr: 0.01            # (for record-keeping; used if head-only LR implemented later)

log_interval: 100
val_interval: 1

# -------------------------
# early stopping
# -------------------------
early_stop:
  patience: 30
  monitor: "val/acc"
  mode: "max"

# -------------------------
# checkpointing
# -------------------------
checkpoint:
  save_best_only: true
  monitor: "val/acc"
  mode: "max"
  save_interval_epochs: 1

# -------------------------
# diagnostics
# -------------------------
diagnostics:
  log_W_histogram: false
  save_attention_maps: false
save_trace: true


# reproducibility
deterministic: true


# logging 
logging:
  debug: false         # for backtracing and diagnosing
  level: INFO          # or DEBUG / WARNING / ERROR
  rotation: 50 MB      # rotate log file when >50MB
  retention: 14 days   # delete older logs
  compression: zip     # compress rotated logs
  tensorboard: false   # disable TB by default
  attention: false     # for saving attention maps

use_wandb: true
wandb:
  project: UGER
  entity: abin24-cids # name-same as project name
  resume: false
