# --------------------------------------------------------
# Fine-tune ResNet-50 (torchvision) on CIFAR-10
# with backbone freeze for first few epochs (SGD optimizer)
# --------------------------------------------------------
exp_name: "isic2018_mobilenetv2_freeze_lt"
seed: 42
output_dir: "runs/uger/isic2018_experts"

# -------------------------
# dataset / dataloader
# -------------------------
dataset: "isic2018"
data_dir: "./data/isic2018"
batch_size: 64
num_workers: 4
pin_memory: true
persistent_workers: true

image_size: 224
to_rgb: false
imagenet_norm: true          # keep true since using pretrained ImageNet weights

data_args: 
  difficulty_map_path: "data/isic2018/difficulty_map_numeric.json"

  # imb_factor: 0.01
  # imb_type: "exp"
  # class_balance: false


model: "mobilenetv2"
model_args:
  num_classes: 7
  pretrained: true


# -------------------------
# loss
# -------------------------
loss: "ce"
loss_args: {}
  # head_classes: [0, 1, 2]      # indices of head classes
  # medium_classes: [3, 4, 5]    # indices of medium classes
  # tail_classes: [6, 7, 8, 9]   # indices of tail classes


# -------------------------
# metrics & evaluation
# -------------------------
metrics:
  - "acc"


epochs: 200
amp: false                   # disable mixed precision for stable fine-tuning
grad_clip: 5.0

# -------------------------
# optimizer / scheduler
# -------------------------
optim: "sgd"
optim_args:
  lr: 0.005
  momentum: 0.9
  weight_decay: 1e-5


# scheduler: "cosine"
# scheduler_args:
#   T_max: 250
#   eta_min: 0.0


scheduler: "multistep"
scheduler_args:
  milestones: [30, 60, 90]   
  gamma: 0.2
  warmup_epochs: 0          

# -------------------------
# training runtime
# -------------------------

# freeze backbone for initial epochs, then unfreeze and fine-tune full model
# freeze_backbone_epochs: 5    # number of epochs to freeze backbone
# unfreeze_reduce_lr: 1       # multiply LR by this when unfreezing (<=1.0 typical)
# head_only_lr: 0.01            # (for record-keeping; used if head-only LR implemented later)

log_interval: 100
val_interval: 1

# -------------------------
# early stopping
# -------------------------
early_stop:
  patience: 30
  monitor: "val/acc"
  mode: "max"

# -------------------------
# checkpointing
# -------------------------
checkpoint:
  save_best_only: true
  monitor: "val/acc"
  mode: "max"
  save_interval_epochs: 1

# -------------------------
# diagnostics
# -------------------------
diagnostics:
  log_W_histogram: false
  save_attention_maps: false
save_trace: true


# reproducibility
deterministic: true


# logging 
logging:
  debug: false         # for backtracing and diagnosing
  level: INFO          # or DEBUG / WARNING / ERROR
  rotation: 50 MB      # rotate log file when >50MB
  retention: 14 days   # delete older logs
  compression: zip     # compress rotated logs
  tensorboard: false   # disable TB by default
  attention: false     # for saving attention maps

use_wandb: true
wandb:
  project: UGER
  entity: abin24-cids # name-same as project name
  resume: false
