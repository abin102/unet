# configs/cifar100_daiic_resnet34_even10.yaml
# DAIIC on CIFAR-100 imbalanced: downsample EVEN classes to 10% (paper-style experiments)
exp_name: "cifar100_daiic_resnet34_even10"
seed: 42
output_dir: "runs/daiic_cifar100_resnet34_even10"

# -------------------------
# dataset / dataloader
# -------------------------
dataset: "cifar100_imb"        # our imbalanced CIFAR100 loader (registered as cifar100_imb)
data_dir: "./data"
batch_size: 128
num_workers: 6
pin_memory: true
persistent_workers: true

image_size: 32
to_rgb: false
imagenet_norm: false

# data-specific arguments passed into REGISTRY[dataset]
data_args:
  mode: "even"               # "even" | "odd" | "custom" | "none"
  downsample_frac: 1 #0.1       # 0.1 -> keep 10% of selected classes
  custom_classes: null       # used only if mode == "custom"
  seed: 0

# -------------------------
# model
# -------------------------
model: "daiic_resnet34"
model_args:
  num_classes: 100
  pretrained: true          # <-- load ImageNet weights
  in_channels: 3
  cifar_stem: false         # <-- keep ImageNet stem (7x7 conv + maxpool)
  fusion: "concat"
  se_reduction: 16
  dropout: 0.5              # typical for ImageNet experiments


# -------------------------
# loss
# -------------------------
loss: "regularised_alr"
loss_args:
  class_weights: null
  eps: 1e-8
  reduction: "mean"
  alr_scale: 1.0      # scale applied to ALR term before blending
  lambda_ent: 0.0     # entropy regularizer on W (positive encourages higher entropy)
  alpha_alr: 0.10     # fraction of final loss contributed by ALR (0.0 = pure CE, 1.0 = pure ALR)

# -------------------------
# metrics & evaluation
# -------------------------
metrics:
  - "acc"                     # top-1 accuracy (for single-label evaluation)
  - "macro_f1"                # macro F1 (recommended for imbalanced tasks)
  - "per_class_auc"           # if you compute AUC per class (optional)

# -------------------------
# optimizer / scheduler
# -------------------------
optim: "adam"
optim_args:
  lr: 1e-3
  weight_decay: 1e-4
  betas: [0.9, 0.999]

# optionally scale LR for backbone vs attention heads (handled in trainer by param_groups)
# e.g. backbone_lr_scale: 0.1  # (if your trainer supports it)

scheduler: "cosine"
scheduler_args:
  T_max: 200
  eta_min: 0.0

# -------------------------
# training runtime
# -------------------------
epochs: 800
amp: false                    # set true to enable mixed precision if trainer supports it
grad_clip: 5.0

log_interval: 50
val_interval: 1

# early stopping
early_stop:
  patience: 20
  monitor: "val/macro_f1"
  mode: "max"

# checkpointing
checkpoint:
  save_best_only: true
  monitor: "val/acc"
  mode: "max"
  save_interval_epochs: 1

# diagnostics (trainer should use these flags)
diagnostics:
  log_W_histogram: true       # log learned W histograms per epoch
  log_se_gates: true          # log SE gating mean/std
  save_attention_maps: true   # whether to save a few attention maps per epoch
  num_attention_maps_to_save: 8

# reproducibility
deterministic: true
